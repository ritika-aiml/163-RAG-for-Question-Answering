{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeac3a9c-bebf-4341-90f2-a5bfb293c4f4",
   "metadata": {},
   "source": "# Traditional to Modern RAG: Evolution of Retrieval Methods (2020-2024)\n\nThis comprehensive tutorial traces the evolution of Retrieval-Augmented Generation (RAG) from the original 2020 paper to modern state-of-the-art implementations, with hands-on implementation of each advancement.\n\n## ğŸ¯ Learning Objectives\n\nBy the end of this tutorial, you will be able to:\n\n1. **Understand RAG Fundamentals**\n   - Implement the exact retrieval method from the original RAG paper (Lewis et al., 2020)\n   - Understand Dense Passage Retrieval (DPR) and Maximum Inner Product Search (MIPS)\n   - Recognize that DPR = vector search used in modern vector databases\n\n2. **Diagnose Performance Issues**\n   - Identify domain mismatch problems (single-hop vs multihop)\n   - Analyze why strong models on one task struggle on another\n   - Conduct error analysis and failure case studies\n\n3. **Compare Modern Embedders**\n   - Evaluate 4 embedding models: DPR (2020) â†’ Contriever (2022) â†’ E5/BGE (2023)\n   - Understand model evolution and selection criteria\n   - Make informed decisions about embedder choice for your task\n\n4. **Build Production RAG Pipelines**\n   - Implement cross-encoder reranking for improved precision\n   - Combine sparse (BM25) and dense (embeddings) search for hybrid retrieval\n   - Measure each component's contribution through experimental evaluation\n\n5. **Evaluate and Optimize**\n   - Apply 6 comprehensive metrics for multihop reasoning evaluation\n   - Profile latency and optimize performance bottlenecks\n   - Compare custom implementations with frameworks (LlamaIndex)\n\n## ğŸ—ºï¸ Tutorial Roadmap\n\n### **PART 1: Foundations (Sections 1-4)**\n```\nSetup â†’ Data Loading â†’ BM25 Baseline â†’ RAG Paper DPR Baseline\nâ””â”€ Build understanding of sparse and dense retrieval\n```\n\n### **PART 2: Analysis (Section 5)**\n```\nWhy RAG Baseline Struggles on Multihop Tasks\nâ””â”€ Learn from failures, understand domain mismatch\n```\n\n### **PART 3: Modern Embedders (Section 6)**\n```\nCompare 4 Embedders: DPR â†’ Contriever â†’ E5 â†’ BGE\nâ””â”€ See 3 years of embedding model evolution (2020-2023)\n```\n\n### **PART 4: Pipeline Enhancements (Sections 7-8)**\n```\nAdd Reranking â†’ Add Hybrid Search\nâ””â”€ Build components incrementally, measure each contribution\n```\n\n## ğŸ” Key Insight: RAG Paper Uses Vector Search!\n\n:::{admonition} Understanding the RAG Paper Baseline\n:class: important\n\n**The Original RAG Paper (Lewis et al., 2020) Uses:**\n- **Dense Passage Retrieval (DPR)** - Bi-encoder architecture\n- **Pre-trained on Natural Questions** - Single-hop factoid QA\n- **No Fine-tuning** - Frozen retriever (this is our baseline!)\n- **MIPS** - Maximum Inner Product Search for similarity\n- **Top-k Retrieval** - Return most similar document vectors\n\n**This IS vector search** - the same technology behind:\n- OpenAI embeddings + Pinecone/Weaviate/Chroma\n- LlamaIndex vector stores\n- LangChain vector retrievers\n- Modern production RAG systems\n\n**Our Tutorial Journey:**\n1. Implement RAG 2020 baseline (DPR-NQ)\n2. Test on HotpotQA multihop â†’ Measure actual performance\n3. Analyze failures â†’ Domain mismatch (trained on single-hop, testing on multihop)\n4. Upgrade to modern embedders â†’ Measure improvements\n5. Add enhancements (reranking, hybrid) â†’ Quantify gains\n:::\n\n## ğŸ“Š What We'll Build & Measure\n\nWe'll implement and compare retrieval components:\n\n| Component | Baseline | Modern | Measured Via |\n|-----------|----------|--------|--------------|\n| **Embedder** | DPR-NQ (2020) | BGE/E5 (2023) | Experiments will show |\n| **Retrieval** | Dense only | Hybrid (BM25 + Dense) | Document Recall@10 |\n| **Reranking** | None | Cross-encoder | Precision improvement |\n| **Evaluation** | Per-question | Comprehensive | 6 HotpotQA metrics |\n\n## ğŸ”§ Learning Philosophy\n\n**Learn by Implementing, Not Just Using**\n\nWhile frameworks like LlamaIndex provide excellent abstractions, we implement from scratch to understand:\n- âœ… How vector embeddings work mathematically\n- âœ… Different similarity metrics (cosine vs inner product vs MIPS)\n- âœ… Trade-offs between sparse and dense representations\n- âœ… Why and when each component improves performance\n- âœ… How to debug and optimize RAG systems\n- âœ… When to use frameworks vs custom implementations\n\n**Progressive Complexity**\n- Start with RAG paper baseline (understand foundations)\n- Analyze failures (learn from mistakes)\n- Add one improvement at a time (measure each contribution)\n- Build complete modern RAG (production-ready system)\n\n**Comparative Learning**\n- Side-by-side comparisons at every step\n- Visualizations showing performance differences\n- Understanding evolution from 2020 to 2024\n\nLet's begin the journey from traditional to modern RAG! ğŸš€"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9277-20f8-469e-9d08-84f822447c13",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install transformers datasets torch sentence-transformers rank-bm25 numpy scikit-learn matplotlib seaborn\n!pip install llama-index  # For comparison later\n\nimport torch\nimport torch.nn.functional as F  # Added for DPR normalization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datasets import load_dataset\nfrom transformers import (\n    DPRQuestionEncoder, DPRContextEncoder,\n    DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer,\n    AutoTokenizer, AutoModelForCausalLM\n)\nfrom sentence_transformers import CrossEncoder\nfrom rank_bm25 import BM25Okapi\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nimport re\nfrom typing import List, Dict, Tuple, Optional\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"âœ… All packages installed and imported successfully!\")\nprint(f\"ğŸ”¥ PyTorch version: {torch.__version__}\")\nprint(f\"ğŸ¯ CUDA available: {torch.cuda.is_available()}\")\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸš€ Using device: {device}\")\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "id": "xmzpsm3jy3f",
   "metadata": {},
   "source": [
    "## ğŸ“Š Data Loading and Preprocessing\n",
    "\n",
    "Let's load the HotpotQA dataset and prepare it for our vector search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vy1ilu1o2tf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HotpotQA dataset\n",
    "print(\"ğŸ”„ Loading HotpotQA dataset...\")\n",
    "dataset = load_dataset('hotpotqa/hotpot_qa', 'distractor')\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "print(f\"ğŸ“Š Dataset loaded successfully!\")\n",
    "print(f\"   Training examples: {len(train_data):,}\")\n",
    "print(f\"   Validation examples: {len(validation_data):,}\")\n",
    "\n",
    "# Take a smaller subset for faster processing during development\n",
    "SAMPLE_SIZE = 100  # Increase this for full evaluation\n",
    "train_sample = train_data.shuffle(seed=42).select(range(min(SAMPLE_SIZE, len(train_data))))\n",
    "val_sample = validation_data.shuffle(seed=42).select(range(min(SAMPLE_SIZE, len(validation_data))))\n",
    "\n",
    "print(f\"ğŸ¯ Working with sample: {len(train_sample)} train, {len(val_sample)} validation\")\n",
    "\n",
    "# Inspect a sample to understand the structure\n",
    "sample_example = train_sample[0]\n",
    "print(\"\\nğŸ“‹ Sample HotpotQA Example Structure:\")\n",
    "print(f\"   Question: {sample_example['question']}\")\n",
    "print(f\"   Answer: {sample_example['answer']}\")\n",
    "print(f\"   Type: {sample_example['type']}\")\n",
    "print(f\"   Level: {sample_example['level']}\")\n",
    "print(f\"   Number of context paragraphs: {len(list(sample_example['context']))}\")\n",
    "print(f\"   Supporting facts: {len(list(sample_example['supporting_facts']))}\")\n",
    "\n",
    "print(\"\\nğŸ” First few context titles:\")\n",
    "context_list = list(sample_example['context'])\n",
    "for i, (title, sentences) in enumerate(context_list[:3]):\n",
    "    print(f\"   {i+1}. {title} ({len(sentences)} sentences)\")\n",
    "\n",
    "print(\"\\nğŸ“ Supporting facts:\")\n",
    "facts_list = list(sample_example['supporting_facts'])\n",
    "for title, sent_idx in facts_list:\n",
    "    print(f\"   - {title}, sentence {sent_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vkckhun492b",
   "metadata": {},
   "outputs": [],
   "source": "# Preprocessing functions for BM25 and DPR\ndef preprocess_text_for_bm25(text):\n    \"\"\"Preprocess text for BM25 sparse retrieval\"\"\"\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    return text.split()\n\ndef extract_passages_from_example(example):\n    \"\"\"\n    Extract individual passages from a SINGLE HotpotQA example\n    \n    KEY INSIGHT FOR HOTPOTQA:\n    - Each example has 10 context paragraphs (2 gold + 8 distractors)\n    - Retrieval happens WITHIN these 10 paragraphs per question\n    - This simulates real-world multihop QA where we filter from a candidate set\n    \"\"\"\n    passages = []\n    passage_metadata = []\n    \n    context_list = list(example['context'])\n    \n    for context_idx, (title, sentences) in enumerate(context_list):\n        # Each sentence becomes a separate passage\n        for sent_idx, sentence in enumerate(sentences):\n            passage_text = sentence.strip()\n            if passage_text:  # Only add non-empty passages\n                passages.append(passage_text)\n                passage_metadata.append({\n                    'title': title,\n                    'context_idx': context_idx,\n                    'sentence_idx': sent_idx,\n                    'full_passage': ' '.join(sentences)  # Full paragraph for context\n                })\n    \n    return passages, passage_metadata\n\ndef simple_answer_extraction(question, retrieved_passages, top_k=3):\n    \"\"\"\n    Simple answer extraction strategy for tutorial purposes\n    \n    Strategy:\n    1. Combine top-k retrieved passages\n    2. Find the shortest span that appears in passages and overlaps with question entities\n    3. For tutorial: just return key entities from top passage\n    \"\"\"\n    if not retrieved_passages:\n        return \"Unable to answer\"\n    \n    # Simple strategy: extract key entities from top passages\n    # Combine top-k passages\n    combined_text = ' '.join(retrieved_passages[:top_k])\n    \n    # Remove question words to find novel information\n    question_words = set(preprocess_text_for_bm25(question))\n    passage_words = preprocess_text_for_bm25(combined_text)\n    \n    # Find candidate answer phrases (simple heuristic: capitalized words or numbers)\n    import re\n    # Look for capitalized phrases (proper nouns) or numbers or years\n    candidates = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b|\\b\\d+\\b', combined_text)\n    \n    if candidates:\n        # Return first candidate that's not in question\n        for candidate in candidates:\n            if candidate.lower() not in [q.lower() for q in question.split()]:\n                return candidate\n    \n    # Fallback: return first few words of top passage\n    words = combined_text.split()\n    return ' '.join(words[:min(5, len(words))])\n\nprint(\"âœ… Preprocessing functions ready!\")\nprint(\"ğŸ“ Functions available:\")\nprint(\"   - preprocess_text_for_bm25(): Clean text for sparse retrieval\")\nprint(\"   - extract_passages_from_example(): Extract passages from ONE example\")\nprint(\"   - simple_answer_extraction(): Generate answer from retrieved passages\")"
  },
  {
   "cell_type": "markdown",
   "id": "mzmswo3z01r",
   "metadata": {},
   "source": "## ğŸ” BM25 Sparse Retrieval Implementation\n\nBM25 is a probabilistic ranking function that scores documents based on query term frequency and document length. It's a traditional sparse method that forms the foundation of modern search engines.\n\n### ğŸ¯ Critical HotpotQA Setup Understanding\n\n**Per-Question Retrieval (Correct Approach):**\n- Each HotpotQA example provides 10 context paragraphs (2 gold + 8 distractors)\n- The task: retrieve the correct 2 documents from these 10 candidates\n- We build a separate BM25 index for each question's context\n- This tests the model's ability to filter signal from noise\n\n**Why NOT Global Corpus Retrieval:**\n- âŒ Building one index from training data and searching from validation won't work\n- âŒ Gold documents for validation questions aren't in the training corpus\n- âŒ Document Recall@10 would be 0% because documents don't exist!\n- âœ… Per-question retrieval matches the actual HotpotQA benchmark setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k8ra2z4boo",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate BM25 with PER-QUESTION retrieval (correct approach for HotpotQA)\nprint(\"ğŸ”„ BM25 Per-Question Retrieval Demonstration\")\nprint(\"=\"*60)\n\n# Get a test example from validation set\ntest_example = val_sample[0]\ntest_question = test_example['question']\n\nprint(f\"ğŸ¯ Test Question: {test_question}\")\nprint(f\"ğŸ¯ Gold Answer: {test_example['answer']}\")\n\n# Extract passages from THIS example's context (10 paragraphs)\nexample_passages, example_metadata = extract_passages_from_example(test_example)\nprint(f\"\\nğŸ“Š Extracted {len(example_passages)} passages from {len(test_example['context'])} context paragraphs\")\n\n# Show context paragraph titles\ncontext_titles = [title for title, _ in test_example['context']]\nprint(f\"\\nğŸ“š Available context paragraphs (2 gold + 8 distractors):\")\nfor i, title in enumerate(context_titles):\n    # Check if this is a gold supporting document\n    gold_titles = set([fact[0] for fact in test_example['supporting_facts']])\n    marker = \"ğŸŸ¢ GOLD\" if title in gold_titles else \"ğŸ”´ DISTRACTOR\"\n    print(f\"   {i+1}. {title} {marker}\")\n\n# Build BM25 index for THIS example's passages\nprint(f\"\\nğŸ—ï¸ Building BM25 index for this example's passages...\")\ntokenized_passages = [preprocess_text_for_bm25(passage) for passage in example_passages]\nbm25 = BM25Okapi(tokenized_passages)\nprint(f\"âœ… BM25 index built with {len(tokenized_passages)} passages\")\n\n# Tokenize query and search\ntest_query_tokens = preprocess_text_for_bm25(test_question)\nprint(f\"\\nğŸ” Query tokens: {test_query_tokens[:10]}...\")\n\n# Get top-k BM25 scores\nk = 10\nbm25_scores = bm25.get_scores(test_query_tokens)\ntop_k_indices = np.argsort(bm25_scores)[::-1][:k]\n\nprint(f\"\\nğŸ“Š Top-{k} BM25 Retrieval Results:\")\nretrieved_titles_bm25 = []\nfor i, idx in enumerate(top_k_indices):\n    score = bm25_scores[idx]\n    passage = example_passages[idx][:100] + \"...\"\n    title = example_metadata[idx]['title']\n    \n    # Mark if it's a gold document\n    is_gold = \"âœ“ GOLD\" if title in gold_titles else \"\"\n    print(f\"   {i+1}. Score: {score:.3f} | {title} {is_gold}\")\n    print(f\"      {passage}\")\n    \n    if title not in retrieved_titles_bm25:\n        retrieved_titles_bm25.append(title)\n\n# Calculate document recall\ngold_titles_list = list(gold_titles)\ndoc_recall = len(set(retrieved_titles_bm25[:k]).intersection(gold_titles)) / len(gold_titles)\nprint(f\"\\nğŸ“ˆ Document Recall@{k}: {doc_recall:.3f} ({len(set(retrieved_titles_bm25[:k]).intersection(gold_titles))}/{len(gold_titles)} gold docs retrieved)\")\n\nprint(\"\\nâœ… BM25 per-question retrieval demonstration complete!\")\nprint(\"ğŸ’¡ Key insight: We retrieve from each question's 10 context paragraphs, not a global corpus!\")"
  },
  {
   "cell_type": "markdown",
   "id": "uw0e3cyeba",
   "metadata": {},
   "source": "## ğŸ¯ SECTION 4: RAG Paper Baseline - Dense Passage Retrieval (2020)\n\n**This implements the EXACT retrieval method from the original RAG paper (Lewis et al., 2020)!**\n\n### ğŸ“„ What is the RAG Paper Baseline?\n\nThe Retrieval-Augmented Generation (RAG) paper by Lewis et al. (2020) introduced a powerful paradigm: combine neural retrieval with neural generation. The retrieval component uses:\n\n- **Dense Passage Retrieval (DPR)** - Bi-encoder architecture with separate encoders for questions and passages\n- **Pre-trained on Natural Questions (NQ)** - Google's single-hop factoid QA dataset  \n- **No fine-tuning** - Frozen retriever weights (this is the baseline configuration)\n- **MIPS (Maximum Inner Product Search)** - Find passages with highest dot product to query vector\n- **Top-k retrieval** - Return k most similar passages based on vector similarity\n\n### ğŸ” DPR IS Vector Search!\n\nDense Passage Retrieval is essentially **vector search** - the same underlying technology used in:\n- OpenAI embeddings + Pinecone/Weaviate/Chroma\n- LlamaIndex vector stores\n- LangChain vector retrievers\n- All modern RAG systems\n\n**The architecture:**\n```\nQuestion â†’ BERT Encoder â†’ 768-dim query vector\nPassages â†’ BERT Encoder â†’ 768-dim passage vectors  \nSimilarity â†’ Dot product (or cosine) between query and passage vectors\nRetrieval â†’ Top-k passages with highest similarity scores\n```\n\n### ğŸ“Š Task Characteristics\n\n**Original RAG Paper Tasks (where DPR-NQ was trained and evaluated):**\n- âœ… Natural Questions: \"Who is the president of France?\"\n- âœ… TriviaQA: \"What year did the Berlin Wall fall?\"  \n- âœ… WebQuestions: \"When was Barack Obama born?\"\n- All are **single-hop** questions requiring one document\n\n**Our Task - HotpotQA (different from training):**\n- â“ Multihop reasoning: \"What year was the director of Inception born?\"\n  - Step 1: Find document about Inception â†’ Learn director is Christopher Nolan\n  - Step 2: Find document about Christopher Nolan â†’ Find birth year 1970\n- Requires **two documents** and reasoning across them\n\n### ğŸ¯ Why Implement This Baseline?\n\n1. **Understand Foundations**: See how RAG paper's retriever actually works\n2. **Learn from Failures**: Observe performance on mismatched task domains\n3. **Appreciate Evolution**: Understand motivations for modern improvements\n4. **Build Intuition**: Know when domain-specific training matters\n\n### ğŸ“ˆ What We'll Measure\n\nWhen we test DPR-NQ on HotpotQA multihop questions, we'll measure:\n- Document Recall@10: Can it find both required documents?\n- Supporting-Fact F1: Sentence-level retrieval accuracy\n- Latency: Encoding and search time\n- Comparison baseline for modern methods\n\nThe experiments will reveal how task mismatch affects performance. This isn't about DPR being \"bad\" - it's about understanding when and why models struggle on out-of-domain tasks.\n\nLet's implement the RAG paper baseline and run experiments!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cpew9hag5o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DPR models (this is vector search!)\n",
    "print(\"ğŸ”„ Loading pre-trained DPR models for vector search...\")\n",
    "\n",
    "# Question encoder (queries â†’ vectors)\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "# Context encoder (passages â†’ vectors) \n",
    "c_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "c_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# Move to device\n",
    "q_encoder.to(device)\n",
    "c_encoder.to(device)\n",
    "\n",
    "print(f\"âœ… DPR models loaded on {device}\")\n",
    "print(\"ğŸ” Vector Search Components Ready:\")\n",
    "print(f\"   - Question Encoder: Transforms questions â†’ 768-dim vectors\")\n",
    "print(f\"   - Context Encoder: Transforms passages â†’ 768-dim vectors\")\n",
    "print(f\"   - Similarity: Cosine similarity / Inner product search\")\n",
    "\n",
    "def encode_questions_dpr(questions, batch_size=32):\n",
    "    \"\"\"Encode questions into dense vectors (Vector Search Step 1)\"\"\"\n",
    "    q_encoder.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch_questions = questions[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize questions\n",
    "            encoded = q_tokenizer(\n",
    "                batch_questions,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            \n",
    "            # Encode to vectors\n",
    "            embeddings = q_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_embeddings.append(embeddings.pooler_output.cpu())\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "def encode_passages_dpr(passages, batch_size=32):\n",
    "    \"\"\"Encode passages into dense vectors (Vector Search Step 2)\"\"\"\n",
    "    c_encoder.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding passages\"):\n",
    "            batch_passages = passages[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize passages\n",
    "            encoded = c_tokenizer(\n",
    "                batch_passages,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            \n",
    "            # Encode to vectors\n",
    "            embeddings = c_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_embeddings.append(embeddings.pooler_output.cpu())\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "def vector_search_dpr(query_embedding, passage_embeddings, passages, metadata, top_k=10):\n",
    "    \"\"\"Perform vector search using cosine similarity (Vector Search Step 3)\"\"\"\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "    passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix (this IS vector search!)\n",
    "    similarities = torch.mm(query_embedding, passage_embeddings.transpose(0, 1))\n",
    "    similarities = similarities.squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Get top-k most similar vectors\n",
    "    top_k_scores, top_k_indices = torch.topk(similarities, min(top_k, len(similarities)))\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(top_k_scores, top_k_indices):\n",
    "        results.append({\n",
    "            'idx': int(idx),\n",
    "            'score': float(score),\n",
    "            'passage': passages[int(idx)],\n",
    "            'title': metadata[int(idx)]['title'],\n",
    "            'metadata': metadata[int(idx)]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\nâœ… DPR Vector Search functions ready!\")\n",
    "print(\"ğŸ¯ Key insight: DPR = Vector Database functionality!\")\n",
    "print(\"   - encode_questions_dpr(): Query â†’ Vector\")\n",
    "print(\"   - encode_passages_dpr(): Documents â†’ Vectors\") \n",
    "print(\"   - vector_search_dpr(): Similarity search (MIPS/Cosine)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i34riu4cie",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate DPR Vector Search with PER-QUESTION retrieval\nprint(\"ğŸ”„ DPR Vector Search Per-Question Demonstration\")\nprint(\"=\"*60)\nprint(\"âš¡ This demonstrates vector search on a per-question basis!\")\n\n# Use the same test example for comparison\nprint(f\"ğŸ¯ Test Question: {test_question}\")\nprint(f\"ğŸ¯ Gold Answer: {test_example['answer']}\")\n\n# Build DPR vector embeddings for THIS example's passages\nprint(f\"\\nğŸ§  Encoding {len(example_passages)} passages into dense vectors...\")\npassage_embeddings = encode_passages_dpr(example_passages, batch_size=16)\n\nprint(f\"âœ… Vector index built!\")\nprint(f\"ğŸ“Š Vector Statistics:\")\nprint(f\"   - Number of vectors: {passage_embeddings.shape[0]}\")\nprint(f\"   - Vector dimension: {passage_embeddings.shape[1]}\")\n\n# Encode query into vector\nprint(f\"\\nğŸ¯ Encoding question into query vector...\")\nquery_embedding = encode_questions_dpr([test_question])\nprint(f\"   Query vector shape: {query_embedding.shape}\")\n\n# Perform vector search\nprint(f\"\\nğŸ” Performing vector similarity search...\")\nvector_results = vector_search_dpr(query_embedding, passage_embeddings, example_passages, example_metadata, top_k=10)\n\nprint(f\"\\nğŸ“Š Top-10 Vector Search Results:\")\nretrieved_titles_dpr = []\nfor i, result in enumerate(vector_results):\n    title = result['title']\n    is_gold = \"âœ“ GOLD\" if title in gold_titles else \"\"\n    print(f\"   {i+1}. Score: {result['score']:.3f} | {title} {is_gold}\")\n    print(f\"      {result['passage'][:100]}...\")\n    \n    if title not in retrieved_titles_dpr:\n        retrieved_titles_dpr.append(title)\n\n# Calculate document recall for DPR\ndoc_recall_dpr = len(set(retrieved_titles_dpr[:10]).intersection(gold_titles)) / len(gold_titles)\nprint(f\"\\nğŸ“ˆ Document Recall@10: {doc_recall_dpr:.3f} ({len(set(retrieved_titles_dpr[:10]).intersection(gold_titles))}/{len(gold_titles)} gold docs retrieved)\")\n\nprint(\"\\nâœ… DPR Vector Search demonstration complete!\")\nprint(\"ğŸ” This demonstrates the core technology behind:\")\nprint(\"   - OpenAI Embeddings + Vector DBs\")\nprint(\"   - LlamaIndex vector stores\") \nprint(\"   - LangChain vector retrievers\")\nprint(\"   - Pinecone, Weaviate, Chroma databases\")\n\nprint(\"\\nğŸ’¡ Key Difference from Production:\")\nprint(\"   - Production: One large vector DB with millions of documents\")\nprint(\"   - HotpotQA: Per-question retrieval from 10 candidate paragraphs\")\nprint(\"   - This setup tests multihop reasoning with controlled distractors\")"
  },
  {
   "cell_type": "markdown",
   "id": "3tq1ppbw4fc",
   "source": "## ğŸ”¬ SECTION 5: Why RAG Baseline Struggles - Multihop Analysis\n\nNow that we've implemented the RAG paper baseline (DPR-NQ), let's analyze **why** it struggles on HotpotQA multihop questions.\n\n### ğŸ¯ Key Question: Why Does Strong Performance on NQ Become Poor on HotpotQA?\n\n**Domain Mismatch in Action:**\n\nThe DPR model was trained on Natural Questions, where:\n- Questions are **single-hop**: \"Who won the 2020 NBA championship?\"\n- Answer requires **ONE document**: Lakers team page\n- Encoder learns: \"Find the most relevant single document\"\n\nHotpotQA requires **multihop reasoning**:\n- Questions span **two+ documents**: \"What position did the 2020 NBA championship MVP play?\"\n- Step 1: Find 2020 NBA championship â†’ Lakers  \n- Step 2: Find Lakers MVP â†’ LeBron James\n- Step 3: Find LeBron's position â†’ Small Forward\n- Encoder needs: \"Find MULTIPLE related but different documents\"\n\n### ğŸ“Š What We'll Analyze\n\n1. **Failure Case Examples**: Show 3-5 questions where DPR-NQ retrieves wrong documents\n2. **Error Categorization**: Classify types of retrieval failures\n3. **Retrieval Overlap**: Visualize retrieved docs vs gold docs\n4. **Patterns**: Identify which question types fail most\n\nThis analysis will motivate our move to modern embedders in Section 6!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oxnd6i7jq19",
   "source": "# Error Analysis Functions for DPR Baseline on Multihop Tasks\n\ndef analyze_retrieval_failure(example, retrieved_titles, method_name=\"DPR-NQ\"):\n    \"\"\"\n    Analyze why retrieval failed for a multihop question\n    \n    Returns failure type and explanation\n    \"\"\"\n    gold_titles = list(set([fact[0] for fact in example['supporting_facts']]))\n    retrieved_set = set(retrieved_titles[:10])  # Top-10\n    gold_set = set(gold_titles)\n    \n    overlap = len(retrieved_set.intersection(gold_set))\n    \n    # Categorize failure type\n    if overlap == len(gold_set):\n        failure_type = \"SUCCESS\"\n        explanation = f\"Retrieved all {len(gold_set)} required documents\"\n    elif overlap == 1 and len(gold_set) == 2:\n        failure_type = \"PARTIAL_RETRIEVAL\"\n        missing = gold_set - retrieved_set\n        explanation = f\"Found 1/2 gold docs. Missing: {list(missing)[0]}\"\n    elif overlap == 0:\n        failure_type = \"COMPLETE_MISS\"\n        explanation = f\"Retrieved 0/{len(gold_set)} gold documents. Retrieved distractors instead.\"\n    else:\n        failure_type = \"PARTIAL_RETRIEVAL\"\n        explanation = f\"Retrieved {overlap}/{len(gold_set)} gold documents\"\n    \n    return {\n        'failure_type': failure_type,\n        'overlap': overlap,\n        'total_required': len(gold_set),\n        'explanation': explanation,\n        'gold_titles': gold_titles,\n        'retrieved_titles': retrieved_titles[:10],\n        'question': example['question'],\n        'answer': example['answer'],\n        'question_type': example['type'],\n        'difficulty': example['level']\n    }\n\ndef categorize_errors(analysis_results):\n    \"\"\"Categorize all errors by type\"\"\"\n    from collections import Counter\n    \n    failure_types = [r['failure_type'] for r in analysis_results]\n    counts = Counter(failure_types)\n    \n    return counts\n\ndef find_failure_examples(analysis_results, failure_type, n=5):\n    \"\"\"Find n examples of a specific failure type\"\"\"\n    examples = [r for r in analysis_results if r['failure_type'] == failure_type]\n    return examples[:n]\n\nprint(\"âœ… Error analysis functions ready!\")\nprint(\"ğŸ“Š Functions available:\")\nprint(\"   - analyze_retrieval_failure(): Categorize individual failures\")\nprint(\"   - categorize_errors(): Count failure types\")\nprint(\"   - find_failure_examples(): Get examples of specific failures\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "96wmin44fhq",
   "source": "# Run Error Analysis on DPR Baseline\nprint(\"ğŸ”¬ Running Failure Analysis on DPR Baseline\")\nprint(\"=\"*60)\n\n# Analyze a subset for demonstration\nanalysis_subset_size = 50\nanalysis_subset = val_sample.select(range(min(analysis_subset_size, len(val_sample))))\n\ndpr_failure_analysis = []\n\nprint(f\"ğŸ“Š Analyzing {len(analysis_subset)} validation examples...\")\nprint()\n\nfor i, example in enumerate(tqdm(analysis_subset, desc=\"Analyzing DPR failures\")):\n    question = example['question']\n    \n    # Extract passages and encode with DPR\n    example_passages, example_metadata = extract_passages_from_example(example)\n    passage_embeddings = encode_passages_dpr(example_passages, batch_size=16)\n    \n    # DPR vector search\n    query_embedding = encode_questions_dpr([question])\n    vector_results = vector_search_dpr(query_embedding, passage_embeddings, \n                                       example_passages, example_metadata, top_k=10)\n    \n    # Extract retrieved titles\n    retrieved_titles = []\n    for result in vector_results:\n        title = result['title']\n        if title not in retrieved_titles:\n            retrieved_titles.append(title)\n    \n    # Analyze this retrieval\n    analysis = analyze_retrieval_failure(example, retrieved_titles, method_name=\"DPR-NQ\")\n    dpr_failure_analysis.append(analysis)\n\n# Categorize all errors\nerror_counts = categorize_errors(dpr_failure_analysis)\n\nprint(\"\\nğŸ“ˆ DPR-NQ Retrieval Performance Analysis\")\nprint(\"=\"*60)\nprint(f\"\\nğŸ¯ Overall Statistics:\")\nprint(f\"   Total examples analyzed: {len(dpr_failure_analysis)}\")\nprint(f\"\\nğŸ“Š Failure Type Distribution:\")\nfor failure_type, count in error_counts.most_common():\n    percentage = (count / len(dpr_failure_analysis)) * 100\n    print(f\"   {failure_type}: {count} ({percentage:.1f}%)\")\n\n# Calculate average document recall\navg_recall = np.mean([a['overlap'] / a['total_required'] for a in dpr_failure_analysis])\nprint(f\"\\nğŸ“ Average Document Recall@10: {avg_recall:.3f}\")\n\nprint(\"\\nğŸ” Example Failures by Type:\")\nprint(\"=\"*60)\n\n# Show examples of each failure type\nfor failure_type in ['COMPLETE_MISS', 'PARTIAL_RETRIEVAL', 'SUCCESS']:\n    examples = find_failure_examples(dpr_failure_analysis, failure_type, n=2)\n    \n    if examples:\n        print(f\"\\nğŸ”¸ {failure_type} Examples:\")\n        for j, ex in enumerate(examples[:2], 1):\n            print(f\"\\n   Example {j}:\")\n            print(f\"   Question: {ex['question'][:100]}...\")\n            print(f\"   Answer: {ex['answer']}\")\n            print(f\"   Type: {ex['question_type']} | Difficulty: {ex['difficulty']}\")\n            print(f\"   Gold docs: {ex['gold_titles']}\")\n            print(f\"   Retrieved: {ex['retrieved_titles'][:3]}...\")\n            print(f\"   {ex['explanation']}\")\n\nprint(\"\\nâœ… Failure analysis complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jngk0r37c3c",
   "source": "## ğŸš€ SECTION 6: Modern Embedder Comparison (2020 â†’ 2023)\n\nWe've seen that DPR-NQ (2020) was trained on single-hop Natural Questions. Let's compare it with a modern embedder to understand the evolution.\n\n### ğŸ“ˆ From 2020 to 2023\n\n**2020: Dense Passage Retrieval (DPR)**\n- First successful dense retrieval for open-domain QA\n- Task-specific training (Natural Questions)\n- Limitation: Domain-specific, requires fine-tuning for new tasks\n\n**2023: BGE (BAAI)**\n- Multi-task training across diverse datasets\n- State-of-the-art on MTEB benchmark\n- General-purpose, no task-specific fine-tuning needed\n\n### ğŸ¯ Simple Comparison\n\nWe'll test 2 embedders on the SAME HotpotQA validation set:\n\n| Model | Year | Training | Design Focus |\n|-------|------|----------|--------------|\n| **DPR-NQ** | 2020 | Natural Questions (single-hop) | Task-specific QA |\n| **BGE-large** | 2023 | Multi-task (diverse datasets) | General-purpose SOTA |\n\n**Questions to answer through experiments:**\n- How does performance differ between 2020 and 2023 models?\n- Does multi-task training help with multihop reasoning?\n- What's the latency trade-off?\n\n### ğŸ”§ Implementation Strategy\n\nWe'll create a **unified embedder interface** that works with both:\n- DPR (separate question/context encoders)\n- SentenceTransformers (unified encoder)\n\nBoth will use the same `vector_search_dpr()` function for fair comparison.\n\n### ğŸ“Š Metrics We'll Measure\n\nFor each embedder:\n1. **Document Recall@10**: Can it find both required documents?\n2. **Latency**: Encoding time\n3. **Improvement**: BGE vs DPR-NQ baseline\n\nLet's implement and run experiments!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cwb3szhol7w",
   "source": "# Unified Embedder Interface for Fair Comparison\n\nfrom sentence_transformers import SentenceTransformer\nimport time\n\nclass UnifiedEmbedder:\n    \"\"\"\n    Unified interface for different embedding models\n    \n    Supports:\n    - DPR (separate question/context encoders)\n    - SentenceTransformers (unified models like BGE)\n    \"\"\"\n    \n    def __init__(self, model_name, model_type='sentence-transformer'):\n        \"\"\"\n        Initialize embedder\n        \n        Args:\n            model_name: Model identifier\n            model_type: 'dpr' or 'sentence-transformer'\n        \"\"\"\n        self.model_name = model_name\n        self.model_type = model_type\n        \n        if model_type == 'dpr':\n            # Use existing DPR encoders (already loaded)\n            self.q_encoder = q_encoder\n            self.c_encoder = c_encoder\n            self.q_tokenizer = q_tokenizer\n            self.c_tokenizer = c_tokenizer\n            print(f\"âœ… Using existing DPR encoders for {model_name}\")\n            \n        elif model_type == 'sentence-transformer':\n            # Load SentenceTransformer model\n            print(f\"ğŸ”„ Loading {model_name}...\")\n            self.model = SentenceTransformer(model_name)\n            if torch.cuda.is_available():\n                self.model = self.model.to(device)\n            print(f\"âœ… Loaded {model_name} on {device}\")\n        \n        else:\n            raise ValueError(f\"Unknown model_type: {model_type}\")\n    \n    def encode_queries(self, queries, batch_size=32, show_progress=False):\n        \"\"\"Encode queries into vectors\"\"\"\n        if self.model_type == 'dpr':\n            return encode_questions_dpr(queries, batch_size)\n        else:\n            # SentenceTransformer\n            embeddings = self.model.encode(\n                queries,\n                batch_size=batch_size,\n                show_progress_bar=show_progress,\n                convert_to_tensor=True,\n                device=device\n            )\n            return embeddings\n    \n    def encode_passages(self, passages, batch_size=32, show_progress=True):\n        \"\"\"Encode passages into vectors\"\"\"\n        if self.model_type == 'dpr':\n            return encode_passages_dpr(passages, batch_size)\n        else:\n            # SentenceTransformer\n            embeddings = self.model.encode(\n                passages,\n                batch_size=batch_size,\n                show_progress_bar=show_progress,\n                convert_to_tensor=True,\n                device=device\n            )\n            return embeddings\n    \n    def search(self, query_embedding, passage_embeddings, passages, metadata, top_k=10):\n        \"\"\"Perform vector similarity search using the detailed vector_search_dpr function\"\"\"\n        # Use the existing vector_search_dpr function (works for any embeddings)\n        return vector_search_dpr(query_embedding, passage_embeddings, passages, metadata, top_k)\n\n# Initialize 2 embedders for comparison\nprint(\"ğŸš€ Initializing Embedders for Comparison\")\nprint(\"=\"*60)\n\nembedders = {}\n\n# 1. DPR-NQ (RAG 2020 baseline) - already loaded\nembedders['DPR-NQ (2020)'] = UnifiedEmbedder('facebook/dpr-nq', model_type='dpr')\n\n# 2. BGE-large (2023 SOTA)\nembedders['BGE-large (2023)'] = UnifiedEmbedder('BAAI/bge-large-en-v1.5', model_type='sentence-transformer')\n\nprint(\"\\nâœ… Both embedders loaded and ready!\")\nprint(\"\\nğŸ“Š Loaded Models:\")\nfor name in embedders.keys():\n    print(f\"   - {name}\")\nprint(\"\\nğŸ” Both use the same vector_search_dpr() function for fair comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "sk1zu4rilp",
   "source": "# Compare 2 Embedders on Same Validation Set\nprint(\"ğŸ EMBEDDER COMPARISON: 2020 vs 2023\")\nprint(\"=\"*70)\n\n# Use a subset for comparison\ncomparison_size = 30\ncomparison_subset = val_sample.select(range(min(comparison_size, len(val_sample))))\n\nprint(f\"ğŸ“Š Evaluating {len(comparison_subset)} validation examples on 2 embedders\")\nprint()\n\n# Store results for each embedder\nembedder_results = {name: [] for name in embedders.keys()}\n\n# Evaluate each embedder\nfor embedder_name, embedder in embedders.items():\n    print(f\"\\n{'='*70}\")\n    print(f\"ğŸ”„ Testing: {embedder_name}\")\n    print(f\"{'='*70}\")\n    \n    start_total = time.time()\n    \n    for i, example in enumerate(tqdm(comparison_subset, desc=f\"Evaluating {embedder_name}\")):\n        question = example['question']\n        gold_supporting_facts = list(example['supporting_facts'])\n        gold_titles = list(set([fact[0] for fact in gold_supporting_facts]))\n        \n        # Extract passages from this example\n        example_passages, example_metadata = extract_passages_from_example(example)\n        \n        # Encode passages\n        start_encode = time.time()\n        passage_embeddings = embedder.encode_passages(example_passages, batch_size=16, show_progress=False)\n        \n        # Encode query\n        query_embedding = embedder.encode_queries([question], show_progress=False)\n        encode_time = time.time() - start_encode\n        \n        # Search using vector_search_dpr function\n        start_search = time.time()\n        if embedder.model_type == 'dpr':\n            # DPR: query_embedding already 2D from encode_questions_dpr\n            search_results = embedder.search(query_embedding, passage_embeddings, \n                                            example_passages, example_metadata, top_k=10)\n        else:\n            # SentenceTransformer: reshape to 2D\n            query_embedding_2d = query_embedding.unsqueeze(0) if query_embedding.dim() == 1 else query_embedding\n            search_results = embedder.search(query_embedding_2d, passage_embeddings,\n                                            example_passages, example_metadata, top_k=10)\n        search_time = time.time() - start_search\n        \n        # Extract retrieved titles\n        retrieved_titles = []\n        for result in search_results:\n            title = result['title']\n            if title not in retrieved_titles:\n                retrieved_titles.append(title)\n        \n        # Calculate Document Recall@10\n        retrieved_set = set(retrieved_titles[:10])\n        gold_set = set(gold_titles)\n        doc_recall = len(retrieved_set.intersection(gold_set)) / len(gold_set)\n        \n        # Store results\n        embedder_results[embedder_name].append({\n            'doc_recall@10': doc_recall,\n            'encode_time': encode_time,\n            'search_time': search_time,\n            'total_time': encode_time + search_time,\n            'retrieved_count': len(retrieved_set.intersection(gold_set)),\n            'gold_count': len(gold_set)\n        })\n    \n    total_time = time.time() - start_total\n    avg_doc_recall = np.mean([r['doc_recall@10'] for r in embedder_results[embedder_name]])\n    avg_latency = np.mean([r['total_time'] for r in embedder_results[embedder_name]])\n    \n    print(f\"\\nâœ… {embedder_name} Results:\")\n    print(f\"   Document Recall@10: {avg_doc_recall:.3f}\")\n    print(f\"   Avg Latency: {avg_latency:.3f}s per question\")\n    print(f\"   Total Time: {total_time:.1f}s\")\n\n# ========== COMPARISON SUMMARY ==========\nprint(f\"\\n\\n{'='*70}\")\nprint(f\"ğŸ“Š FINAL COMPARISON SUMMARY\")\nprint(f\"{'='*70}\\n\")\n\n# Create comparison table\ncomparison_data = []\nbaseline_recall = np.mean([r['doc_recall@10'] for r in embedder_results['DPR-NQ (2020)']])\n\nfor embedder_name in embedders.keys():\n    results = embedder_results[embedder_name]\n    avg_recall = np.mean([r['doc_recall@10'] for r in results])\n    avg_latency = np.mean([r['total_time'] for r in results])\n    improvement = ((avg_recall - baseline_recall) / baseline_recall * 100) if baseline_recall > 0 else 0\n    \n    comparison_data.append({\n        'Model': embedder_name,\n        'Doc Recall@10': f\"{avg_recall:.3f}\",\n        'Improvement': f\"{improvement:+.1f}%\",\n        'Avg Latency (s)': f\"{avg_latency:.3f}\"\n    })\n\n# Print as table\nimport pandas as pd\ndf_comparison = pd.DataFrame(comparison_data)\nprint(df_comparison.to_string(index=False))\n\nprint(f\"\\n\\nğŸ¯ KEY OBSERVATIONS:\")\nprint(f\"=\"*50)\n\nbge_recall = np.mean([r['doc_recall@10'] for r in embedder_results['BGE-large (2023)']])\nimprovement_pct = ((bge_recall - baseline_recall) / baseline_recall * 100) if baseline_recall > 0 else 0\n\nprint(f\"\\n1. **Performance Evolution:**\")\nprint(f\"   â€¢ DPR-NQ (2020): {baseline_recall:.3f} (baseline)\")\nprint(f\"   â€¢ BGE-large (2023): {bge_recall:.3f} ({improvement_pct:+.1f}% change)\")\n\nprint(f\"\\n2. **Why the difference?**\")\nprint(f\"   â€¢ DPR-NQ: Trained only on single-hop Natural Questions\")\nprint(f\"   â€¢ BGE-large: Multi-task training across diverse datasets\")\nprint(f\"   â€¢ Task match matters for retrieval performance\")\n\nprint(f\"\\n3. **Latency:**\")\nfor embedder_name in embedders.keys():\n    avg_latency = np.mean([r['total_time'] for r in embedder_results[embedder_name]])\n    print(f\"   â€¢ {embedder_name}: {avg_latency:.3f}s\")\n\nprint(f\"\\nâœ… Embedder comparison complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lhy01niirzk",
   "source": "## ğŸ¯ SECTION 7: Cross-Encoder Reranking - Precision Boost\n\nWe've improved retrieval by upgrading our embedder (Section 6). Now let's add **reranking** - a second-stage refinement that can improve precision.\n\n### ğŸ” Bi-Encoder vs Cross-Encoder\n\n**Bi-Encoder (What we've used so far):**\n```\nQuestion â†’ Encoder A â†’ Query vector [768]\nPassage â†’ Encoder B â†’ Passage vector [768]\nSimilarity â†’ Dot product of vectors\n```\n- âœ… **Fast**: Pre-compute passage vectors, quick dot product\n- âœ… **Scalable**: Can index millions of passages\n- âŒ **No interaction**: Question and passage never \"see\" each other\n\n**Cross-Encoder (For reranking):**\n```\n[Question + Passage] â†’ Joint Encoder â†’ Relevance score [0-1]\n```\n- âœ… **Accurate**: Question and passage processed together (full attention)\n- âœ… **Better relevance**: Direct relevance modeling\n- âŒ **Slow**: Must process every question-passage pair separately\n- âŒ **Not scalable**: Can't pre-compute, O(n) complexity\n\n### ğŸ¯ Two-Stage Retrieval Strategy\n\nCombine both for best results:\n\n**Stage 1: Bi-Encoder (Fast Retrieval)**\n- Retrieve top-100 candidates from full corpus\n- Uses BGE/E5 embeddings\n- Fast, scalable\n\n**Stage 2: Cross-Encoder (Precise Reranking)**\n- Rerank top-100 â†’ top-10\n- Uses cross-encoder for accurate scoring\n- Slow but only on 100 candidates (acceptable)\n\n### ğŸ“Š What We'll Measure\n\nWe'll compare bi-encoder retrieval with and without reranking:\n- Document Recall@10: Does reranking improve document selection?\n- Rank changes: Which passages move up/down?\n- Latency impact: Cost of reranking stage\n- Precision@k: Quality of top-k results\n\n### ğŸ”§ Implementation\n\nWe'll use `BAAI/bge-reranker-large` - current SOTA reranker:\n- Trained specifically for reranking\n- Optimized for question-passage relevance\n- Compatible with BGE embeddings (but works with any)\n\nExperiments will show whether the latency cost is worth the accuracy gain.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "v6du13c4kg",
   "source": "# Load Cross-Encoder Reranker\nprint(\"ğŸ”„ Loading Cross-Encoder Reranker...\")\n\nfrom sentence_transformers import CrossEncoder\n\n# Load BGE reranker (current SOTA)\nreranker = CrossEncoder('BAAI/bge-reranker-large')\n\nprint(\"âœ… Cross-encoder reranker loaded!\")\nprint(f\"   Model: BAAI/bge-reranker-large\")\nprint(f\"   Purpose: Rerank retrieved passages for better precision\")\n\ndef rerank_passages(question, search_results, top_k=10, rerank_from_k=100):\n    \"\"\"\n    Rerank passages using cross-encoder\n    \n    Args:\n        question: Query string\n        search_results: List of search results from bi-encoder\n        top_k: Number of final results to return\n        rerank_from_k: Number of candidates to rerank\n    \n    Returns:\n        Reranked search results (top_k)\n    \"\"\"\n    # Take top rerank_from_k candidates from bi-encoder\n    candidates = search_results[:min(rerank_from_k, len(search_results))]\n    \n    # Prepare question-passage pairs for cross-encoder\n    pairs = [[question, result['passage']] for result in candidates]\n    \n    # Get cross-encoder scores\n    ce_scores = reranker.predict(pairs)\n    \n    # Combine original results with new scores\n    for i, result in enumerate(candidates):\n        result['rerank_score'] = float(ce_scores[i])\n        result['original_score'] = result['score']  # Keep bi-encoder score\n        result['original_rank'] = i + 1\n    \n    # Sort by rerank scores\n    reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n    \n    # Return top-k\n    return reranked[:top_k]\n\nprint(\"\\nâœ… Reranking function ready!\")\nprint(\"ğŸ“Š Usage: rerank_passages(question, search_results, top_k=10)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "k29famyw0tm",
   "source": "# Demonstrate Reranking Impact\nprint(\"ğŸ¯ RERANKING DEMONSTRATION\")\nprint(\"=\"*70)\n\n# Select a test example\ndemo_example = val_sample[0]\ndemo_question = demo_example['question']\ndemo_gold_titles = list(set([fact[0] for fact in demo_example['supporting_facts']]))\n\nprint(f\"Question: {demo_question}\")\nprint(f\"Gold documents: {demo_gold_titles}\")\nprint()\n\n# Extract passages\ndemo_passages, demo_metadata = extract_passages_from_example(demo_example)\n\n# Use best embedder (BGE-large)\nbest_embedder = embedders['BGE-large (2023)']\n\n# Stage 1: Bi-encoder retrieval (top-20)\nprint(\"ğŸ“ STAGE 1: Bi-Encoder Retrieval (BGE-large)\")\nprint(\"-\"*70)\n\npassage_embeddings = best_embedder.encode_passages(demo_passages, batch_size=16, show_progress=False)\nquery_embedding = best_embedder.encode_queries([demo_question], show_progress=False)\n\n# Reshape if needed\nif query_embedding.dim() == 1:\n    query_embedding = query_embedding.unsqueeze(0)\n\ninitial_results = best_embedder.search(query_embedding, passage_embeddings, \n                                       demo_passages, demo_metadata, top_k=20)\n\nprint(f\"\\nTop-10 results from bi-encoder:\")\nbi_encoder_titles = []\nfor i, result in enumerate(initial_results[:10]):\n    title = result['title']\n    is_gold = \"âœ… GOLD\" if title in demo_gold_titles else \"\"\n    print(f\"   {i+1}. Score: {result['score']:.3f} | {title} {is_gold}\")\n    if title not in bi_encoder_titles:\n        bi_encoder_titles.append(title)\n\nbi_encoder_recall = len(set(bi_encoder_titles[:10]).intersection(demo_gold_titles)) / len(demo_gold_titles)\nprint(f\"\\nğŸ“Š Bi-Encoder Document Recall@10: {bi_encoder_recall:.3f}\")\n\n# Stage 2: Cross-encoder reranking\nprint(f\"\\nğŸ“ STAGE 2: Cross-Encoder Reranking\")\nprint(\"-\"*70)\n\nreranked_results = rerank_passages(demo_question, initial_results, top_k=10, rerank_from_k=20)\n\nprint(f\"\\nTop-10 results after reranking:\")\nreranked_titles = []\nrank_changes = []\n\nfor i, result in enumerate(reranked_results[:10]):\n    title = result['title']\n    is_gold = \"âœ… GOLD\" if title in demo_gold_titles else \"\"\n    rank_change = result['original_rank'] - (i + 1)  # Positive = moved up\n    \n    if rank_change > 0:\n        change_indicator = f\"â†‘{rank_change}\"\n    elif rank_change < 0:\n        change_indicator = f\"â†“{abs(rank_change)}\"\n    else:\n        change_indicator = \"=\"\n    \n    print(f\"   {i+1}. Rerank: {result['rerank_score']:.3f} | Original: {result['original_score']:.3f} | {title} {is_gold} ({change_indicator})\")\n    \n    if title not in reranked_titles:\n        reranked_titles.append(title)\n    \n    rank_changes.append(rank_change)\n\nreranked_recall = len(set(reranked_titles[:10]).intersection(demo_gold_titles)) / len(demo_gold_titles)\nprint(f\"\\nğŸ“Š After Reranking Document Recall@10: {reranked_recall:.3f}\")\n\n# Show improvement\nimprovement = reranked_recall - bi_encoder_recall\nprint(f\"\\n{'='*70}\")\nprint(f\"ğŸ“ˆ RERANKING IMPACT:\")\nprint(f\"   Before reranking: {bi_encoder_recall:.3f}\")\nprint(f\"   After reranking:  {reranked_recall:.3f}\")\nprint(f\"   Improvement:      {improvement:+.3f} ({(improvement/bi_encoder_recall*100):+.1f}%)\" if bi_encoder_recall > 0 else \"   Improvement:      N/A\")\n\nprint(f\"\\nğŸ” Observations:\")\nprint(f\"   â€¢ Cross-encoder reordered {sum(1 for c in rank_changes if c != 0)} passages\")\nprint(f\"   â€¢ Gold documents moved up: {sum(1 for i, r in enumerate(reranked_results[:10]) if r['title'] in demo_gold_titles and rank_changes[i] > 0)}\")\nprint(f\"   â€¢ Reranking provides more accurate relevance scoring\")\n\nprint(f\"\\nâœ… Reranking demonstration complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2mjdlsntyce",
   "source": "## âš¡ SECTION 8: Hybrid Search - Best of Both Worlds\n\nWe've seen two retrieval paradigms:\n- **BM25 (Sparse)**: Keyword matching, exact term overlap\n- **Dense (Embeddings)**: Semantic similarity, meaning-based\n\nWhat if we **combine both**? This is hybrid search - leveraging complementary strengths!\n\n### ğŸ¯ Why Hybrid Search?\n\n**BM25 Strengths:**\n- âœ… Exact keyword matching (great for entities, names, dates)\n- âœ… Fast, interpretable\n- âœ… Works well on entity-heavy questions\n- âŒ Misses semantic similarity\n\n**Dense Retrieval Strengths:**\n- âœ… Semantic understanding (handles paraphrases, synonyms)\n- âœ… Captures meaning beyond exact words\n- âœ… Better for conceptual questions\n- âŒ Can miss exact entity matches\n\n**Hybrid = Combine Both!**\n- Use BM25 for entity recall\n- Use dense for semantic relevance\n- Weighted fusion of scores\n- Potentially best of both worlds!\n\n### ğŸ“Š Hybrid Search Formula\n\nFor each passage, compute a combined score:\n\n```\nfinal_score = Î± Ã— normalized_bm25_score + (1-Î±) Ã— normalized_dense_score\n```\n\nWhere:\n- Î± = weight for BM25 (typically 0.2-0.4)\n- (1-Î±) = weight for dense (typically 0.6-0.8)\n- Normalization ensures scores are comparable\n\n### ğŸ”§ Implementation Strategy\n\n1. **Retrieve with both methods** separately\n   - BM25: Get top-100 with BM25 scores\n   - Dense: Get top-100 with cosine similarity scores\n\n2. **Normalize scores** to [0, 1] range\n   - Min-max normalization\n   - Makes scores comparable\n\n3. **Fuse scores** with weighted combination\n   - Experiment with different Î± weights\n   - Common: Î±=0.3 (30% BM25, 70% dense)\n\n4. **Rerank** with cross-encoder (optional but recommended)\n   - Apply cross-encoder to top-100 hybrid results\n   - Final top-10 selection\n\n### ğŸ“Š What We'll Measure\n\nWe'll compare three approaches on the same questions:\n1. **BM25 only**: Pure sparse retrieval\n2. **Dense only**: Pure semantic retrieval  \n3. **Hybrid**: Weighted combination\n\nMetrics:\n- Document Recall@10 for each approach\n- Which question types benefit from hybrid?\n- Score distribution analysis\n- Best weight combination (Î± tuning)\n\nThe experiments will show whether combining methods provides robustness across different question types.\n\nLet's implement and measure!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "re3f8zfi2iq",
   "source": "# Hybrid Search Implementation\n\ndef normalize_scores(scores):\n    \"\"\"Normalize scores to [0, 1] range using min-max normalization\"\"\"\n    if len(scores) == 0:\n        return scores\n    \n    scores_array = np.array(scores)\n    min_score = scores_array.min()\n    max_score = scores_array.max()\n    \n    if max_score == min_score:\n        return np.ones_like(scores_array)\n    \n    normalized = (scores_array - min_score) / (max_score - min_score)\n    return normalized\n\ndef hybrid_search(question, passages, metadata, embedder, \n                 bm25_weight=0.3, dense_weight=0.7, top_k=10):\n    \"\"\"\n    Hybrid search combining BM25 (sparse) and dense retrieval\n    \n    Args:\n        question: Query string\n        passages: List of passage texts\n        metadata: List of metadata dicts for passages\n        embedder: UnifiedEmbedder instance\n        bm25_weight: Weight for BM25 scores (default: 0.3)\n        dense_weight: Weight for dense scores (default: 0.7)\n        top_k: Number of results to return\n    \n    Returns:\n        List of search results with hybrid scores\n    \"\"\"\n    # Validate weights\n    assert abs(bm25_weight + dense_weight - 1.0) < 1e-6, \"Weights must sum to 1.0\"\n    \n    # 1. BM25 Retrieval\n    tokenized_passages = [preprocess_text_for_bm25(p) for p in passages]\n    bm25 = BM25Okapi(tokenized_passages)\n    query_tokens = preprocess_text_for_bm25(question)\n    bm25_scores = bm25.get_scores(query_tokens)\n    \n    # 2. Dense Retrieval\n    passage_embeddings = embedder.encode_passages(passages, batch_size=16, show_progress=False)\n    query_embedding = embedder.encode_queries([question], show_progress=False)\n    \n    # Reshape if needed\n    if query_embedding.dim() == 1:\n        query_embedding = query_embedding.unsqueeze(0)\n    \n    dense_results = embedder.search(query_embedding, passage_embeddings, \n                                   passages, metadata, top_k=len(passages))\n    \n    # Extract dense scores\n    dense_scores = np.array([r['score'] for r in dense_results])\n    \n    # 3. Normalize both sets of scores\n    bm25_scores_norm = normalize_scores(bm25_scores)\n    dense_scores_norm = normalize_scores(dense_scores)\n    \n    # 4. Create mapping from passage index to dense score\n    dense_score_map = {}\n    for i, result in enumerate(dense_results):\n        idx = result['idx']\n        dense_score_map[idx] = dense_scores_norm[i]\n    \n    # 5. Combine scores\n    hybrid_results = []\n    for idx in range(len(passages)):\n        bm25_score_norm = bm25_scores_norm[idx]\n        dense_score_norm = dense_score_map.get(idx, 0.0)\n        \n        # Weighted combination\n        hybrid_score = bm25_weight * bm25_score_norm + dense_weight * dense_score_norm\n        \n        hybrid_results.append({\n            'idx': idx,\n            'passage': passages[idx],\n            'metadata': metadata[idx],\n            'title': metadata[idx]['title'],\n            'hybrid_score': float(hybrid_score),\n            'bm25_score': float(bm25_scores[idx]),\n            'dense_score': float(dense_score_map.get(idx, 0.0)),\n            'bm25_score_norm': float(bm25_score_norm),\n            'dense_score_norm': float(dense_score_norm)\n        })\n    \n    # 6. Sort by hybrid score\n    hybrid_results_sorted = sorted(hybrid_results, key=lambda x: x['hybrid_score'], reverse=True)\n    \n    # 7. Return top-k\n    return hybrid_results_sorted[:top_k]\n\nprint(\"âœ… Hybrid search implementation ready!\")\nprint(\"ğŸ“Š Combines BM25 (sparse) + Dense (semantic)\")\nprint(f\"   Default weights: BM25={0.3}, Dense={0.7}\")\nprint(f\"   Usage: hybrid_search(question, passages, metadata, embedder)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6dpoml00z9",
   "source": "# Demonstrate Hybrid Search vs BM25 vs Dense\nprint(\"ğŸ”¬ HYBRID SEARCH COMPARISON\")\nprint(\"=\"*70)\nprint(\"Comparing: BM25-only vs Dense-only vs Hybrid (BM25 + Dense)\")\nprint()\n\n# Use same demo example\nhybrid_demo_example = val_sample[1]  # Different example for variety\nhybrid_question = hybrid_demo_example['question']\nhybrid_gold_titles = list(set([fact[0] for fact in hybrid_demo_example['supporting_facts']]))\n\nprint(f\"Question: {hybrid_question}\")\nprint(f\"Gold documents: {hybrid_gold_titles}\")\nprint()\n\n# Extract passages\nhybrid_passages, hybrid_metadata = extract_passages_from_example(hybrid_demo_example)\n\n# Use BGE embedder\nbge_embedder = embedders['BGE-large (2023)']\n\n# Method 1: BM25 Only\nprint(\"ğŸ“ METHOD 1: BM25 Only (Sparse)\")\nprint(\"-\"*70)\n\ntokenized = [preprocess_text_for_bm25(p) for p in hybrid_passages]\nbm25_model = BM25Okapi(tokenized)\nquery_tokens = preprocess_text_for_bm25(hybrid_question)\nbm25_only_scores = bm25_model.get_scores(query_tokens)\nbm25_top_indices = np.argsort(bm25_only_scores)[::-1][:10]\n\nbm25_only_titles = []\nfor i, idx in enumerate(bm25_top_indices):\n    title = hybrid_metadata[idx]['title']\n    is_gold = \"âœ…\" if title in hybrid_gold_titles else \"\"\n    print(f\"   {i+1}. Score: {bm25_only_scores[idx]:.3f} | {title} {is_gold}\")\n    if title not in bm25_only_titles:\n        bm25_only_titles.append(title)\n\nbm25_only_recall = len(set(bm25_only_titles).intersection(hybrid_gold_titles)) / len(hybrid_gold_titles)\nprint(f\"\\nğŸ“Š BM25-only Doc Recall@10: {bm25_only_recall:.3f}\")\n\n# Method 2: Dense Only\nprint(f\"\\nğŸ“ METHOD 2: Dense Only (BGE Embeddings)\")\nprint(\"-\"*70)\n\npassage_embs = bge_embedder.encode_passages(hybrid_passages, batch_size=16, show_progress=False)\nquery_emb = bge_embedder.encode_queries([hybrid_question], show_progress=False)\n\nif query_emb.dim() == 1:\n    query_emb = query_emb.unsqueeze(0)\n\ndense_only_results = bge_embedder.search(query_emb, passage_embs, \n                                         hybrid_passages, hybrid_metadata, top_k=10)\n\ndense_only_titles = []\nfor i, result in enumerate(dense_only_results):\n    title = result['title']\n    is_gold = \"âœ…\" if title in hybrid_gold_titles else \"\"\n    print(f\"   {i+1}. Score: {result['score']:.3f} | {title} {is_gold}\")\n    if title not in dense_only_titles:\n        dense_only_titles.append(title)\n\ndense_only_recall = len(set(dense_only_titles).intersection(hybrid_gold_titles)) / len(hybrid_gold_titles)\nprint(f\"\\nğŸ“Š Dense-only Doc Recall@10: {dense_only_recall:.3f}\")\n\n# Method 3: Hybrid\nprint(f\"\\nğŸ“ METHOD 3: Hybrid (30% BM25 + 70% Dense)\")\nprint(\"-\"*70)\n\nhybrid_only_results = hybrid_search(hybrid_question, hybrid_passages, hybrid_metadata, \n                                   bge_embedder, bm25_weight=0.3, dense_weight=0.7, top_k=10)\n\nhybrid_only_titles = []\nfor i, result in enumerate(hybrid_only_results):\n    title = result['title']\n    is_gold = \"âœ…\" if title in hybrid_gold_titles else \"\"\n    print(f\"   {i+1}. Hybrid: {result['hybrid_score']:.3f} | BM25: {result['bm25_score_norm']:.3f} | Dense: {result['dense_score_norm']:.3f} | {title} {is_gold}\")\n    if title not in hybrid_only_titles:\n        hybrid_only_titles.append(title)\n\nhybrid_only_recall = len(set(hybrid_only_titles).intersection(hybrid_gold_titles)) / len(hybrid_gold_titles)\nprint(f\"\\nğŸ“Š Hybrid Doc Recall@10: {hybrid_only_recall:.3f}\")\n\n# Summary Comparison\nprint(f\"\\n{'='*70}\")\nprint(f\"ğŸ“ˆ COMPARISON SUMMARY\")\nprint(f\"{'='*70}\")\nprint(f\"\\n   BM25-only:   {bm25_only_recall:.3f}\")\nprint(f\"   Dense-only:  {dense_only_recall:.3f}\")\nprint(f\"   Hybrid:      {hybrid_only_recall:.3f}\")\n\nbest_method = max([\n    (\"BM25-only\", bm25_only_recall),\n    (\"Dense-only\", dense_only_recall),\n    (\"Hybrid\", hybrid_only_recall)\n], key=lambda x: x[1])\n\nprint(f\"\\nğŸ† Best performer: {best_method[0]} ({best_method[1]:.3f})\")\n\nprint(f\"\\nğŸ’¡ Key Insights:\")\nif hybrid_only_recall >= max(bm25_only_recall, dense_only_recall):\n    print(f\"   â€¢ Hybrid search leverages strengths of both methods\")\n    print(f\"   â€¢ BM25 helps with entity/keyword matching\")\n    print(f\"   â€¢ Dense helps with semantic understanding\")\nprint(f\"   â€¢ The best approach often depends on question type\")\nprint(f\"   â€¢ Hybrid provides robustness across different question types\")\n\nprint(f\"\\nâœ… Hybrid search demonstration complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x7u4jsy6ir",
   "metadata": {},
   "source": [
    "## ğŸš€ LlamaIndex Comparison: Framework vs Custom Implementation\n",
    "\n",
    "Let's compare our custom vector search implementation with LlamaIndex to show how modern frameworks abstract these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6l00hcjvdjq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex Implementation Example (for comparison)\n",
    "print(\"ğŸš€ LlamaIndex Vector Search Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"ğŸ“š Our Custom Implementation (learning-focused):\")\n",
    "print(\"   1. Manual passage extraction from HotpotQA\")\n",
    "print(\"   2. Explicit DPR model loading and tokenization\")\n",
    "print(\"   3. Custom vector encoding functions\")\n",
    "print(\"   4. Manual cosine similarity computation\")\n",
    "print(\"   5. Custom retrieval and ranking logic\")\n",
    "print(\"   6. Full control over similarity metrics\")\n",
    "\n",
    "print(\"\\nğŸ—ï¸ How LlamaIndex Would Abstract This:\")\n",
    "print(\"   1. SimpleDirectoryReader() - automatic document loading\")\n",
    "print(\"   2. VectorStoreIndex.from_documents() - auto-embedding\")\n",
    "print(\"   3. Built-in similarity search\")\n",
    "print(\"   4. Automatic retrieval and generation\")\n",
    "print(\"   5. Multiple vector database backends\")\n",
    "print(\"   6. Pre-configured pipelines\")\n",
    "\n",
    "print(\"\\nğŸ’¡ LlamaIndex Equivalent Code (pseudo-code):\")\n",
    "print(\"\"\"\n",
    "```python\n",
    "# LlamaIndex - High-level abstraction\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# Load documents (abstracts our manual extraction)\n",
    "documents = SimpleDirectoryReader('data/').load_data()\n",
    "\n",
    "# Create vector index (abstracts our manual DPR encoding)\n",
    "embed_model = HuggingFaceEmbedding(model_name='facebook/dpr-ctx_encoder-single-nq-base')\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Query (abstracts our manual vector search)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query('What is the question?')\n",
    "```\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ¯ Key Learning Points:\")\n",
    "print(\"   âœ… Our implementation: Understand the underlying math and operations\")\n",
    "print(\"   âœ… LlamaIndex: Production-ready with optimizations and abstractions\")\n",
    "print(\"   âœ… Both use the SAME vector search concepts:\")\n",
    "print(\"      - Document â†’ Vector encoding\")\n",
    "print(\"      - Query â†’ Vector encoding\") \n",
    "print(\"      - Similarity search (cosine/inner product)\")\n",
    "print(\"      - Top-k retrieval\")\n",
    "\n",
    "print(\"\\nğŸ” Why Learn Custom Implementation First:\")\n",
    "print(\"   1. Understand vector search mathematics\")\n",
    "print(\"   2. Debug and optimize retrieval performance\")\n",
    "print(\"   3. Implement custom similarity metrics\")\n",
    "print(\"   4. Adapt to specific domain requirements\")\n",
    "print(\"   5. Build domain-specific evaluation metrics\")\n",
    "\n",
    "print(\"\\nâš¡ When to Use LlamaIndex:\")\n",
    "print(\"   1. Rapid prototyping and production deployment\")\n",
    "print(\"   2. Standard RAG pipelines without customization\")\n",
    "print(\"   3. Multiple vector database backend support\")\n",
    "print(\"   4. Built-in optimization and caching\")\n",
    "print(\"   5. Integration with LLM frameworks\")\n",
    "\n",
    "print(\"\\nğŸ† Best Practice: Learn fundamentals first, then use frameworks!\")\n",
    "print(\"ğŸ“Š Our approach: Custom â†’ Framework comparison â†’ Production choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56y6w1u685",
   "metadata": {},
   "source": [
    "## ğŸ“Š Evaluation Framework\n",
    "\n",
    "Let's implement our evaluation framework using the 6 chosen metrics for HotpotQA multihop reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrtrumkkxhs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation framework from @03_Evaluation_Code.ipynb\n",
    "class HotpotQAEvaluator:\n",
    "    \"\"\"Comprehensive evaluator for HotpotQA multihop reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def normalize_answer(self, text):\n",
    "        \"\"\"Normalize answer text for comparison\"\"\"\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove articles\n",
    "        text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def answer_f1_score(self, prediction, ground_truth):\n",
    "        \"\"\"Calculate F1 score between prediction and ground truth\"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        pred_tokens = self.normalize_answer(prediction).split()\n",
    "        gold_tokens = self.normalize_answer(ground_truth).split()\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "            return 1.0\n",
    "        if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        common_tokens = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "        num_same = sum(common_tokens.values())\n",
    "        \n",
    "        if num_same == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(gold_tokens)\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    def answer_exact_match(self, prediction, ground_truth):\n",
    "        \"\"\"Calculate exact match score\"\"\"\n",
    "        return float(self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n",
    "    \n",
    "    def document_recall_at_k(self, retrieved_titles, gold_titles, k=10):\n",
    "        \"\"\"Calculate document recall@k\"\"\"\n",
    "        if len(gold_titles) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        retrieved_k = set(retrieved_titles[:k])\n",
    "        gold_set = set(gold_titles)\n",
    "        \n",
    "        return len(retrieved_k.intersection(gold_set)) / len(gold_set)\n",
    "    \n",
    "    def supporting_fact_f1(self, predicted_facts, gold_facts):\n",
    "        \"\"\"Calculate supporting facts F1 score\"\"\"\n",
    "        if len(gold_facts) == 0:\n",
    "            return 1.0 if len(predicted_facts) == 0 else 0.0\n",
    "        \n",
    "        pred_set = set(predicted_facts)\n",
    "        gold_set = set(gold_facts)\n",
    "        \n",
    "        if len(pred_set) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = pred_set.intersection(gold_set)\n",
    "        precision = len(intersection) / len(pred_set)\n",
    "        recall = len(intersection) / len(gold_set)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    def joint_exact_match(self, pred_answer, gold_answer, pred_facts, gold_facts):\n",
    "        \"\"\"Calculate joint exact match (answer + supporting facts)\"\"\"\n",
    "        answer_em = self.answer_exact_match(pred_answer, gold_answer)\n",
    "        facts_em = 1.0 if set(pred_facts) == set(gold_facts) else 0.0\n",
    "        \n",
    "        return float(answer_em == 1.0 and facts_em == 1.0)\n",
    "    \n",
    "    def evaluate_single(self, prediction_dict, gold_data, k=10, processing_time=None):\n",
    "        \"\"\"Evaluate a single prediction against gold data\"\"\"\n",
    "        \n",
    "        # Extract predictions\n",
    "        pred_answer = prediction_dict.get('answer', '')\n",
    "        pred_titles = prediction_dict.get('retrieved_titles', [])\n",
    "        pred_facts = prediction_dict.get('supporting_facts', [])\n",
    "        \n",
    "        # Extract gold data\n",
    "        gold_answer = gold_data.get('answer', '')\n",
    "        gold_facts = []\n",
    "        gold_titles = []\n",
    "        \n",
    "        # Extract gold supporting facts and titles\n",
    "        if 'supporting_facts' in gold_data:\n",
    "            facts_list = list(gold_data['supporting_facts'])\n",
    "            for title, sent_id in facts_list:\n",
    "                gold_facts.append((title, sent_id))\n",
    "                if title not in gold_titles:\n",
    "                    gold_titles.append(title)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'answer_f1': self.answer_f1_score(pred_answer, gold_answer),\n",
    "            'answer_em': self.answer_exact_match(pred_answer, gold_answer),\n",
    "            'document_recall@k': self.document_recall_at_k(pred_titles, gold_titles, k),\n",
    "            'supporting_fact_f1': self.supporting_fact_f1(pred_facts, gold_facts),\n",
    "            'joint_em': self.joint_exact_match(pred_answer, gold_answer, pred_facts, gold_facts),\n",
    "            'latency': processing_time if processing_time is not None else 0.0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = HotpotQAEvaluator()\n",
    "print(\"âœ… HotpotQA Evaluation Framework Ready!\")\n",
    "print(\"ğŸ“Š Available metrics:\")\n",
    "print(\"   1. Answer F1 Score\")\n",
    "print(\"   2. Answer Exact Match\")  \n",
    "print(\"   3. Document Recall@k\")\n",
    "print(\"   4. Supporting-Fact F1\")\n",
    "print(\"   5. Joint Exact Match\")\n",
    "print(\"   6. Latency (processing time)\")\n",
    "\n",
    "# Test with sample data\n",
    "sample_prediction = {\n",
    "    'answer': 'test answer',\n",
    "    'retrieved_titles': ['Title 1', 'Title 2', 'Title 3'],\n",
    "    'supporting_facts': [('Title 1', 0), ('Title 2', 1)]\n",
    "}\n",
    "\n",
    "sample_gold = {\n",
    "    'answer': 'test answer',\n",
    "    'supporting_facts': [('Title 1', 0), ('Title 2', 1)]\n",
    "}\n",
    "\n",
    "test_metrics = evaluator.evaluate_single(sample_prediction, sample_gold, k=10, processing_time=0.5)\n",
    "print(f\"\\nğŸ§ª Test evaluation result: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlxxdl1s36c",
   "metadata": {},
   "source": [
    "## ğŸƒâ€â™‚ï¸ Run Complete Pipeline and Compare Methods\n",
    "\n",
    "Now let's run both BM25 (sparse) and DPR (vector search) on a subset of validation data and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7kesw1akrt5",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive evaluation on validation set with PER-QUESTION retrieval\nprint(\"ğŸ”¬ COMPREHENSIVE EVALUATION: BM25 vs DPR Vector Search\")\nprint(\"=\"* 60)\nprint(\"âœ… Using CORRECT per-question retrieval approach for HotpotQA!\")\nprint()\n\n# Use a subset of validation data for evaluation\neval_size = 20  # Adjust based on compute resources\nval_subset = val_sample.select(range(min(eval_size, len(val_sample))))\n\nprint(f\"ğŸ“Š Evaluating on {len(val_subset)} validation examples\")\n\n# Initialize results storage\nresults = {\n    'BM25': [],\n    'DPR_Vector_Search': []\n}\n\n# Evaluate each method\nfor i, example in enumerate(val_subset):\n    question = example['question']\n    gold_answer = example['answer']\n    \n    # Extract gold supporting docs and facts\n    gold_supporting_facts = list(example['supporting_facts'])\n    gold_titles = list(set([fact[0] for fact in gold_supporting_facts]))\n    \n    gold_data = {\n        'answer': gold_answer,\n        'supporting_facts': gold_supporting_facts\n    }\n    \n    print(f\"\\nğŸ“ Example {i+1}/{len(val_subset)}\")\n    print(f\"   Question: {question[:80]}...\")\n    print(f\"   Gold answer: {gold_answer}\")\n    print(f\"   Gold documents: {gold_titles}\")\n    \n    # Extract passages from THIS example's 10 context paragraphs\n    example_passages, example_metadata = extract_passages_from_example(example)\n    \n    # ========== BM25 Evaluation ==========\n    print(f\"\\n   ğŸ” BM25 Sparse Retrieval:\")\n    start_time = time.time()\n    \n    # Build BM25 index for this example\n    tokenized_passages = [preprocess_text_for_bm25(p) for p in example_passages]\n    bm25 = BM25Okapi(tokenized_passages)\n    \n    # BM25 search\n    query_tokens = preprocess_text_for_bm25(question)\n    bm25_scores = bm25.get_scores(query_tokens)\n    top_k_indices = np.argsort(bm25_scores)[::-1][:10]\n    \n    # Extract retrieved titles and facts\n    retrieved_titles = []\n    retrieved_facts = []\n    retrieved_passages_text = []\n    \n    for idx in top_k_indices:\n        title = example_metadata[idx]['title']\n        if title not in retrieved_titles:\n            retrieved_titles.append(title)\n        # Add sentence-level facts\n        retrieved_facts.append((title, example_metadata[idx]['sentence_idx']))\n        retrieved_passages_text.append(example_passages[idx])\n    \n    # Generate answer using simple extraction\n    bm25_answer = simple_answer_extraction(question, retrieved_passages_text, top_k=3)\n    \n    bm25_time = time.time() - start_time\n    \n    bm25_prediction = {\n        'answer': bm25_answer,\n        'retrieved_titles': retrieved_titles,\n        'supporting_facts': retrieved_facts[:5]  # Top 5 facts\n    }\n    \n    bm25_metrics = evaluator.evaluate_single(bm25_prediction, gold_data, k=10, processing_time=bm25_time)\n    results['BM25'].append(bm25_metrics)\n    \n    print(f\"      Answer: {bm25_answer}\")\n    print(f\"      Retrieved docs: {retrieved_titles[:3]}...\")\n    print(f\"      Document Recall@10: {bm25_metrics['document_recall@k']:.3f}\")\n    print(f\"      Latency: {bm25_time:.3f}s\")\n    \n    # ========== DPR Vector Search Evaluation ==========\n    print(f\"\\n   ğŸ§  DPR Vector Search:\")\n    start_time = time.time()\n    \n    # Build DPR embeddings for this example\n    passage_embeddings = encode_passages_dpr(example_passages, batch_size=16)\n    \n    # Vector search\n    query_embedding = encode_questions_dpr([question])\n    vector_results = vector_search_dpr(query_embedding, passage_embeddings, example_passages, example_metadata, top_k=10)\n    \n    # Extract retrieved titles and facts\n    dpr_titles = []\n    dpr_facts = []\n    dpr_passages_text = []\n    \n    for result in vector_results:\n        title = result['title']\n        if title not in dpr_titles:\n            dpr_titles.append(title)\n        dpr_facts.append((title, result['metadata']['sentence_idx']))\n        dpr_passages_text.append(result['passage'])\n    \n    # Generate answer using simple extraction\n    dpr_answer = simple_answer_extraction(question, dpr_passages_text, top_k=3)\n    \n    dpr_time = time.time() - start_time\n    \n    dpr_prediction = {\n        'answer': dpr_answer,\n        'retrieved_titles': dpr_titles,\n        'supporting_facts': dpr_facts[:5]  # Top 5 facts\n    }\n    \n    dpr_metrics = evaluator.evaluate_single(dpr_prediction, gold_data, k=10, processing_time=dpr_time)\n    results['DPR_Vector_Search'].append(dpr_metrics)\n    \n    print(f\"      Answer: {dpr_answer}\")\n    print(f\"      Retrieved docs: {dpr_titles[:3]}...\")\n    print(f\"      Document Recall@10: {dpr_metrics['document_recall@k']:.3f}\")\n    print(f\"      Latency: {dpr_time:.3f}s\")\n\nprint(f\"\\nğŸ“Š FINAL RESULTS COMPARISON\")\nprint(\"=\"*40)\n\n# Calculate average metrics\nfor method, method_results in results.items():\n    print(f\"\\nğŸ”¸ {method}:\")\n    \n    if method_results:\n        avg_metrics = {}\n        for metric in method_results[0].keys():\n            avg_metrics[metric] = np.mean([r[metric] for r in method_results])\n        \n        print(f\"   Document Recall@10: {avg_metrics['document_recall@k']:.3f}\")\n        print(f\"   Supporting-Fact F1: {avg_metrics['supporting_fact_f1']:.3f}\")\n        print(f\"   Answer F1: {avg_metrics['answer_f1']:.3f}\")\n        print(f\"   Answer EM: {avg_metrics['answer_em']:.3f}\")\n        print(f\"   Joint EM: {avg_metrics['joint_em']:.3f}\")\n        print(f\"   Avg Latency: {avg_metrics['latency']:.3f}s\")\n\nprint(f\"\\nğŸ¯ KEY INSIGHTS:\")\nprint(\"=\"*30)\nprint(\"ğŸ” BM25 (Sparse Retrieval):\")\nprint(\"   âœ… Fast and lightweight\")\nprint(\"   âœ… Exact keyword matching\")\nprint(\"   âœ… Works well with entity-heavy questions\")\nprint(\"   âŒ Limited semantic understanding\")\n\nprint(\"\\nğŸ§  DPR (Vector Search):\")\nprint(\"   âœ… Semantic similarity\")\nprint(\"   âœ… Handles paraphrases\")\nprint(\"   âœ… Dense representations capture meaning\")\nprint(\"   âŒ Slower encoding time\")\nprint(\"   âŒ Less interpretable\")\n\nprint(f\"\\nâœ… Per-Question Retrieval Benefits:\")\nprint(\"   â€¢ Realistic evaluation matching HotpotQA setup\")\nprint(\"   â€¢ Tests ability to filter signal from noise (distractors)\")\nprint(\"   â€¢ Meaningful Document Recall@10 scores\")\nprint(\"   â€¢ Demonstrates multihop reasoning challenges\")\n\nprint(f\"\\nâœ… Evaluation complete on {len(val_subset)} examples!\")"
  },
  {
   "cell_type": "markdown",
   "id": "3pk142s0daw",
   "metadata": {},
   "source": "## ğŸ“ Summary and Key Takeaways\n\nThis notebook demonstrated how traditional methods implement vector search concepts AND the critical importance of matching your retrieval setup to the evaluation benchmark.\n\n### ğŸ¯ Critical Learning: Per-Question vs Global Retrieval\n\n**The Bug We Fixed:**\n```python\n# âŒ WRONG: Build index from training, search from validation\ntrain_passages, train_metadata = extract_passages_from_hotpotqa(train_sample)\nbm25_global = BM25Okapi(train_passages)\n# This gives 0% Document Recall because gold docs don't exist!\n\n# âœ… CORRECT: Per-question retrieval from each example's contexts\nfor example in validation:\n    example_passages, metadata = extract_passages_from_example(example)\n    bm25_local = BM25Okapi(example_passages)\n    results = bm25_local.search(example['question'])\n    # Now Document Recall is meaningful!\n```\n\n**Why This Matters:**\n- HotpotQA provides 10 context paragraphs per question (2 gold + 8 distractors)\n- The challenge is filtering correct documents from this candidate set\n- Global retrieval would test \"can you find documents in a corpus?\" (wrong task)\n- Per-question retrieval tests \"can you identify relevant docs from noisy candidates?\" (correct task)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ht6gs664kzc",
   "metadata": {},
   "outputs": [],
   "source": "print(\"ğŸ“ SUMMARY: Vector Search Learning Journey + Critical Implementation Lessons\")\nprint(\"=\"*70)\n\nprint(\"âœ… What We Learned:\")\nprint(\"\\n1. **BM25 = Sparse Vector Search**\")\nprint(\"   - Represents documents as sparse term-frequency vectors\")\nprint(\"   - Uses statistical weighting (IDF) for relevance\")\nprint(\"   - Fast, interpretable, keyword-focused\")\n\nprint(\"\\n2. **DPR = Dense Vector Search**\") \nprint(\"   - Transforms text into 768-dimensional dense vectors\")\nprint(\"   - Uses neural networks (BERT) for semantic encoding\")\nprint(\"   - Cosine similarity for vector matching\")\nprint(\"   - Same technology as OpenAI embeddings + Pinecone!\")\n\nprint(\"\\n3. **Vector Search Fundamentals**\")\nprint(\"   - Document encoding: Text â†’ Vector representations\")\nprint(\"   - Query encoding: Questions â†’ Query vectors\")\nprint(\"   - Similarity search: Find closest vectors (MIPS/Cosine)\")\nprint(\"   - Top-k retrieval: Return most similar documents\")\n\nprint(\"\\nğŸ”§ **CRITICAL IMPLEMENTATION LESSON**\")\nprint(\"=\"*70)\nprint(\"âš ï¸  Per-Question vs Global Retrieval - THIS DETERMINES SUCCESS!\")\nprint()\nprint(\"âŒ WRONG APPROACH (gives ~0% performance):\")\nprint(\"   â€¢ Build ONE global index from training data\")\nprint(\"   â€¢ Search this index for validation questions\")\nprint(\"   â€¢ Problem: Gold docs for validation don't exist in training!\")\nprint(\"   â€¢ Result: Document Recall = 0%, everything fails\")\nprint()\nprint(\"âœ… CORRECT APPROACH (gives realistic performance):\")\nprint(\"   â€¢ Each question comes with 10 context paragraphs\")\nprint(\"   â€¢ Build a SEPARATE index for each question's contexts\")\nprint(\"   â€¢ Search within those 10 candidates (2 gold + 8 distractors)\")\nprint(\"   â€¢ Result: Meaningful metrics, proper evaluation\")\nprint()\nprint(\"ğŸ’¡ KEY INSIGHT:\")\nprint(\"   HotpotQA tests: 'Can you filter signal from noise?'\")\nprint(\"   NOT: 'Can you find documents in a large corpus?'\")\n\nprint(\"\\nğŸ¯ **Implementation vs Framework Comparison**\")\nprint(\"=\"*50)\nprint(\"   Our Custom Implementation:\")\nprint(\"   âœ… Full control over encoding and similarity metrics\")\nprint(\"   âœ… Understanding of underlying mathematics\")\nprint(\"   âœ… Custom evaluation for multihop reasoning\")\nprint(\"   âœ… Learned critical per-question retrieval pattern\")\n\nprint(\"\\n   LlamaIndex Framework:\")\nprint(\"   âœ… Production-ready optimizations\")\nprint(\"   âœ… Multiple vector database backends\")\nprint(\"   âœ… Built-in caching and indexing\")\nprint(\"   âš ï¸  Must still configure retrieval correctly!\")\n\nprint(\"\\nğŸš€ **Vector Search in Modern RAG Systems**\")\nprint(\"=\"*40)\nprint(\"   â€¢ Pinecone, Weaviate, Chroma: Production vector databases\")\nprint(\"   â€¢ OpenAI Ada-002: Dense embeddings for general domain\")\nprint(\"   â€¢ LlamaIndex/LangChain: High-level RAG frameworks\")\nprint(\"   â€¢ Hybrid search: Combining sparse (BM25) + dense (embeddings)\")\nprint(\"   â€¢ All require proper corpus setup for your use case!\")\n\nprint(\"\\nğŸ“Š **HotpotQA Multihop Evaluation**\")\nprint(\"=\"*35)\nprint(\"   â€¢ 6 comprehensive metrics for multihop reasoning\")\nprint(\"   â€¢ Document Recall@k: Did we retrieve the right docs?\")\nprint(\"   â€¢ Supporting-Fact F1: Did we identify the right evidence?\")\nprint(\"   â€¢ Joint EM: Perfect answer + reasoning path\")\n\nprint(\"\\nğŸ¯ **Best Practices for Learning**\")\nprint(\"=\"*30)\nprint(\"   1. âœ… Understand your dataset's evaluation setup FIRST\")\nprint(\"   2. âœ… Match retrieval approach to benchmark requirements\")\nprint(\"   3. âœ… Verify metrics are meaningful (not 0%!)\")\nprint(\"   4. âœ… Start simple with clear baseline implementations\")\nprint(\"   5. âœ… Use frameworks after understanding fundamentals\")\n\nprint(\"\\nğŸ’¡ **Key Insight: Traditional Methods ARE Vector Search!**\")\nprint(\"=\"*55)\nprint(\"   â€¢ BM25: Sparse vector similarity with TF-IDF weighting\")\nprint(\"   â€¢ DPR: Dense vector similarity with neural encoding\")\nprint(\"   â€¢ Both use core concept: vector similarity search\")\nprint(\"   â€¢ Modern vector databases scale and optimize these concepts\")\nprint(\"   â€¢ BUT: Wrong retrieval setup = Wrong results, regardless of method!\")\n\nprint(\"\\nğŸ”® **Next Steps**\")\nprint(\"=\"*15)\nprint(\"   â€¢ Experiment with different embedding models\")\nprint(\"   â€¢ Try hybrid sparse + dense retrieval\")\nprint(\"   â€¢ Implement production vector database integration\")\nprint(\"   â€¢ Add reranking and generation components\")\nprint(\"   â€¢ Scale to full HotpotQA dataset\")\n\nprint(\"\\nâœ¨ Vector search is the foundation of modern RAG - now you understand it!\")\nprint(\"ğŸ“ AND you know how to implement it correctly for your specific task!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v9g10dc1u8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing import\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35544a-522e-43e9-be20-23b56a67a590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}