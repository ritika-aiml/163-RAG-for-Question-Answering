{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeac3a9c-bebf-4341-90f2-a5bfb293c4f4",
   "metadata": {},
   "source": [
    "# Traditional Methods Implementation: Vector Search in Action\n",
    "\n",
    "This notebook implements traditional information retrieval methods for HotpotQA multihop reasoning, with special emphasis on **vector search concepts** and comparison with modern frameworks like LlamaIndex.\n",
    "\n",
    "## üîç Vector Search in Traditional Methods\n",
    "\n",
    ":::{admonition} Key Insight: Traditional Methods Use Vector Search!\n",
    ":class: important\n",
    "\n",
    "**Dense Passage Retrieval (DPR) IS Vector Search!**\n",
    "- **Query Encoding**: Transform questions into dense vector embeddings\n",
    "- **Document Encoding**: Transform passages into dense vector embeddings  \n",
    "- **Similarity Search**: Use Maximum Inner Product Search (MIPS) or cosine similarity. Note they are not the same. \n",
    "- **Top-k Retrieval**: Return most similar document vectors to query vector\n",
    "\n",
    "This is the same underlying technology as modern vector databases (Pinecone, Chroma, Weaviate).\n",
    ":::\n",
    "\n",
    "## üìã Implementation Overview\n",
    "\n",
    "We'll implement:\n",
    "1. **BM25 Sparse Retrieval**: Statistical term weighting (traditional baseline)\n",
    "2. **DPR Vector Search**: Dense embeddings with similarity search (same as modern vector DBs)\n",
    "3. **BGE Cross-Encoder Reranking**: Advanced relevance scoring\n",
    "4. **Mistral-7B Answer Generation**: Final reasoning and synthesis\n",
    "5. **LlamaIndex Comparison**: Show how modern frameworks abstract the same concepts\n",
    "\n",
    "## üîß Learning Focus\n",
    "\n",
    "While frameworks like **LlamaIndex** provide excellent abstractions for vector search and RAG, we implement from scratch to understand:\n",
    "- How vector embeddings work under the hood\n",
    "- Different similarity metrics (cosine vs inner product)\n",
    "- Trade-offs between sparse and dense representations\n",
    "- Custom evaluation metrics for multihop reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd9277-20f8-469e-9d08-84f822447c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch sentence-transformers rank-bm25 numpy scikit-learn matplotlib seaborn\n",
    "!pip install llama-index  # For comparison later\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DPRQuestionEncoder, DPRContextEncoder,\n",
    "    DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer,\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "from sentence_transformers import CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All packages installed and imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmzpsm3jy3f",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Preprocessing\n",
    "\n",
    "Let's load the HotpotQA dataset and prepare it for our vector search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vy1ilu1o2tf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HotpotQA dataset\n",
    "print(\"üîÑ Loading HotpotQA dataset...\")\n",
    "dataset = load_dataset('hotpotqa/hotpot_qa', 'distractor')\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "\n",
    "print(f\"üìä Dataset loaded successfully!\")\n",
    "print(f\"   Training examples: {len(train_data):,}\")\n",
    "print(f\"   Validation examples: {len(validation_data):,}\")\n",
    "\n",
    "# Take a smaller subset for faster processing during development\n",
    "SAMPLE_SIZE = 100  # Increase this for full evaluation\n",
    "train_sample = train_data.shuffle(seed=42).select(range(min(SAMPLE_SIZE, len(train_data))))\n",
    "val_sample = validation_data.shuffle(seed=42).select(range(min(SAMPLE_SIZE, len(validation_data))))\n",
    "\n",
    "print(f\"üéØ Working with sample: {len(train_sample)} train, {len(val_sample)} validation\")\n",
    "\n",
    "# Inspect a sample to understand the structure\n",
    "sample_example = train_sample[0]\n",
    "print(\"\\nüìã Sample HotpotQA Example Structure:\")\n",
    "print(f\"   Question: {sample_example['question']}\")\n",
    "print(f\"   Answer: {sample_example['answer']}\")\n",
    "print(f\"   Type: {sample_example['type']}\")\n",
    "print(f\"   Level: {sample_example['level']}\")\n",
    "print(f\"   Number of context paragraphs: {len(list(sample_example['context']))}\")\n",
    "print(f\"   Supporting facts: {len(list(sample_example['supporting_facts']))}\")\n",
    "\n",
    "print(\"\\nüîç First few context titles:\")\n",
    "context_list = list(sample_example['context'])\n",
    "for i, (title, sentences) in enumerate(context_list[:3]):\n",
    "    print(f\"   {i+1}. {title} ({len(sentences)} sentences)\")\n",
    "\n",
    "print(\"\\nüìç Supporting facts:\")\n",
    "facts_list = list(sample_example['supporting_facts'])\n",
    "for title, sent_idx in facts_list:\n",
    "    print(f\"   - {title}, sentence {sent_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vkckhun492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions for BM25 and DPR\n",
    "def preprocess_text_for_bm25(text):\n",
    "    \"\"\"Preprocess text for BM25 sparse retrieval\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text.split()\n",
    "\n",
    "def extract_passages_from_hotpotqa(examples):\n",
    "    \"\"\"Extract individual passages from HotpotQA context structure\"\"\"\n",
    "    passages = []\n",
    "    passage_metadata = []\n",
    "    \n",
    "    for example_idx, example in enumerate(examples):\n",
    "        context_list = list(example['context'])\n",
    "        \n",
    "        for context_idx, (title, sentences) in enumerate(context_list):\n",
    "            # Each sentence becomes a separate passage\n",
    "            for sent_idx, sentence in enumerate(sentences):\n",
    "                passage_text = sentence.strip()\n",
    "                if passage_text:  # Only add non-empty passages\n",
    "                    passages.append(passage_text)\n",
    "                    passage_metadata.append({\n",
    "                        'example_idx': example_idx,\n",
    "                        'title': title,\n",
    "                        'context_idx': context_idx,\n",
    "                        'sentence_idx': sent_idx,\n",
    "                        'full_passage': ' '.join(sentences)  # Full paragraph for context\n",
    "                    })\n",
    "    \n",
    "    return passages, passage_metadata\n",
    "\n",
    "print(\"‚úÖ Preprocessing functions ready!\")\n",
    "print(\"üìù Functions available:\")\n",
    "print(\"   - preprocess_text_for_bm25(): Clean text for sparse retrieval\")\n",
    "print(\"   - extract_passages_from_hotpotqa(): Convert HotpotQA format to passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mzmswo3z01r",
   "metadata": {},
   "source": [
    "## üîç BM25 Sparse Retrieval Implementation\n",
    "\n",
    "BM25 is a probabilistic ranking function that scores documents based on query term frequency and document length. It's a traditional sparse method that forms the foundation of modern search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k8ra2z4boo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index from HotpotQA passages\n",
    "print(\"üîÑ Building BM25 corpus from HotpotQA training data...\")\n",
    "\n",
    "# Extract passages from training data\n",
    "train_passages, train_metadata = extract_passages_from_hotpotqa(train_sample)\n",
    "print(f\"üìä Extracted {len(train_passages):,} passages from {len(train_sample)} training examples\")\n",
    "\n",
    "# Show sample passages\n",
    "print(\"\\nüìù Sample passages:\")\n",
    "for i in range(min(3, len(train_passages))):\n",
    "    print(f\"   {i+1}. {train_passages[i][:100]}...\")\n",
    "    print(f\"      Title: {train_metadata[i]['title']}\")\n",
    "\n",
    "# Preprocess passages for BM25\n",
    "print(\"\\nüîÑ Preprocessing passages for BM25...\")\n",
    "tokenized_passages = [preprocess_text_for_bm25(passage) for passage in train_passages]\n",
    "\n",
    "# Build BM25 index\n",
    "print(\"üèóÔ∏è Building BM25 index...\")\n",
    "bm25 = BM25Okapi(tokenized_passages)\n",
    "print(f\"‚úÖ BM25 index built with {len(tokenized_passages):,} documents\")\n",
    "\n",
    "# Test BM25 with a sample query\n",
    "test_question = train_sample[0]['question']\n",
    "print(f\"\\nüéØ Testing BM25 with sample question:\")\n",
    "print(f\"   Question: {test_question}\")\n",
    "\n",
    "# Tokenize query and search\n",
    "test_query_tokens = preprocess_text_for_bm25(test_question)\n",
    "print(f\"   Query tokens: {test_query_tokens[:10]}...\")\n",
    "\n",
    "# Get top-k BM25 scores\n",
    "k = 10\n",
    "bm25_scores = bm25.get_scores(test_query_tokens)\n",
    "top_k_indices = np.argsort(bm25_scores)[::-1][:k]\n",
    "\n",
    "print(f\"\\nüìä Top-{k} BM25 results:\")\n",
    "for i, idx in enumerate(top_k_indices):\n",
    "    score = bm25_scores[idx]\n",
    "    passage = train_passages[idx][:100] + \"...\"\n",
    "    title = train_metadata[idx]['title']\n",
    "    print(f\"   {i+1}. Score: {score:.3f} | Title: {title}\")\n",
    "    print(f\"      {passage}\")\n",
    "\n",
    "print(\"\\n‚úÖ BM25 retrieval system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uw0e3cyeba",
   "metadata": {},
   "source": [
    "## üß† DPR Vector Search Implementation\n",
    "\n",
    "Dense Passage Retrieval (DPR) represents the **core of modern vector search**. It transforms questions and passages into dense embeddings and finds similar passages using vector similarity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cpew9hag5o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DPR models (this is vector search!)\n",
    "print(\"üîÑ Loading pre-trained DPR models for vector search...\")\n",
    "\n",
    "# Question encoder (queries ‚Üí vectors)\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "# Context encoder (passages ‚Üí vectors) \n",
    "c_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "c_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# Move to device\n",
    "q_encoder.to(device)\n",
    "c_encoder.to(device)\n",
    "\n",
    "print(f\"‚úÖ DPR models loaded on {device}\")\n",
    "print(\"üîç Vector Search Components Ready:\")\n",
    "print(f\"   - Question Encoder: Transforms questions ‚Üí 768-dim vectors\")\n",
    "print(f\"   - Context Encoder: Transforms passages ‚Üí 768-dim vectors\")\n",
    "print(f\"   - Similarity: Cosine similarity / Inner product search\")\n",
    "\n",
    "def encode_questions_dpr(questions, batch_size=32):\n",
    "    \"\"\"Encode questions into dense vectors (Vector Search Step 1)\"\"\"\n",
    "    q_encoder.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch_questions = questions[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize questions\n",
    "            encoded = q_tokenizer(\n",
    "                batch_questions,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            \n",
    "            # Encode to vectors\n",
    "            embeddings = q_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_embeddings.append(embeddings.pooler_output.cpu())\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "def encode_passages_dpr(passages, batch_size=32):\n",
    "    \"\"\"Encode passages into dense vectors (Vector Search Step 2)\"\"\"\n",
    "    c_encoder.eval()\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(passages), batch_size), desc=\"Encoding passages\"):\n",
    "            batch_passages = passages[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize passages\n",
    "            encoded = c_tokenizer(\n",
    "                batch_passages,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(device)\n",
    "            attention_mask = encoded['attention_mask'].to(device)\n",
    "            \n",
    "            # Encode to vectors\n",
    "            embeddings = c_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_embeddings.append(embeddings.pooler_output.cpu())\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "def vector_search_dpr(query_embedding, passage_embeddings, passages, metadata, top_k=10):\n",
    "    \"\"\"Perform vector search using cosine similarity (Vector Search Step 3)\"\"\"\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    query_embedding = F.normalize(query_embedding, p=2, dim=1)\n",
    "    passage_embeddings = F.normalize(passage_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute similarity matrix (this IS vector search!)\n",
    "    similarities = torch.mm(query_embedding, passage_embeddings.transpose(0, 1))\n",
    "    similarities = similarities.squeeze(0)  # Remove batch dimension\n",
    "    \n",
    "    # Get top-k most similar vectors\n",
    "    top_k_scores, top_k_indices = torch.topk(similarities, min(top_k, len(similarities)))\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(top_k_scores, top_k_indices):\n",
    "        results.append({\n",
    "            'idx': int(idx),\n",
    "            'score': float(score),\n",
    "            'passage': passages[int(idx)],\n",
    "            'title': metadata[int(idx)]['title'],\n",
    "            'metadata': metadata[int(idx)]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n‚úÖ DPR Vector Search functions ready!\")\n",
    "print(\"üéØ Key insight: DPR = Vector Database functionality!\")\n",
    "print(\"   - encode_questions_dpr(): Query ‚Üí Vector\")\n",
    "print(\"   - encode_passages_dpr(): Documents ‚Üí Vectors\") \n",
    "print(\"   - vector_search_dpr(): Similarity search (MIPS/Cosine)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i34riu4cie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DPR vector index (same as building a vector database!)\n",
    "print(\"üîÑ Building DPR vector embeddings for passage corpus...\")\n",
    "print(\"‚ö° This is exactly what vector databases like Pinecone/Chroma do!\")\n",
    "\n",
    "# Encode all passages into vectors (this takes a few minutes)\n",
    "passage_embeddings = encode_passages_dpr(train_passages, batch_size=16)\n",
    "\n",
    "print(f\"‚úÖ Vector index built!\")\n",
    "print(f\"üìä Vector Statistics:\")\n",
    "print(f\"   - Number of vectors: {passage_embeddings.shape[0]:,}\")\n",
    "print(f\"   - Vector dimension: {passage_embeddings.shape[1]}\")\n",
    "print(f\"   - Total parameters: {passage_embeddings.shape[0] * passage_embeddings.shape[1]:,}\")\n",
    "print(f\"   - Memory usage: ~{passage_embeddings.numel() * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Test vector search with the same question\n",
    "print(f\"\\nüéØ Testing Vector Search with same question:\")\n",
    "print(f\"   Question: {test_question}\")\n",
    "\n",
    "# Encode query into vector\n",
    "query_embedding = encode_questions_dpr([test_question])\n",
    "print(f\"   Query vector shape: {query_embedding.shape}\")\n",
    "\n",
    "# Perform vector search (this is the core of RAG!)\n",
    "vector_results = vector_search_dpr(query_embedding, passage_embeddings, train_passages, train_metadata, top_k=10)\n",
    "\n",
    "print(f\"\\nüìä Top-10 Vector Search Results:\")\n",
    "for i, result in enumerate(vector_results):\n",
    "    print(f\"   {i+1}. Score: {result['score']:.3f} | Title: {result['title']}\")\n",
    "    print(f\"      {result['passage'][:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Vector Search system ready!\")\n",
    "print(\"üîç This demonstrates the core technology behind:\")\n",
    "print(\"   - OpenAI Embeddings + Vector DBs\")\n",
    "print(\"   - LlamaIndex vector stores\") \n",
    "print(\"   - LangChain vector retrievers\")\n",
    "print(\"   - Pinecone, Weaviate, Chroma databases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x7u4jsy6ir",
   "metadata": {},
   "source": [
    "## üöÄ LlamaIndex Comparison: Framework vs Custom Implementation\n",
    "\n",
    "Let's compare our custom vector search implementation with LlamaIndex to show how modern frameworks abstract these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6l00hcjvdjq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex Implementation Example (for comparison)\n",
    "print(\"üöÄ LlamaIndex Vector Search Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üìö Our Custom Implementation (learning-focused):\")\n",
    "print(\"   1. Manual passage extraction from HotpotQA\")\n",
    "print(\"   2. Explicit DPR model loading and tokenization\")\n",
    "print(\"   3. Custom vector encoding functions\")\n",
    "print(\"   4. Manual cosine similarity computation\")\n",
    "print(\"   5. Custom retrieval and ranking logic\")\n",
    "print(\"   6. Full control over similarity metrics\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è How LlamaIndex Would Abstract This:\")\n",
    "print(\"   1. SimpleDirectoryReader() - automatic document loading\")\n",
    "print(\"   2. VectorStoreIndex.from_documents() - auto-embedding\")\n",
    "print(\"   3. Built-in similarity search\")\n",
    "print(\"   4. Automatic retrieval and generation\")\n",
    "print(\"   5. Multiple vector database backends\")\n",
    "print(\"   6. Pre-configured pipelines\")\n",
    "\n",
    "print(\"\\nüí° LlamaIndex Equivalent Code (pseudo-code):\")\n",
    "print(\"\"\"\n",
    "```python\n",
    "# LlamaIndex - High-level abstraction\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "# Load documents (abstracts our manual extraction)\n",
    "documents = SimpleDirectoryReader('data/').load_data()\n",
    "\n",
    "# Create vector index (abstracts our manual DPR encoding)\n",
    "embed_model = HuggingFaceEmbedding(model_name='facebook/dpr-ctx_encoder-single-nq-base')\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "# Query (abstracts our manual vector search)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query('What is the question?')\n",
    "```\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ Key Learning Points:\")\n",
    "print(\"   ‚úÖ Our implementation: Understand the underlying math and operations\")\n",
    "print(\"   ‚úÖ LlamaIndex: Production-ready with optimizations and abstractions\")\n",
    "print(\"   ‚úÖ Both use the SAME vector search concepts:\")\n",
    "print(\"      - Document ‚Üí Vector encoding\")\n",
    "print(\"      - Query ‚Üí Vector encoding\") \n",
    "print(\"      - Similarity search (cosine/inner product)\")\n",
    "print(\"      - Top-k retrieval\")\n",
    "\n",
    "print(\"\\nüîç Why Learn Custom Implementation First:\")\n",
    "print(\"   1. Understand vector search mathematics\")\n",
    "print(\"   2. Debug and optimize retrieval performance\")\n",
    "print(\"   3. Implement custom similarity metrics\")\n",
    "print(\"   4. Adapt to specific domain requirements\")\n",
    "print(\"   5. Build domain-specific evaluation metrics\")\n",
    "\n",
    "print(\"\\n‚ö° When to Use LlamaIndex:\")\n",
    "print(\"   1. Rapid prototyping and production deployment\")\n",
    "print(\"   2. Standard RAG pipelines without customization\")\n",
    "print(\"   3. Multiple vector database backend support\")\n",
    "print(\"   4. Built-in optimization and caching\")\n",
    "print(\"   5. Integration with LLM frameworks\")\n",
    "\n",
    "print(\"\\nüèÜ Best Practice: Learn fundamentals first, then use frameworks!\")\n",
    "print(\"üìä Our approach: Custom ‚Üí Framework comparison ‚Üí Production choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56y6w1u685",
   "metadata": {},
   "source": [
    "## üìä Evaluation Framework\n",
    "\n",
    "Let's implement our evaluation framework using the 6 chosen metrics for HotpotQA multihop reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrtrumkkxhs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation framework from @03_Evaluation_Code.ipynb\n",
    "class HotpotQAEvaluator:\n",
    "    \"\"\"Comprehensive evaluator for HotpotQA multihop reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def normalize_answer(self, text):\n",
    "        \"\"\"Normalize answer text for comparison\"\"\"\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove articles\n",
    "        text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def answer_f1_score(self, prediction, ground_truth):\n",
    "        \"\"\"Calculate F1 score between prediction and ground truth\"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        pred_tokens = self.normalize_answer(prediction).split()\n",
    "        gold_tokens = self.normalize_answer(ground_truth).split()\n",
    "        \n",
    "        if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "            return 1.0\n",
    "        if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        common_tokens = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "        num_same = sum(common_tokens.values())\n",
    "        \n",
    "        if num_same == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(gold_tokens)\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    def answer_exact_match(self, prediction, ground_truth):\n",
    "        \"\"\"Calculate exact match score\"\"\"\n",
    "        return float(self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n",
    "    \n",
    "    def document_recall_at_k(self, retrieved_titles, gold_titles, k=10):\n",
    "        \"\"\"Calculate document recall@k\"\"\"\n",
    "        if len(gold_titles) == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        retrieved_k = set(retrieved_titles[:k])\n",
    "        gold_set = set(gold_titles)\n",
    "        \n",
    "        return len(retrieved_k.intersection(gold_set)) / len(gold_set)\n",
    "    \n",
    "    def supporting_fact_f1(self, predicted_facts, gold_facts):\n",
    "        \"\"\"Calculate supporting facts F1 score\"\"\"\n",
    "        if len(gold_facts) == 0:\n",
    "            return 1.0 if len(predicted_facts) == 0 else 0.0\n",
    "        \n",
    "        pred_set = set(predicted_facts)\n",
    "        gold_set = set(gold_facts)\n",
    "        \n",
    "        if len(pred_set) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = pred_set.intersection(gold_set)\n",
    "        precision = len(intersection) / len(pred_set)\n",
    "        recall = len(intersection) / len(gold_set)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    def joint_exact_match(self, pred_answer, gold_answer, pred_facts, gold_facts):\n",
    "        \"\"\"Calculate joint exact match (answer + supporting facts)\"\"\"\n",
    "        answer_em = self.answer_exact_match(pred_answer, gold_answer)\n",
    "        facts_em = 1.0 if set(pred_facts) == set(gold_facts) else 0.0\n",
    "        \n",
    "        return float(answer_em == 1.0 and facts_em == 1.0)\n",
    "    \n",
    "    def evaluate_single(self, prediction_dict, gold_data, k=10, processing_time=None):\n",
    "        \"\"\"Evaluate a single prediction against gold data\"\"\"\n",
    "        \n",
    "        # Extract predictions\n",
    "        pred_answer = prediction_dict.get('answer', '')\n",
    "        pred_titles = prediction_dict.get('retrieved_titles', [])\n",
    "        pred_facts = prediction_dict.get('supporting_facts', [])\n",
    "        \n",
    "        # Extract gold data\n",
    "        gold_answer = gold_data.get('answer', '')\n",
    "        gold_facts = []\n",
    "        gold_titles = []\n",
    "        \n",
    "        # Extract gold supporting facts and titles\n",
    "        if 'supporting_facts' in gold_data:\n",
    "            facts_list = list(gold_data['supporting_facts'])\n",
    "            for title, sent_id in facts_list:\n",
    "                gold_facts.append((title, sent_id))\n",
    "                if title not in gold_titles:\n",
    "                    gold_titles.append(title)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'answer_f1': self.answer_f1_score(pred_answer, gold_answer),\n",
    "            'answer_em': self.answer_exact_match(pred_answer, gold_answer),\n",
    "            'document_recall@k': self.document_recall_at_k(pred_titles, gold_titles, k),\n",
    "            'supporting_fact_f1': self.supporting_fact_f1(pred_facts, gold_facts),\n",
    "            'joint_em': self.joint_exact_match(pred_answer, gold_answer, pred_facts, gold_facts),\n",
    "            'latency': processing_time if processing_time is not None else 0.0\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = HotpotQAEvaluator()\n",
    "print(\"‚úÖ HotpotQA Evaluation Framework Ready!\")\n",
    "print(\"üìä Available metrics:\")\n",
    "print(\"   1. Answer F1 Score\")\n",
    "print(\"   2. Answer Exact Match\")  \n",
    "print(\"   3. Document Recall@k\")\n",
    "print(\"   4. Supporting-Fact F1\")\n",
    "print(\"   5. Joint Exact Match\")\n",
    "print(\"   6. Latency (processing time)\")\n",
    "\n",
    "# Test with sample data\n",
    "sample_prediction = {\n",
    "    'answer': 'test answer',\n",
    "    'retrieved_titles': ['Title 1', 'Title 2', 'Title 3'],\n",
    "    'supporting_facts': [('Title 1', 0), ('Title 2', 1)]\n",
    "}\n",
    "\n",
    "sample_gold = {\n",
    "    'answer': 'test answer',\n",
    "    'supporting_facts': [('Title 1', 0), ('Title 2', 1)]\n",
    "}\n",
    "\n",
    "test_metrics = evaluator.evaluate_single(sample_prediction, sample_gold, k=10, processing_time=0.5)\n",
    "print(f\"\\nüß™ Test evaluation result: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlxxdl1s36c",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Run Complete Pipeline and Compare Methods\n",
    "\n",
    "Now let's run both BM25 (sparse) and DPR (vector search) on a subset of validation data and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7kesw1akrt5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation on validation set\n",
    "print(\"üî¨ COMPREHENSIVE EVALUATION: BM25 vs DPR Vector Search\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use a subset of validation data for evaluation\n",
    "eval_size = 20  # Adjust based on compute resources\n",
    "val_subset = val_sample.select(range(min(eval_size, len(val_sample))))\n",
    "\n",
    "print(f\"üìä Evaluating on {len(val_subset)} validation examples\")\n",
    "\n",
    "# Initialize results storage\n",
    "results = {\n",
    "    'BM25': [],\n",
    "    'DPR_Vector_Search': []\n",
    "}\n",
    "\n",
    "# Evaluate each method\n",
    "for i, example in enumerate(val_subset):\n",
    "    question = example['question']\n",
    "    gold_answer = example['answer']\n",
    "    gold_data = {\n",
    "        'answer': gold_answer,\n",
    "        'supporting_facts': example['supporting_facts']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìù Example {i+1}/{len(val_subset)}: {question[:80]}...\")\n",
    "    print(f\"üéØ Gold answer: {gold_answer}\")\n",
    "    \n",
    "    # BM25 Evaluation\n",
    "    print(\"   üîç BM25 Sparse Retrieval:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # BM25 search\n",
    "    query_tokens = preprocess_text_for_bm25(question)\n",
    "    bm25_scores = bm25.get_scores(query_tokens)\n",
    "    top_k_indices = np.argsort(bm25_scores)[::-1][:10]\n",
    "    \n",
    "    # Extract retrieved titles and facts\n",
    "    retrieved_titles = []\n",
    "    retrieved_facts = []\n",
    "    \n",
    "    for idx in top_k_indices:\n",
    "        title = train_metadata[idx]['title']\n",
    "        if title not in retrieved_titles:\n",
    "            retrieved_titles.append(title)\n",
    "        # Add sentence-level facts\n",
    "        retrieved_facts.append((title, train_metadata[idx]['sentence_idx']))\n",
    "    \n",
    "    # Simple answer extraction (use top passage)\n",
    "    bm25_answer = train_passages[top_k_indices[0]][:50] + \"...\"  # Simplified\n",
    "    \n",
    "    bm25_time = time.time() - start_time\n",
    "    \n",
    "    bm25_prediction = {\n",
    "        'answer': bm25_answer,\n",
    "        'retrieved_titles': retrieved_titles,\n",
    "        'supporting_facts': retrieved_facts[:5]  # Top 5 facts\n",
    "    }\n",
    "    \n",
    "    bm25_metrics = evaluator.evaluate_single(bm25_prediction, gold_data, k=10, processing_time=bm25_time)\n",
    "    results['BM25'].append(bm25_metrics)\n",
    "    \n",
    "    print(f\"      Answer: {bm25_answer}\")\n",
    "    print(f\"      Document Recall@10: {bm25_metrics['document_recall@k']:.3f}\")\n",
    "    print(f\"      Latency: {bm25_time:.3f}s\")\n",
    "    \n",
    "    # DPR Vector Search Evaluation  \n",
    "    print(\"   üß† DPR Vector Search:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Vector search\n",
    "    query_embedding = encode_questions_dpr([question])\n",
    "    vector_results = vector_search_dpr(query_embedding, passage_embeddings, train_passages, train_metadata, top_k=10)\n",
    "    \n",
    "    # Extract retrieved titles and facts\n",
    "    dpr_titles = []\n",
    "    dpr_facts = []\n",
    "    \n",
    "    for result in vector_results:\n",
    "        title = result['title']\n",
    "        if title not in dpr_titles:\n",
    "            dpr_titles.append(title)\n",
    "        dpr_facts.append((title, result['metadata']['sentence_idx']))\n",
    "    \n",
    "    # Simple answer extraction (use top passage)\n",
    "    dpr_answer = vector_results[0]['passage'][:50] + \"...\"  # Simplified\n",
    "    \n",
    "    dpr_time = time.time() - start_time\n",
    "    \n",
    "    dpr_prediction = {\n",
    "        'answer': dpr_answer,\n",
    "        'retrieved_titles': dpr_titles,\n",
    "        'supporting_facts': dpr_facts[:5]  # Top 5 facts\n",
    "    }\n",
    "    \n",
    "    dpr_metrics = evaluator.evaluate_single(dpr_prediction, gold_data, k=10, processing_time=dpr_time)\n",
    "    results['DPR_Vector_Search'].append(dpr_metrics)\n",
    "    \n",
    "    print(f\"      Answer: {dpr_answer}\")\n",
    "    print(f\"      Document Recall@10: {dpr_metrics['document_recall@k']:.3f}\")\n",
    "    print(f\"      Latency: {dpr_time:.3f}s\")\n",
    "\n",
    "print(\"\\nüìä FINAL RESULTS COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate average metrics\n",
    "for method, method_results in results.items():\n",
    "    print(f\"\\nüî∏ {method}:\")\n",
    "    \n",
    "    if method_results:\n",
    "        avg_metrics = {}\n",
    "        for metric in method_results[0].keys():\n",
    "            avg_metrics[metric] = np.mean([r[metric] for r in method_results])\n",
    "        \n",
    "        print(f\"   Document Recall@10: {avg_metrics['document_recall@k']:.3f}\")\n",
    "        print(f\"   Supporting-Fact F1: {avg_metrics['supporting_fact_f1']:.3f}\")\n",
    "        print(f\"   Answer F1: {avg_metrics['answer_f1']:.3f}\")\n",
    "        print(f\"   Answer EM: {avg_metrics['answer_em']:.3f}\")\n",
    "        print(f\"   Joint EM: {avg_metrics['joint_em']:.3f}\")\n",
    "        print(f\"   Avg Latency: {avg_metrics['latency']:.3f}s\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"üîç BM25 (Sparse Retrieval):\")\n",
    "print(\"   ‚úÖ Fast and lightweight\")\n",
    "print(\"   ‚úÖ Exact keyword matching\")\n",
    "print(\"   ‚úÖ Interpretable scoring\")\n",
    "print(\"   ‚ùå Limited semantic understanding\")\n",
    "\n",
    "print(\"\\nüß† DPR (Vector Search):\")\n",
    "print(\"   ‚úÖ Semantic similarity\")\n",
    "print(\"   ‚úÖ Handles paraphrases\")\n",
    "print(\"   ‚úÖ Dense representations\")\n",
    "print(\"   ‚ùå Slower encoding time\")\n",
    "print(\"   ‚ùå Less interpretable\")\n",
    "\n",
    "print(\"\\nüöÄ Vector Search in Production:\")\n",
    "print(\"   ‚Ä¢ Both methods demonstrate core retrieval concepts\")\n",
    "print(\"   ‚Ä¢ DPR shows vector search fundamentals used in:\")\n",
    "print(\"     - OpenAI embeddings + Pinecone\")\n",
    "print(\"     - LlamaIndex vector stores\")\n",
    "print(\"     - LangChain retrievers\")\n",
    "print(\"   ‚Ä¢ Hybrid approaches often work best in practice\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete on {len(val_subset)} examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pk142s0daw",
   "metadata": {},
   "source": [
    "## üìù Summary and Key Takeaways\n",
    "\n",
    "This notebook demonstrated how traditional methods actually implement vector search concepts that modern RAG systems build upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ht6gs664kzc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì SUMMARY: Vector Search Learning Journey\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"‚úÖ What We Learned:\")\n",
    "print(\"1. **BM25 = Sparse Vector Search**\")\n",
    "print(\"   - Represents documents as sparse term-frequency vectors\")\n",
    "print(\"   - Uses statistical weighting (IDF) for relevance\")\n",
    "print(\"   - Fast, interpretable, keyword-focused\")\n",
    "\n",
    "print(\"\\n2. **DPR = Dense Vector Search**\") \n",
    "print(\"   - Transforms text into 768-dimensional dense vectors\")\n",
    "print(\"   - Uses neural networks (BERT) for semantic encoding\")\n",
    "print(\"   - Cosine similarity for vector matching\")\n",
    "print(\"   - Same technology as OpenAI embeddings + Pinecone!\")\n",
    "\n",
    "print(\"\\n3. **Vector Search Fundamentals**\")\n",
    "print(\"   - Document encoding: Text ‚Üí Vector representations\")\n",
    "print(\"   - Query encoding: Questions ‚Üí Query vectors\")\n",
    "print(\"   - Similarity search: Find closest vectors (MIPS/Cosine)\")\n",
    "print(\"   - Top-k retrieval: Return most similar documents\")\n",
    "\n",
    "print(\"\\nüîß **Implementation vs Framework Comparison**\")\n",
    "print(\"   Our Custom Implementation:\")\n",
    "print(\"   ‚úÖ Full control over encoding and similarity metrics\")\n",
    "print(\"   ‚úÖ Understanding of underlying mathematics\")\n",
    "print(\"   ‚úÖ Custom evaluation for multihop reasoning\")\n",
    "print(\"   ‚úÖ Adaptable to domain-specific requirements\")\n",
    "\n",
    "print(\"\\n   LlamaIndex Framework:\")\n",
    "print(\"   ‚úÖ Production-ready optimizations\")\n",
    "print(\"   ‚úÖ Multiple vector database backends\")\n",
    "print(\"   ‚úÖ Built-in caching and indexing\")\n",
    "print(\"   ‚úÖ Rapid prototyping capabilities\")\n",
    "\n",
    "print(\"\\nüöÄ **Vector Search in Modern RAG Systems**\")\n",
    "print(\"   ‚Ä¢ Pinecone, Weaviate, Chroma: Production vector databases\")\n",
    "print(\"   ‚Ä¢ OpenAI Ada-002: Dense embeddings for general domain\")\n",
    "print(\"   ‚Ä¢ LlamaIndex/LangChain: High-level RAG frameworks\")\n",
    "print(\"   ‚Ä¢ Hybrid search: Combining sparse (BM25) + dense (embeddings)\")\n",
    "\n",
    "print(\"\\nüìä **HotpotQA Multihop Evaluation**\")\n",
    "print(\"   ‚Ä¢ 6 comprehensive metrics for multihop reasoning\")\n",
    "print(\"   ‚Ä¢ Document Recall@k: Retrieval effectiveness\")\n",
    "print(\"   ‚Ä¢ Supporting-Fact F1: Evidence identification\")\n",
    "print(\"   ‚Ä¢ Joint EM: Combined answer + reasoning accuracy\")\n",
    "\n",
    "print(\"\\nüéØ **Best Practices for Learning**\")\n",
    "print(\"   1. Start with custom implementation to understand fundamentals\")\n",
    "print(\"   2. Compare sparse vs dense approaches on your domain\")\n",
    "print(\"   3. Use frameworks for production deployment\")\n",
    "print(\"   4. Implement comprehensive evaluation metrics\")\n",
    "print(\"   5. Consider hybrid approaches for best performance\")\n",
    "\n",
    "print(\"\\nüí° **Key Insight: Traditional Methods ARE Vector Search!**\")\n",
    "print(\"   ‚Ä¢ BM25: Sparse vector similarity with TF-IDF weighting\")\n",
    "print(\"   ‚Ä¢ DPR: Dense vector similarity with neural encoding\")\n",
    "print(\"   ‚Ä¢ Both use the same core concept: vector similarity search\")\n",
    "print(\"   ‚Ä¢ Modern vector databases scale and optimize these concepts\")\n",
    "\n",
    "print(\"\\nüîÆ **Next Steps**\")\n",
    "print(\"   ‚Ä¢ Experiment with different embedding models\")\n",
    "print(\"   ‚Ä¢ Try hybrid sparse + dense retrieval\")\n",
    "print(\"   ‚Ä¢ Implement production vector database integration\")\n",
    "print(\"   ‚Ä¢ Add reranking and generation components\")\n",
    "print(\"   ‚Ä¢ Scale to full HotpotQA dataset for comprehensive evaluation\")\n",
    "\n",
    "print(\"\\n‚ú® Vector search is the foundation of modern RAG - now you understand it!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v9g10dc1u8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing import\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35544a-522e-43e9-be20-23b56a67a590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
