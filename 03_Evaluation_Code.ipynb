{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4357581-c0e0-4fc3-8371-3e0f7150d283",
   "metadata": {},
   "source": "# HotpotQA Answer Generation Quality Evaluation Framework\n\n## Overview\n\nThis notebook implements **6 evaluation metrics** for HotpotQA answer generation quality assessment:\n\n1. **Answer F1** - Token-level overlap measuring partial credit for answers\n2. **Answer EM** - Exact match for strict answer correctness evaluation  \n3. **Citation Precision** - Percentage of predicted citations that are correct\n4. **Citation Recall** - Percentage of ground truth citations correctly identified\n5. **Citation F1** - Harmonic mean of citation precision and recall\n6. **Insufficient Context Detection Rate** - Accuracy when model identifies unanswerable questions\n\n## Generation Quality vs Retrieval Evaluation\n\nThis framework evaluates **generation quality** - how well the model produces accurate answers with proper citation support. This is **separate from retrieval evaluation**, which measures document/passage selection quality (covered separately).\n\n### Why These 6 Metrics?\n\n**Answer Metrics (F1 & EM)**:\n- Standard QA evaluation providing both lenient (F1) and strict (EM) assessment\n- Comparable to major benchmarks (SQuAD, Natural Questions)\n- F1 gives partial credit for semantically similar answers\n\n**Citation Metrics (Precision, Recall, F1)**:\n- Unique to RAG systems - ensures factual grounding\n- Prevents citation hallucination and random guessing\n- Critical for explainability and trustworthiness\n- Tests multihop reasoning by checking if all evidence is identified\n\n**Insufficient Context Detection**:\n- Tests model's ability to recognize limitations\n- Prevents hallucination on unanswerable questions\n- Critical for reliable deployment in production\n\nThis comprehensive framework evaluates answer correctness, citation accuracy, and edge case handling - the three pillars of trustworthy RAG systems."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c2d90",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries for answer generation quality evaluation\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Dict, Tuple, Union, Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"‚úÖ Answer generation quality evaluation libraries imported successfully!\")\nprint(\"üìä Ready to implement the 6 generation quality metrics:\")\nprint(\"   1. Answer F1 - Token-level answer similarity\")\nprint(\"   2. Answer EM - Exact answer matching\") \nprint(\"   3. Citation Precision - Accuracy of predicted citations\")\nprint(\"   4. Citation Recall - Completeness of citation identification\")\nprint(\"   5. Citation F1 - Balanced citation quality metric\")\nprint(\"   6. Insufficient Context Detection - Edge case handling\")\nprint(\"üéØ Designed for comprehensive RAG answer generation assessment\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76504560",
   "metadata": {},
   "outputs": [],
   "source": "class HotpotQAGenerationEvaluator:\n    \"\"\"\n    Answer generation quality evaluation framework for HotpotQA RAG systems\n    \n    Implements the 6 chosen generation quality metrics:\n    1. Answer F1 - Token-level answer overlap\n    2. Answer EM - Exact answer matching\n    3. Citation Precision - Accuracy of predicted citations\n    4. Citation Recall - Completeness of citation identification\n    5. Citation F1 - Balanced citation quality\n    6. Insufficient Context Detection - Edge case handling accuracy\n    \"\"\"\n    \n    def __init__(self):\n        print(\"üéØ HotpotQA Generation Quality Evaluator Initialized\")\n        print(\"   üìä 6 Generation Quality Metrics:\")\n        print(\"   ‚Ä¢ Answer F1 & EM (answer correctness)\")\n        print(\"   ‚Ä¢ Citation Precision, Recall & F1 (factual grounding)\")\n        print(\"   ‚Ä¢ Insufficient Context Detection (reliability)\")\n    \n    def normalize_answer(self, text: str) -> str:\n        \"\"\"Standard answer normalization for HotpotQA evaluation\"\"\"\n        def remove_articles(text):\n            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n        \n        def white_space_fix(text):\n            return ' '.join(text.split())\n        \n        def remove_punc(text):\n            exclude = set(string.punctuation)\n            return ''.join(ch for ch in text if ch not in exclude)\n        \n        def lower(text):\n            return text.lower()\n        \n        return white_space_fix(remove_articles(remove_punc(lower(text))))\n    \n    def answer_exact_match(self, prediction: str, ground_truth: str) -> float:\n        \"\"\"\n        Calculate Answer Exact Match score\n        \n        Returns 1.0 if normalized prediction exactly matches ground truth, else 0.0\n        Special handling: Both \"insufficient context\" = 1.0\n        \"\"\"\n        # Check for insufficient context case\n        pred_insufficient = self.normalize_answer(prediction) == 'insufficient context'\n        gt_insufficient = self.normalize_answer(ground_truth) == 'insufficient context'\n        \n        if gt_insufficient and pred_insufficient:\n            return 1.0\n        \n        return float(self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n    \n    def answer_f1(self, prediction: str, ground_truth: str) -> float:\n        \"\"\"\n        Calculate Answer F1 score at token level\n        \n        Measures overlap between predicted and ground truth answer tokens.\n        Special handling: Both \"insufficient context\" = 1.0\n        \"\"\"\n        # Check for insufficient context case\n        pred_insufficient = self.normalize_answer(prediction) == 'insufficient context'\n        gt_insufficient = self.normalize_answer(ground_truth) == 'insufficient context'\n        \n        if gt_insufficient and pred_insufficient:\n            return 1.0\n        \n        pred_tokens = self.normalize_answer(prediction).split()\n        gt_tokens = self.normalize_answer(ground_truth).split()\n        \n        if len(pred_tokens) == 0 and len(gt_tokens) == 0:\n            return 1.0\n        if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n            return 0.0\n        \n        common_tokens = Counter(pred_tokens) & Counter(gt_tokens)\n        num_same = sum(common_tokens.values())\n        \n        if num_same == 0:\n            return 0.0\n        \n        precision = num_same / len(pred_tokens)\n        recall = num_same / len(gt_tokens)\n        \n        return 2 * precision * recall / (precision + recall)\n    \n    def citation_precision(self, pred_citations: List[int], gold_citations: List[int]) -> float:\n        \"\"\"\n        Calculate Citation Precision\n        \n        Formula: (Correct predicted citations) / (Total predicted citations)\n        Measures accuracy - prevents hallucinated citations\n        Special handling: Empty predictions with \"insufficient context\" = 1.0\n        \"\"\"\n        if not pred_citations:\n            # If no predictions and gold is also empty, perfect precision\n            return 1.0 if not gold_citations else 0.0\n        \n        pred_set = set(pred_citations)\n        gold_set = set(gold_citations)\n        \n        correct = len(pred_set & gold_set)\n        return correct / len(pred_set)\n    \n    def citation_recall(self, pred_citations: List[int], gold_citations: List[int]) -> float:\n        \"\"\"\n        Calculate Citation Recall\n        \n        Formula: (Correct predicted citations) / (Total ground truth citations)\n        Measures completeness - ensures all evidence is identified\n        Special handling: Empty gold with \"insufficient context\" = 1.0\n        \"\"\"\n        if not gold_citations:\n            # If no gold citations and predictions are also empty, perfect recall\n            return 1.0 if not pred_citations else 0.0\n        \n        pred_set = set(pred_citations)\n        gold_set = set(gold_citations)\n        \n        correct = len(pred_set & gold_set)\n        return correct / len(gold_set)\n    \n    def citation_f1(self, pred_citations: List[int], gold_citations: List[int]) -> float:\n        \"\"\"\n        Calculate Citation F1 Score\n        \n        Formula: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n        Balances citation accuracy and completeness\n        \"\"\"\n        precision = self.citation_precision(pred_citations, gold_citations)\n        recall = self.citation_recall(pred_citations, gold_citations)\n        \n        if precision + recall == 0:\n            return 0.0\n        \n        return 2 * precision * recall / (precision + recall)\n    \n    def insufficient_context_detection(self, prediction: str, ground_truth: str) -> float:\n        \"\"\"\n        Calculate Insufficient Context Detection Rate\n        \n        Returns 1.0 when model correctly identifies insufficient context cases\n        Returns 0.0 when model fails to recognize insufficient context\n        Only applies when ground truth is \"insufficient context\"\n        \"\"\"\n        gt_insufficient = self.normalize_answer(ground_truth) == 'insufficient context'\n        \n        if not gt_insufficient:\n            # Not an insufficient context case, return None (not applicable)\n            return None\n        \n        pred_insufficient = self.normalize_answer(prediction) == 'insufficient context'\n        return float(pred_insufficient)\n    \n    def evaluate_single(self, prediction: Dict, gold_data: Dict) -> Dict[str, float]:\n        \"\"\"\n        Evaluate a single HotpotQA prediction with all 6 generation quality metrics\n        \n        Args:\n            prediction: {\n                'answer': str,\n                'citations': List[int]  # List of supporting passage indices\n            }\n            gold_data: {\n                'answer': str,\n                'citations': List[int]\n            }\n            \n        Returns:\n            Dictionary with all 6 evaluation metrics\n        \"\"\"\n        # 1. Answer F1\n        ans_f1 = self.answer_f1(prediction['answer'], gold_data['answer'])\n        \n        # 2. Answer EM\n        ans_em = self.answer_exact_match(prediction['answer'], gold_data['answer'])\n        \n        # 3. Citation Precision\n        cit_precision = self.citation_precision(\n            prediction.get('citations', []), \n            gold_data.get('citations', [])\n        )\n        \n        # 4. Citation Recall\n        cit_recall = self.citation_recall(\n            prediction.get('citations', []), \n            gold_data.get('citations', [])\n        )\n        \n        # 5. Citation F1\n        cit_f1 = self.citation_f1(\n            prediction.get('citations', []), \n            gold_data.get('citations', [])\n        )\n        \n        # 6. Insufficient Context Detection (may be None if not applicable)\n        insuf_detection = self.insufficient_context_detection(\n            prediction['answer'], \n            gold_data['answer']\n        )\n        \n        return {\n            'answer_f1': ans_f1,\n            'answer_em': ans_em,\n            'citation_precision': cit_precision,\n            'citation_recall': cit_recall,\n            'citation_f1': cit_f1,\n            'insufficient_context_detection': insuf_detection\n        }\n    \n    def evaluate_batch(self, predictions: List[Dict], gold_data: List[Dict]) -> Dict[str, float]:\n        \"\"\"\n        Evaluate a batch of HotpotQA predictions\n        \n        Returns averages of all 6 generation quality metrics.\n        Insufficient context detection is averaged only over applicable cases.\n        \"\"\"\n        if len(predictions) != len(gold_data):\n            raise ValueError(\"Predictions and gold data must have the same length\")\n        \n        all_metrics = {\n            'answer_f1': [],\n            'answer_em': [],\n            'citation_precision': [],\n            'citation_recall': [],\n            'citation_f1': [],\n            'insufficient_context_detection': []\n        }\n        \n        for pred, gold in zip(predictions, gold_data):\n            single_result = self.evaluate_single(pred, gold)\n            \n            for metric, value in single_result.items():\n                if value is not None:  # Only include non-None values\n                    all_metrics[metric].append(value)\n        \n        # Calculate averages (insufficient context detection averaged over applicable cases only)\n        return {\n            metric: np.mean(scores) if scores else 0.0 \n            for metric, scores in all_metrics.items()\n        }\n\n# Initialize evaluator\nevaluator = HotpotQAGenerationEvaluator()\nprint(\"\\n‚úÖ HotpotQA Generation Quality Evaluator initialized successfully!\")\nprint(\"üöÄ Ready for comprehensive answer generation evaluation with citation accuracy!\")"
  },
  {
   "cell_type": "markdown",
   "id": "jf8y651j36a",
   "source": "## Understanding the 6 Generation Quality Metrics\n\nThis evaluation framework measures **answer generation quality** across three dimensions:\n\n### 1. Answer Correctness (F1 & EM)\n- **Answer F1**: Token-level overlap providing partial credit for semantically similar answers\n- **Answer EM**: Strict binary metric requiring exact match after normalization\n\n### 2. Citation Accuracy (Precision, Recall, F1)\n- **Citation Precision**: Prevents hallucinated citations - measures accuracy\n- **Citation Recall**: Ensures complete evidence chains - measures completeness  \n- **Citation F1**: Balanced metric combining precision and recall\n\n### 3. Edge Case Handling (Insufficient Context Detection)\n- Tests model's ability to recognize when questions cannot be answered\n- Prevents confident wrong answers on unanswerable questions\n- Critical for deployment reliability\n\n### Why This Matters for RAG Systems\n\nUnlike traditional QA, RAG systems must not only generate correct answers but also:\n1. **Cite sources accurately** - Preventing fabricated evidence\n2. **Identify all required evidence** - Demonstrating complete reasoning\n3. **Recognize limitations** - Admitting when context is insufficient\n\nThese 6 metrics comprehensively evaluate all three requirements.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "rq3rgpmvzh",
   "source": "# Demonstration of 6 Generation Quality Metrics\nprint(\"üéØ GENERATION QUALITY METRICS DEMONSTRATION\")\nprint(\"=\"*60)\nprint(\"Testing comprehensive evaluation with realistic HotpotQA examples\\n\")\n\n# Create realistic examples showcasing different generation quality scenarios\ngeneration_examples = [\n    {\n        \"name\": \"Perfect Generation\",\n        \"description\": \"Correct answer with accurate and complete citations\",\n        \"question\": \"Which magazine was started first Arthur's Magazine or First for Women?\",\n        \"gold_data\": {\n            'answer': \"Arthur's Magazine\",\n            'citations': [1, 2]  # Both supporting passages\n        },\n        \"prediction\": {\n            'answer': \"Arthur's Magazine\",\n            'citations': [1, 2]\n        }\n    },\n    {\n        \"name\": \"Partial Answer Match\",\n        \"description\": \"Semantically close answer with perfect citations\",\n        \"question\": \"How is COVID-19 primarily transmitted?\",\n        \"gold_data\": {\n            'answer': \"respiratory droplets and aerosols\",\n            'citations': [1, 3]\n        },\n        \"prediction\": {\n            'answer': \"respiratory droplets\",  # Partial but semantically similar\n            'citations': [1, 3]\n        }\n    },\n    {\n        \"name\": \"Citation Precision Issue\",\n        \"description\": \"Correct answer but includes incorrect citation\",\n        \"question\": \"What position did the drafted player play?\",\n        \"gold_data\": {\n            'answer': \"point guard\",\n            'citations': [2, 4]\n        },\n        \"prediction\": {\n            'answer': \"point guard\",\n            'citations': [2, 4, 5]  # Added wrong citation\n        }\n    },\n    {\n        \"name\": \"Citation Recall Issue\",\n        \"description\": \"Correct answer but missing required citation\",\n        \"question\": \"What year was the magazine founded?\",\n        \"gold_data\": {\n            'answer': \"1844\",\n            'citations': [1, 3]\n        },\n        \"prediction\": {\n            'answer': \"1844\",\n            'citations': [1]  # Missing citation 3\n        }\n    },\n    {\n        \"name\": \"Perfect Insufficient Context Detection\",\n        \"description\": \"Correctly identifies unanswerable question\",\n        \"question\": \"What is the population of the fictional city?\",\n        \"gold_data\": {\n            'answer': \"insufficient context\",\n            'citations': []\n        },\n        \"prediction\": {\n            'answer': \"insufficient context\",\n            'citations': []\n        }\n    },\n    {\n        \"name\": \"Failed Insufficient Context Detection\",\n        \"description\": \"Hallucinates answer when should say insufficient context\",\n        \"question\": \"What is the exact profit margin mentioned?\",\n        \"gold_data\": {\n            'answer': \"insufficient context\",\n            'citations': []\n        },\n        \"prediction\": {\n            'answer': \"25 percent\",  # Hallucinated answer\n            'citations': [2]\n        }\n    },\n    {\n        \"name\": \"Complete Failure\",\n        \"description\": \"Wrong answer with wrong citations\",\n        \"question\": \"Which company founded by Elon Musk focuses on space travel?\",\n        \"gold_data\": {\n            'answer': \"SpaceX\",\n            'citations': [1, 3]\n        },\n        \"prediction\": {\n            'answer': \"Tesla\",\n            'citations': [2]\n        }\n    }\n]\n\nprint(\"üìä EVALUATION RESULTS FOR EACH SCENARIO:\")\nprint(\"=\"*60)\n\nresults_summary = []\n\nfor example in generation_examples:\n    print(f\"\\nüîç {example['name']}\")\n    print(f\"   {example['description']}\")\n    print(f\"   Q: {example['question']}\")\n    print(\"-\" * 50)\n    \n    # Evaluate the example\n    result = evaluator.evaluate_single(example['prediction'], example['gold_data'])\n    \n    # Display all 6 metrics\n    print(f\"üìã Metrics:\")\n    print(f\"   1. Answer F1:           {result['answer_f1']:.3f}\")\n    print(f\"   2. Answer EM:           {result['answer_em']:.3f}\")\n    print(f\"   3. Citation Precision:  {result['citation_precision']:.3f}\")\n    print(f\"   4. Citation Recall:     {result['citation_recall']:.3f}\")\n    print(f\"   5. Citation F1:         {result['citation_f1']:.3f}\")\n    \n    # Insufficient context detection (only applicable for some cases)\n    if result['insufficient_context_detection'] is not None:\n        print(f\"   6. Insuf. Context Det.: {result['insufficient_context_detection']:.3f}\")\n    else:\n        print(f\"   6. Insuf. Context Det.: N/A (not applicable)\")\n    \n    # Add to summary\n    row = {\n        'Scenario': example['name'],\n        'Ans_F1': f\"{result['answer_f1']:.3f}\",\n        'Ans_EM': f\"{result['answer_em']:.3f}\",\n        'Cit_Prec': f\"{result['citation_precision']:.3f}\",\n        'Cit_Rec': f\"{result['citation_recall']:.3f}\",\n        'Cit_F1': f\"{result['citation_f1']:.3f}\"\n    }\n    results_summary.append(row)\n    \n    # Performance insights\n    print(f\"üí° Insight:\", end=\" \")\n    if result['answer_f1'] == 1.0 and result['citation_f1'] == 1.0:\n        print(\"Perfect generation - answer and citations both correct!\")\n    elif result['answer_f1'] > result['answer_em']:\n        print(\"F1 captures partial credit where EM gives 0.0\")\n    elif result['citation_precision'] < 1.0:\n        print(\"Citation hallucination - includes incorrect citations\")\n    elif result['citation_recall'] < 1.0:\n        print(\"Incomplete evidence - missing required citations\")\n    elif result['insufficient_context_detection'] == 0.0:\n        print(\"Failed to recognize unanswerable question - reliability issue\")\n    elif result['insufficient_context_detection'] == 1.0:\n        print(\"Correctly identified insufficient context - reliable system\")\n    else:\n        print(\"Complete failure in answer generation\")\n\n# Create summary table\nprint(f\"\\nüìä SUMMARY TABLE:\")\nprint(\"=\"*90)\nsummary_df = pd.DataFrame(results_summary)\nprint(summary_df.to_string(index=False))\n\nprint(f\"\\nüí° KEY INSIGHTS:\")\nprint(\"=\"*50)\nprint(\"1. üéØ Answer F1 vs EM: F1 rewards partial semantic similarity\")\nprint(\"2. üîç Citation Precision: Prevents hallucinated citations\")\nprint(\"3. üìö Citation Recall: Ensures complete evidence chains\")\nprint(\"4. ‚öñÔ∏è Citation F1: Balances accuracy and completeness\")\nprint(\"5. üîê Insufficient Context: Critical for reliable deployments\")\nprint(\"6. üìä Together: Comprehensive answer generation assessment\")\n\nprint(f\"\\nüöÄ This framework evaluates:\")\nprint(\"‚Ä¢ Answer correctness (both strict and lenient)\")\nprint(\"‚Ä¢ Citation accuracy (prevents hallucination)\")\nprint(\"‚Ä¢ Citation completeness (ensures full reasoning)\")\nprint(\"‚Ä¢ Edge case handling (recognizes limitations)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f8948",
   "metadata": {},
   "outputs": [],
   "source": "# Batch Evaluation Example\nprint(\"üìä BATCH EVALUATION WITH 6 GENERATION QUALITY METRICS\")\nprint(\"=\"*60)\nprint(\"Demonstrating batch evaluation for RAG system assessment\\n\")\n\n# Simulate batch predictions from a RAG system\nbatch_predictions = [\n    {\n        'answer': \"Arthur's Magazine\",\n        'citations': [1, 2]\n    },\n    {\n        'answer': \"respiratory droplets\",\n        'citations': [1, 3]\n    },\n    {\n        'answer': \"point guard\",\n        'citations': [2, 4, 5]  # Extra citation\n    },\n    {\n        'answer': \"1844\",\n        'citations': [1]  # Missing citation\n    },\n    {\n        'answer': \"insufficient context\",\n        'citations': []\n    },\n    {\n        'answer': \"2.5 million\",\n        'citations': [1, 2]\n    },\n    {\n        'answer': \"Tesla\",  # Wrong answer\n        'citations': [2]\n    },\n    {\n        'answer': \"insufficient context\",\n        'citations': []\n    }\n]\n\n# Corresponding ground truth data\nbatch_gold_data = [\n    {\n        'answer': \"Arthur's Magazine\",\n        'citations': [1, 2]\n    },\n    {\n        'answer': \"respiratory droplets and aerosols\",\n        'citations': [1, 3]\n    },\n    {\n        'answer': \"point guard\",\n        'citations': [2, 4]\n    },\n    {\n        'answer': \"1844\",\n        'citations': [1, 3]\n    },\n    {\n        'answer': \"insufficient context\",\n        'citations': []\n    },\n    {\n        'answer': \"2.5 million\",\n        'citations': [1, 2]\n    },\n    {\n        'answer': \"SpaceX\",\n        'citations': [1, 3]\n    },\n    {\n        'answer': \"insufficient context\",\n        'citations': []\n    }\n]\n\nprint(\"üéØ Evaluating batch of 8 predictions...\")\n\n# Perform batch evaluation\nbatch_results = evaluator.evaluate_batch(batch_predictions, batch_gold_data)\n\nprint(f\"\\nüìä BATCH EVALUATION RESULTS (Average across 8 examples):\")\nprint(\"=\"*60)\nprint(f\"1. Answer F1:                    {batch_results['answer_f1']:.3f}\")\nprint(f\"2. Answer EM:                    {batch_results['answer_em']:.3f}\")\nprint(f\"3. Citation Precision:           {batch_results['citation_precision']:.3f}\")\nprint(f\"4. Citation Recall:              {batch_results['citation_recall']:.3f}\")\nprint(f\"5. Citation F1:                  {batch_results['citation_f1']:.3f}\")\nprint(f\"6. Insufficient Context Detection: {batch_results['insufficient_context_detection']:.3f}\")\n\n# Per-example detailed analysis\nprint(f\"\\nüìù DETAILED PER-EXAMPLE ANALYSIS:\")\nprint(\"=\"*90)\n\ndetailed_results = []\nfor i, (pred, gold) in enumerate(zip(batch_predictions, batch_gold_data)):\n    result = evaluator.evaluate_single(pred, gold)\n    \n    insuf_str = f\"{result['insufficient_context_detection']:.3f}\" if result['insufficient_context_detection'] is not None else \"N/A\"\n    \n    row = {\n        \"Ex\": f\"#{i+1}\",\n        \"Ans_F1\": f\"{result['answer_f1']:.3f}\",\n        \"Ans_EM\": f\"{result['answer_em']:.3f}\",\n        \"Cit_Prec\": f\"{result['citation_precision']:.3f}\",\n        \"Cit_Rec\": f\"{result['citation_recall']:.3f}\",\n        \"Cit_F1\": f\"{result['citation_f1']:.3f}\",\n        \"Insuf_Det\": insuf_str\n    }\n    detailed_results.append(row)\n\ndetailed_df = pd.DataFrame(detailed_results)\nprint(detailed_df.to_string(index=False))\n\n# Performance insights\nprint(f\"\\nüîç BATCH PERFORMANCE INSIGHTS:\")\nprint(\"=\"*50)\n\n# Count success metrics\nperfect_answer_em = sum(1 for result in detailed_results if float(result['Ans_EM']) == 1.0)\nperfect_citation_f1 = sum(1 for result in detailed_results if float(result['Cit_F1']) == 1.0)\ninsuf_cases = sum(1 for result in detailed_results if result['Insuf_Det'] != \"N/A\")\n\nprint(f\"üìä Success Rates:\")\nprint(f\"   ‚Ä¢ Perfect Answer EM: {perfect_answer_em}/8 ({(perfect_answer_em/8)*100:.0f}%)\")\nprint(f\"   ‚Ä¢ Perfect Citation F1: {perfect_citation_f1}/8 ({(perfect_citation_f1/8)*100:.0f}%)\")\nprint(f\"   ‚Ä¢ Insufficient Context Cases: {insuf_cases}/8 ({(insuf_cases/8)*100:.0f}%)\")\n\nprint(f\"\\nüí° KEY OBSERVATIONS:\")\nprint(\"‚Ä¢ Answer EM is strict - requires perfect match\")\nprint(\"‚Ä¢ Answer F1 captures partial semantic overlap\")\nprint(\"‚Ä¢ Citation metrics reveal evidence selection quality\")\nprint(\"‚Ä¢ Insufficient context detection measures reliability\")\nprint(\"‚Ä¢ Together: Comprehensive generation quality assessment\")\n\nprint(f\"\\nüöÄ USE CASES FOR BATCH EVALUATION:\")\nprint(\"‚Ä¢ Compare different RAG system variants\")\nprint(\"‚Ä¢ Track model improvements over time\")\nprint(\"‚Ä¢ Identify systematic failure patterns\")\nprint(\"‚Ä¢ Validate citation accuracy and completeness\")\nprint(\"‚Ä¢ Ensure reliable handling of edge cases\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6ce98",
   "metadata": {},
   "outputs": [],
   "source": "# Visualization of 6 Generation Quality Metrics\nprint(\"üìä VISUALIZATION OF 6 GENERATION QUALITY METRICS\")\nprint(\"=\"*60)\n\n# Create comprehensive visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('HotpotQA Answer Generation Quality: 6-Metric Evaluation', fontsize=16, fontweight='bold')\n\n# Extract data from detailed results\nexamples = [result['Ex'] for result in detailed_results]\nanswer_f1 = [float(result['Ans_F1']) for result in detailed_results]\nanswer_em = [float(result['Ans_EM']) for result in detailed_results]\ncit_prec = [float(result['Cit_Prec']) for result in detailed_results]\ncit_rec = [float(result['Cit_Rec']) for result in detailed_results]\ncit_f1 = [float(result['Cit_F1']) for result in detailed_results]\n\n# 1. Answer Metrics Comparison (F1 vs EM)\nax = axes[0, 0]\nx = np.arange(len(examples))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, answer_f1, width, label='Answer F1', alpha=0.8, color='#4ECDC4')\nbars2 = ax.bar(x + width/2, answer_em, width, label='Answer EM', alpha=0.8, color='#FF6B6B')\n\nax.set_xlabel('Examples', fontweight='bold')\nax.set_ylabel('Score', fontweight='bold')\nax.set_title('1. Answer F1 vs Answer EM', fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(examples)\nax.legend()\nax.set_ylim(0, 1.1)\nax.grid(True, alpha=0.3, axis='y')\n\n# 2. Citation Metrics (Precision, Recall, F1)\nax = axes[0, 1]\nx = np.arange(len(examples))\nwidth = 0.25\n\nbars1 = ax.bar(x - width, cit_prec, width, label='Precision', alpha=0.8, color='#95E1D3')\nbars2 = ax.bar(x, cit_rec, width, label='Recall', alpha=0.8, color='#F38181')\nbars3 = ax.bar(x + width, cit_f1, width, label='F1', alpha=0.8, color='#FFAA5A')\n\nax.set_xlabel('Examples', fontweight='bold')\nax.set_ylabel('Score', fontweight='bold')\nax.set_title('2. Citation Metrics (Prec, Rec, F1)', fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(examples)\nax.legend()\nax.set_ylim(0, 1.1)\nax.grid(True, alpha=0.3, axis='y')\n\n# 3. Answer F1 Distribution\nax = axes[0, 2]\nax.hist(answer_f1, bins=10, alpha=0.7, color='#4ECDC4', edgecolor='black')\nax.axvline(np.mean(answer_f1), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(answer_f1):.3f}')\nax.set_xlabel('Answer F1 Score', fontweight='bold')\nax.set_ylabel('Frequency', fontweight='bold')\nax.set_title('3. Answer F1 Distribution', fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\n# 4. Citation F1 Distribution\nax = axes[1, 0]\nax.hist(cit_f1, bins=10, alpha=0.7, color='#FFAA5A', edgecolor='black')\nax.axvline(np.mean(cit_f1), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(cit_f1):.3f}')\nax.set_xlabel('Citation F1 Score', fontweight='bold')\nax.set_ylabel('Frequency', fontweight='bold')\nax.set_title('4. Citation F1 Distribution', fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\n# 5. Heatmap of All Metrics\nax = axes[1, 1]\nheatmap_data = np.array([answer_f1, answer_em, cit_prec, cit_rec, cit_f1])\nmetric_names = ['Ans F1', 'Ans EM', 'Cit Prec', 'Cit Rec', 'Cit F1']\n\nim = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\nax.set_yticks(range(len(metric_names)))\nax.set_yticklabels(metric_names, fontweight='bold')\nax.set_xticks(range(len(examples)))\nax.set_xticklabels(examples)\nax.set_title('5. Performance Heatmap', fontweight='bold')\n\n# Add colorbar\ncbar = plt.colorbar(im, ax=ax)\ncbar.set_label('Score', fontweight='bold')\n\n# 6. Overall Performance Radar Chart\nax = axes[1, 2]\nmetrics = ['Answer F1', 'Answer EM', 'Citation\\nPrecision', 'Citation\\nRecall', 'Citation F1']\navg_values = [\n    np.mean(answer_f1),\n    np.mean(answer_em),\n    np.mean(cit_prec),\n    np.mean(cit_rec),\n    np.mean(cit_f1)\n]\n\n# Create radar chart\nangles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)\navg_values_plot = avg_values + avg_values[:1]  # Complete the circle\nangles_plot = np.concatenate((angles, [angles[0]]))\n\nax = plt.subplot(2, 3, 6, projection='polar')\nax.plot(angles_plot, avg_values_plot, 'o-', linewidth=2, color='#FF6B6B', label='Avg Performance')\nax.fill(angles_plot, avg_values_plot, alpha=0.25, color='#FF6B6B')\nax.set_xticks(angles)\nax.set_xticklabels(metrics, fontweight='bold')\nax.set_ylim(0, 1)\nax.set_title('6. Overall Performance Profile', fontweight='bold', pad=20)\nax.grid(True)\nax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüîç VISUALIZATION INSIGHTS:\")\nprint(\"=\"*50)\nprint(\"1. üìä Answer F1 vs EM: Shows where partial credit helps\")\nprint(\"2. üéØ Citation Metrics: Reveals precision/recall trade-offs\")\nprint(\"3. üìà Distributions: Identify system consistency\")\nprint(\"4. üå°Ô∏è Heatmap: Quick performance overview per example\")\nprint(\"5. üéÆ Radar Chart: Balanced view of all metrics\")\n\nprint(f\"\\nüí° INTERPRETATION GUIDE:\")\nprint(\"‚Ä¢ High Answer F1, Low EM: Partial semantic matches\")\nprint(\"‚Ä¢ Low Citation Precision: Hallucinated citations\")\nprint(\"‚Ä¢ Low Citation Recall: Missing evidence\")\nprint(\"‚Ä¢ Balanced metrics: Reliable RAG system\")\n\nprint(f\"\\n‚úÖ 6-Metric Framework Complete!\")\nprint(\"üöÄ Ready for production RAG system evaluation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14129e86",
   "metadata": {},
   "outputs": [],
   "source": "# Utility Functions for Generation Quality Evaluation\nprint(\"üîß UTILITY FUNCTIONS FOR GENERATION QUALITY EVALUATION\")\nprint(\"=\"*60)\n\ndef save_evaluation_results(results_dict: Dict, filename: str = \"generation_evaluation_results.json\"):\n    \"\"\"Save generation quality evaluation results to JSON file\"\"\"\n    import json\n    with open(filename, 'w') as f:\n        json.dump(results_dict, f, indent=2)\n    print(f\"üìÅ Results saved to {filename}\")\n\ndef load_evaluation_results(filename: str = \"generation_evaluation_results.json\"):\n    \"\"\"Load generation quality evaluation results from JSON file\"\"\"\n    import json\n    with open(filename, 'r') as f:\n        results = json.load(f)\n    print(f\"üìÅ Results loaded from {filename}\")\n    return results\n\ndef compare_rag_systems(system_results: Dict[str, Dict]) -> pd.DataFrame:\n    \"\"\"\n    Create comparison table of different RAG systems using 6 generation quality metrics\n    \n    Args:\n        system_results: Dict mapping system names to their evaluation results\n        \n    Returns:\n        DataFrame with side-by-side comparison\n    \"\"\"\n    comparison_data = []\n    \n    for system_name, results in system_results.items():\n        row = {\n            'System': system_name,\n            'Answer F1': f\"{results['answer_f1']:.3f}\",\n            'Answer EM': f\"{results['answer_em']:.3f}\",\n            'Cit Precision': f\"{results['citation_precision']:.3f}\",\n            'Cit Recall': f\"{results['citation_recall']:.3f}\",\n            'Cit F1': f\"{results['citation_f1']:.3f}\",\n            'Insuf Context Det': f\"{results.get('insufficient_context_detection', 0.0):.3f}\"\n        }\n        comparison_data.append(row)\n    \n    return pd.DataFrame(comparison_data)\n\ndef evaluate_from_files(predictions_file: str, gold_file: str) -> Dict[str, float]:\n    \"\"\"\n    Evaluate predictions from JSON files using 6 generation quality metrics\n    \n    Args:\n        predictions_file: JSON file with list of predictions\n        gold_file: JSON file with list of ground truth data\n        \n    Expected format:\n        predictions: [{'answer': str, 'citations': List[int]}, ...]\n        gold_data: [{'answer': str, 'citations': List[int]}, ...]\n    \n    Returns:\n        Dictionary with averaged metrics\n    \"\"\"\n    import json\n    \n    # Load predictions and ground truth\n    with open(predictions_file, 'r') as f:\n        predictions = json.load(f)\n    \n    with open(gold_file, 'r') as f:\n        gold_data = json.load(f)\n    \n    # Evaluate using 6 metrics\n    results = evaluator.evaluate_batch(predictions, gold_data)\n    \n    print(f\"üìä Generation Quality Evaluation Results:\")\n    print(f\"   1. Answer F1:                    {results['answer_f1']:.3f}\")\n    print(f\"   2. Answer EM:                    {results['answer_em']:.3f}\")\n    print(f\"   3. Citation Precision:           {results['citation_precision']:.3f}\")\n    print(f\"   4. Citation Recall:              {results['citation_recall']:.3f}\")\n    print(f\"   5. Citation F1:                  {results['citation_f1']:.3f}\")\n    print(f\"   6. Insufficient Context Detection: {results['insufficient_context_detection']:.3f}\")\n    \n    return results\n\ndef create_performance_report(system_results: Dict[str, Dict], output_file: str = \"generation_quality_report.html\"):\n    \"\"\"\n    Create HTML performance report comparing RAG systems on generation quality\n    \n    Args:\n        system_results: Dict mapping system names to evaluation results\n        output_file: Output HTML filename\n    \"\"\"\n    comparison_df = compare_rag_systems(system_results)\n    \n    html_content = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>HotpotQA Answer Generation Quality Report</title>\n        <style>\n            body {{ font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }}\n            .container {{ background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n            h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}\n            h2 {{ color: #34495e; margin-top: 30px; }}\n            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: center; }}\n            th {{ background-color: #3498db; color: white; font-weight: bold; }}\n            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n            .metric-desc {{ font-style: italic; color: #666; margin: 10px 0; }}\n            .highlight {{ background-color: #ffffcc; }}\n        </style>\n    </head>\n    <body>\n        <div class=\"container\">\n            <h1>üéØ HotpotQA Answer Generation Quality Evaluation Report</h1>\n            \n            <h2>üìä 6 Generation Quality Metrics</h2>\n            <ul>\n                <li><strong>Answer F1:</strong> Token-level overlap providing partial credit for answers</li>\n                <li><strong>Answer EM:</strong> Exact match for strict answer correctness</li>\n                <li><strong>Citation Precision:</strong> Percentage of predicted citations that are correct</li>\n                <li><strong>Citation Recall:</strong> Percentage of ground truth citations identified</li>\n                <li><strong>Citation F1:</strong> Harmonic mean of citation precision and recall</li>\n                <li><strong>Insufficient Context Detection:</strong> Accuracy in identifying unanswerable questions</li>\n            </ul>\n            \n            <h2>üîç System Comparison Results</h2>\n            {comparison_df.to_html(index=False, classes='comparison-table')}\n            \n            <h2>üí° Interpretation Guide</h2>\n            <ul>\n                <li><strong>High Answer F1/EM:</strong> Accurate answer generation</li>\n                <li><strong>High Citation Precision:</strong> No hallucinated citations</li>\n                <li><strong>High Citation Recall:</strong> Complete evidence identification</li>\n                <li><strong>High Citation F1:</strong> Balanced citation quality</li>\n                <li><strong>High Insufficient Context Detection:</strong> Reliable edge case handling</li>\n            </ul>\n            \n            <h2>üéØ Framework Features</h2>\n            <ul>\n                <li>‚úÖ Comprehensive answer correctness assessment (F1 & EM)</li>\n                <li>‚úÖ Citation accuracy and completeness measurement</li>\n                <li>‚úÖ Edge case reliability testing</li>\n                <li>‚úÖ RAG-specific metrics for trustworthy systems</li>\n            </ul>\n        </div>\n    </body>\n    </html>\n    \"\"\"\n    \n    with open(output_file, 'w') as f:\n        f.write(html_content)\n    \n    print(f\"üìä Performance report saved to {output_file}\")\n\n# Example usage templates\nprint(\"\\nüìã USAGE EXAMPLES:\")\nprint(\"=\"*50)\nprint(\"1. Evaluate single prediction:\")\nprint(\"   result = evaluator.evaluate_single(prediction, gold_data)\")\nprint()\nprint(\"2. Evaluate batch of predictions:\")\nprint(\"   results = evaluator.evaluate_batch(predictions, gold_data)\")\nprint()\nprint(\"3. Compare RAG systems:\")\nprint(\"   df = compare_rag_systems({'GPT-4': gpt4_results, 'Claude': claude_results})\")\nprint()\nprint(\"4. Evaluate from files:\")\nprint(\"   results = evaluate_from_files('predictions.json', 'gold.json')\")\nprint()\nprint(\"5. Generate HTML report:\")\nprint(\"   create_performance_report({'System1': results1, 'System2': results2})\")\n\nprint(\"\\n‚úÖ Generation Quality Evaluation Framework Complete!\")\nprint(\"üéØ 6 metrics: Answer correctness + Citation accuracy + Edge case handling\")\nprint(\"üöÄ Ready for comprehensive RAG system evaluation\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322de60a",
   "metadata": {},
   "outputs": [],
   "source": "## Framework Summary\n\nThis notebook provides a **complete evaluation framework for HotpotQA answer generation quality** using 6 comprehensive metrics that assess the core requirements of trustworthy RAG systems.\n\n### ‚úÖ What This Framework Provides\n\n**1. Answer Correctness Assessment (2 metrics)**\n- Answer F1: Lenient, rewards partial semantic matches\n- Answer EM: Strict, requires perfect answers\n\n**2. Citation Quality Measurement (3 metrics)**\n- Citation Precision: Prevents hallucinated citations\n- Citation Recall: Ensures complete evidence chains\n- Citation F1: Balanced citation quality metric\n\n**3. Edge Case Reliability (1 metric)**\n- Insufficient Context Detection: Tests recognition of unanswerable questions\n\n### üéØ Key Features\n\n- **Standardized normalization**: Removes articles, punctuation, normalizes whitespace\n- **Special handling**: Proper treatment of \"insufficient context\" cases\n- **Batch evaluation**: Efficient processing of multiple predictions\n- **Comparison tools**: Side-by-side RAG system comparisons\n- **Export capabilities**: JSON results and HTML reports\n- **Visualization suite**: Comprehensive charts and heatmaps\n\n### üî¨ Metric Rationale\n\nThese 6 metrics were specifically chosen because they:\n1. **Cover all aspects** of generation quality (correctness + grounding + reliability)\n2. **Provide both strict and lenient** assessment (EM vs F1)\n3. **Are RAG-specific** (citations are unique to RAG systems)\n4. **Ensure trustworthiness** (insufficient context detection prevents hallucination)\n5. **Enable comparison** to major QA benchmarks (SQuAD-style F1/EM)\n\n### üìä Use Cases\n\n- ‚úÖ Evaluate RAG system answer generation quality\n- ‚úÖ Compare different generation strategies (greedy vs sampling)\n- ‚úÖ Validate citation accuracy and completeness\n- ‚úÖ Test reliability on edge cases\n- ‚úÖ Track improvements during model development\n- ‚úÖ Generate performance reports for stakeholders\n\n### üöÄ Ready for Production\n\nThis framework is production-ready and suitable for:\n- Academic research papers\n- Industrial RAG system deployment\n- Model comparison studies\n- Continuous evaluation pipelines\n\n---\n\n**Note**: This framework evaluates **generation quality**. For **retrieval quality evaluation** (Document Recall, Supporting-Fact F1), see the separate retrieval evaluation framework."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}