{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0065640c",
   "metadata": {
    "id": "0065640c"
   },
   "source": [
    "##  Setup, Dependencies, and Data Loading\n",
    "\n",
    "This section focuses on setting up the environment for QLoRA training, including installing necessary packages, authenticating with required services (like Weights & Biases and Hugging Face), and loading the HotpotQA dataset.\n",
    "\n",
    "### \ud83d\udce6 Installing Required Packages\n",
    "\n",
    "We begin by installing the Python libraries necessary for our QLoRA fine-tuning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addba08",
   "metadata": {
    "id": "1addba08"
   },
   "source": [
    "**Note**: Replace `[repository_url]` with the actual URL of the Git repository you want to clone. You can find repositories related to Claude on platforms like GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d06b0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53000,
     "status": "ok",
     "timestamp": 1760019507406,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "a4d06b0a",
    "outputId": "e8cc3bcc-6a57-48d3-9530-8e3292102bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udd27 Installing required packages for RTX A5000...\n",
      "\ud83d\udce6 Installing transformers>=4.36.0... Latest Transformers with Mistral support\n",
      "\u2705 transformers>=4.36.0 installed successfully\n",
      "\ud83d\udce6 Installing peft>=0.7.0... Parameter-Efficient Fine-Tuning\n",
      "\u2705 peft>=0.7.0 installed successfully\n",
      "\ud83d\udce6 Installing datasets>=2.15.0... HuggingFace Datasets\n",
      "\u2705 datasets>=2.15.0 installed successfully\n",
      "\ud83d\udce6 Installing accelerate>=0.25.0... Distributed training support\n",
      "\u2705 accelerate>=0.25.0 installed successfully\n",
      "\ud83d\udce6 Installing bitsandbytes>=0.41.0... 4-bit quantization\n",
      "\u2705 bitsandbytes>=0.41.0 installed successfully\n",
      "\u2705 wandb already available\n",
      "\ud83d\udce6 Installing evaluate... Model evaluation metrics\n",
      "\u2705 evaluate installed successfully\n",
      "\ud83d\udce6 Installing scipy... Scientific computing\n",
      "\u2705 scipy installed successfully\n",
      "\ud83d\udce6 Installing scikit-learn... ML utilities\n",
      "\u2705 scikit-learn installed successfully\n",
      "\ud83d\udce6 Installing pydantic... data validation\n",
      "\u2705 pydantic installed successfully\n",
      "\n",
      "\u2705 All packages installed successfully!\n",
      "\n",
      "\ud83c\udfaf RTX A5000 Optimization Settings:\n",
      "   - Batch size: 2 (optimal for 24GB VRAM)\n",
      "   - Sequence length: 2048 (memory efficient)\n",
      "   - Gradient accumulation: 4 steps\n",
      "   - Mixed precision: BF16 (A5000 optimized)\n",
      "   - Estimated training time: 3-4 hours\n",
      "   - Estimated cost: $1.50 - $2.00\n",
      "\n",
      "\u2705 Ready for cost-effective QLoRA training!\n",
      "\ud83d\udcdd Next: Run GPU detection cell to confirm 24GB VRAM\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for PyTorch 2.1 container\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package, description=\"\"):\n",
    "    \"\"\"Install package with proper error handling\"\"\"\n",
    "    try:\n",
    "        # Check if already installed\n",
    "        if package.split('==')[0] in ['transformers', 'peft', 'datasets', 'accelerate', 'bitsandbytes', 'wandb', 'evaluate']:\n",
    "            __import__(package.split('==')[0])\n",
    "            print(f\"\u2705 {package} already available\")\n",
    "            return True\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        print(f\"\ud83d\udce6 Installing {package}... {description}\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", package])\n",
    "        print(f\"\u2705 {package} installed successfully\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\u274c Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Essential packages for QLoRA training (compatible with PyTorch 2.1.0)\n",
    "packages = [\n",
    "    (\"transformers>=4.36.0\", \"Latest Transformers with Mistral support\"),\n",
    "    (\"peft>=0.7.0\", \"Parameter-Efficient Fine-Tuning\"),\n",
    "    (\"datasets>=2.15.0\", \"HuggingFace Datasets\"),\n",
    "    (\"accelerate>=0.25.0\", \"Distributed training support\"),\n",
    "    (\"bitsandbytes>=0.41.0\", \"4-bit quantization\"),\n",
    "    (\"wandb\", \"Experiment tracking\"),\n",
    "    (\"evaluate\", \"Model evaluation metrics\"),\n",
    "    (\"scipy\", \"Scientific computing\"),\n",
    "    (\"scikit-learn\", \"ML utilities\"),\n",
    "    (\"pydantic\", \"data validation\"),\n",
    "]\n",
    "\n",
    "\n",
    "print(\"\\n\ud83d\udd27 Installing required packages for RTX A5000...\")\n",
    "failed_packages = []\n",
    "\n",
    "for package, desc in packages:\n",
    "    if not install_package(package, desc):\n",
    "        failed_packages.append(package)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"\\n\u26a0\ufe0f Failed to install: {failed_packages}\")\n",
    "    print(\"Please install manually or check container permissions\")\n",
    "else:\n",
    "    print(\"\\n\u2705 All packages installed successfully!\")\n",
    "\n",
    "print(\"\\n\ud83c\udfaf RTX A5000 Optimization Settings:\")\n",
    "print(\"   - Batch size: 2 (optimal for 24GB VRAM)\")\n",
    "print(\"   - Sequence length: 2048 (memory efficient)\")\n",
    "print(\"   - Gradient accumulation: 4 steps\")\n",
    "print(\"   - Mixed precision: BF16 (A5000 optimized)\")\n",
    "print(\"   - Estimated training time: 3-4 hours\")\n",
    "print(\"   - Estimated cost: $1.50 - $2.00\")\n",
    "\n",
    "print(\"\\n\u2705 Ready for cost-effective QLoRA training!\")\n",
    "print(\"\ud83d\udcdd Next: Run GPU detection cell to confirm 24GB VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeca62e",
   "metadata": {
    "id": "ffeca62e"
   },
   "source": [
    "###  Cloud Platform Imports\n",
    "\n",
    "Importing necessary libraries and modules, ensuring compatibility with cloud environments like RunPod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7233713",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19657,
     "status": "ok",
     "timestamp": 1760019527065,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "b7233713",
    "outputId": "b6474330-a1ed-4e1a-db81-6f4605b4d6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 All imports successful on cloud platform!\n",
      "\ud83c\udf29\ufe0f Using standard transformers + PEFT stack\n",
      "\u26a1 Ready for QLoRA training with pre-configured packages!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "from pydantic import BaseModel, Field\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML libraries (should work on cloud platforms)\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    TrainingArguments, Trainer, TrainerCallback, TrainerState\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, load_dataset\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "print(\"\u2705 All imports successful on cloud platform!\")\n",
    "print(\"\ud83c\udf29\ufe0f Using standard transformers + PEFT stack\")\n",
    "print(\"\u26a1 Ready for QLoRA training with pre-configured packages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce8efb",
   "metadata": {
    "id": "82ce8efb"
   },
   "source": [
    "###  GPU Configuration and Cost Analysis\n",
    "\n",
    "Detecting the available GPU and setting optimized parameters for QLoRA training, along with a realistic cost analysis based on dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef7b43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1760019527073,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "7fef7b43",
    "outputId": "ea688c81-c318-424f-a916-696d2b068345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd25 PyTorch version: 2.8.0+cu126\n",
      "\ud83c\udfaf CUDA available: True\n",
      "\ud83d\ude80 GPU: NVIDIA L4\n",
      "\ud83d\udcbe VRAM: 22.2 GB\n",
      "\ud83c\udfc6 RTX A5000 detected - using optimized settings\n",
      "\n",
      "\u2699\ufe0f GPU Configuration: RTX_A5000\n",
      "\ud83d\udccf Max Sequence Length: 10000 tokens\n",
      "\ud83d\udce6 Batch Size: 2 (effective: 8)\n",
      "\ud83d\udcb0 Hourly Rate: $0.5/hr\n",
      "\u26a1 Speed: 60 tokens/second\n",
      "\n",
      "\ud83d\udcca REALISTIC TRAINING ANALYSIS:\n",
      "==================================================\n",
      "\n",
      "\ud83c\udfaf Cost-optimized subset: 2,000 examples (2.2% of full dataset)\n",
      "   Steps per epoch: 250\n",
      "   Total steps: 500\n",
      "   Training time: 185.2 hours\n",
      "   \ud83d\udcb0 Total cost: $92.59\n",
      "   \u26a0\ufe0f  Very expensive - consider subset for experimentation\n",
      "\n",
      "\ud83c\udfaf Balanced training: 10,000 examples (11.1% of full dataset)\n",
      "   Steps per epoch: 1250\n",
      "   Total steps: 2500\n",
      "   Training time: 925.9 hours\n",
      "   \ud83d\udcb0 Total cost: $462.96\n",
      "   \u26a0\ufe0f  Very expensive - consider subset for experimentation\n",
      "\n",
      "\ud83c\udfaf Full dataset (expensive!): 90,347 examples (100.0% of full dataset)\n",
      "   Steps per epoch: 11293\n",
      "   Total steps: 22586\n",
      "   Training time: 8365.2 hours\n",
      "   \ud83d\udcb0 Total cost: $4182.59\n",
      "   \u26a0\ufe0f  Very expensive - consider subset for experimentation\n",
      "\n",
      "\ud83d\udcbe MEMORY UTILIZATION:\n",
      "   Base model (4-bit): 12 GB\n",
      "   Training overhead: 6 GB\n",
      "   Batch processing: 40.0 GB\n",
      "   Total required: 58.0 GB\n",
      "   Available VRAM: 22.2 GB\n",
      "   Safety headroom: -35.8 GB (-162%)\n",
      "\n",
      "\ud83c\udfaf RTX A5000 REALISTIC EXPECTATIONS:\n",
      "   \u2705 2,048 token sequences (optimal for 24GB)\n",
      "   \u2705 2\u00d74=8 effective batch size for stable gradients\n",
      "   \u2705 Professional workstation GPU performance\n",
      "   \u26a0\ufe0f  Training times are much longer than initially estimated!\n",
      "   \ud83d\udca1 Consider starting with 2K samples to test, then scale up\n",
      "   \ud83d\udcb0 Budget ~$15-20 for 2K samples, $50+ for 10K samples\n",
      "\n",
      "\u2705 Configuration set for RTX_A5000 with REALISTIC time estimates!\n"
     ]
    }
   ],
   "source": [
    "# RTX A5000 GPU Configuration (24GB VRAM optimized for cost-effectiveness)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"\ud83d\udd25 PyTorch version: {torch.__version__}\")\n",
    "print(f\"\ud83c\udfaf CUDA available: {torch.cuda.is_available()}\")\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUM_STEPS = 4\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\ud83d\ude80 GPU: {device}\")\n",
    "    print(f\"\ud83d\udcbe VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "    # RTX A5000 optimized settings\n",
    "    if \"A5000\" in device or (vram_gb >= 20 and vram_gb <= 30):\n",
    "        GPU_TYPE = \"RTX_A5000\"\n",
    "        MAX_SEQ_LENGTH = 10000  # Optimal for 24GB VRAM\n",
    "        BATCH_SIZE = 2         # Memory efficient\n",
    "        GRAD_ACCUM_STEPS = 4   # Effective batch size = 8\n",
    "        HOURLY_RATE = 0.50     # RTX A5000 RunPod price\n",
    "        SPEED_TOKENS_PER_SEC = 60  # Realistic speed\n",
    "        print(\"\ud83c\udfc6 RTX A5000 detected - using optimized settings\")\n",
    "\n",
    "    elif \"4090\" in device or (vram_gb >= 20 and vram_gb < 26):\n",
    "        GPU_TYPE = \"RTX_4090\"\n",
    "        MAX_SEQ_LENGTH = 10000\n",
    "        BATCH_SIZE = 2\n",
    "        GRAD_ACCUM_STEPS = 4\n",
    "        HOURLY_RATE = 0.34\n",
    "        SPEED_TOKENS_PER_SEC = 50\n",
    "        print(\"\u2705 RTX 4090 detected - using memory-optimized settings\")\n",
    "\n",
    "    elif \"A100\" in device or vram_gb >= 40:\n",
    "        GPU_TYPE = \"A100\"\n",
    "        MAX_SEQ_LENGTH = 10000  # Can handle longer sequences\n",
    "        BATCH_SIZE = 4         # Larger batch\n",
    "        GRAD_ACCUM_STEPS = 2   # Effective batch size = 8\n",
    "        HOURLY_RATE = 1.19     # A100 80GB RunPod price\n",
    "        SPEED_TOKENS_PER_SEC = 150  # Much faster\n",
    "        print(\"\ud83c\udfc6 A100 detected - using high-performance settings\")\n",
    "\n",
    "    else:\n",
    "        GPU_TYPE = \"Other\"\n",
    "        MAX_SEQ_LENGTH = 10000\n",
    "        BATCH_SIZE = 1\n",
    "        GRAD_ACCUM_STEPS = 8\n",
    "        HOURLY_RATE = 0.50\n",
    "        SPEED_TOKENS_PER_SEC = 30\n",
    "        print(\"\u26a0\ufe0f Unknown GPU - using conservative settings\")\n",
    "\n",
    "    print(f\"\\n\u2699\ufe0f GPU Configuration: {GPU_TYPE}\")\n",
    "    print(f\"\ud83d\udccf Max Sequence Length: {MAX_SEQ_LENGTH} tokens\")\n",
    "    print(f\"\ud83d\udce6 Batch Size: {BATCH_SIZE} (effective: {BATCH_SIZE * GRAD_ACCUM_STEPS})\")\n",
    "    print(f\"\ud83d\udcb0 Hourly Rate: ${HOURLY_RATE}/hr\")\n",
    "    print(f\"\u26a1 Speed: {SPEED_TOKENS_PER_SEC} tokens/second\")\n",
    "\n",
    "    # REALISTIC cost analysis for different dataset sizes\n",
    "    def calculate_training_cost(train_size, epochs=2):\n",
    "        effective_batch_size = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "        steps_per_epoch = train_size // effective_batch_size\n",
    "        total_steps = steps_per_epoch * epochs\n",
    "\n",
    "        # Realistic time calculation based on token processing\n",
    "        tokens_per_step = effective_batch_size * MAX_SEQ_LENGTH\n",
    "        seconds_per_step = tokens_per_step / SPEED_TOKENS_PER_SEC\n",
    "        total_hours = (total_steps * seconds_per_step) / 3600\n",
    "        total_cost = total_hours * HOURLY_RATE\n",
    "\n",
    "        return {\n",
    "            'steps_per_epoch': steps_per_epoch,\n",
    "            'total_steps': total_steps,\n",
    "            'training_hours': total_hours,\n",
    "            'total_cost': total_cost,\n",
    "            'tokens_per_step': tokens_per_step,\n",
    "            'seconds_per_step': seconds_per_step\n",
    "        }\n",
    "\n",
    "    print(f\"\\n\ud83d\udcca REALISTIC TRAINING ANALYSIS:\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Different dataset size options\n",
    "    options = [\n",
    "        (2000, \"Cost-optimized subset\"),\n",
    "        (10000, \"Balanced training\"),\n",
    "        (90347, \"Full dataset (expensive!)\")\n",
    "    ]\n",
    "\n",
    "    for train_size, description in options:\n",
    "        analysis = calculate_training_cost(train_size)\n",
    "        pct_of_full = (train_size / 90347) * 100 if train_size <= 90347 else 100\n",
    "\n",
    "        print(f\"\\n\ud83c\udfaf {description}: {train_size:,} examples ({pct_of_full:.1f}% of full dataset)\")\n",
    "        print(f\"   Steps per epoch: {analysis['steps_per_epoch']}\")\n",
    "        print(f\"   Total steps: {analysis['total_steps']}\")\n",
    "        print(f\"   Training time: {analysis['training_hours']:.1f} hours\")\n",
    "        print(f\"   \ud83d\udcb0 Total cost: ${analysis['total_cost']:.2f}\")\n",
    "\n",
    "        if analysis['training_hours'] > 100:\n",
    "            print(f\"   \u26a0\ufe0f  Very expensive - consider subset for experimentation\")\n",
    "        elif analysis['training_hours'] > 20:\n",
    "            print(f\"   \u2696\ufe0f  Moderate cost - good for serious experiments\")\n",
    "        else:\n",
    "            print(f\"   \u2705 Reasonable cost for experimentation\")\n",
    "\n",
    "    # Memory utilization analysis\n",
    "    base_model_vram = 12  # QLoRA Mistral-7B in 4-bit\n",
    "    training_overhead = 6  # Optimizer states, gradients\n",
    "    batch_vram = (BATCH_SIZE * MAX_SEQ_LENGTH * 0.002)  # Dynamic batch memory\n",
    "    total_vram_needed = base_model_vram + training_overhead + batch_vram\n",
    "\n",
    "    print(f\"\\n\ud83d\udcbe MEMORY UTILIZATION:\")\n",
    "    print(f\"   Base model (4-bit): {base_model_vram} GB\")\n",
    "    print(f\"   Training overhead: {training_overhead} GB\")\n",
    "    print(f\"   Batch processing: {batch_vram:.1f} GB\")\n",
    "    print(f\"   Total required: {total_vram_needed:.1f} GB\")\n",
    "    print(f\"   Available VRAM: {vram_gb:.1f} GB\")\n",
    "    print(f\"   Safety headroom: {vram_gb - total_vram_needed:.1f} GB ({((vram_gb - total_vram_needed)/vram_gb)*100:.0f}%)\")\n",
    "\n",
    "    if GPU_TYPE == \"RTX_A5000\":\n",
    "        print(f\"\\n\ud83c\udfaf RTX A5000 REALISTIC EXPECTATIONS:\")\n",
    "        print(f\"   \u2705 2,048 token sequences (optimal for 24GB)\")\n",
    "        print(f\"   \u2705 2\u00d74=8 effective batch size for stable gradients\")\n",
    "        print(f\"   \u2705 Professional workstation GPU performance\")\n",
    "        print(f\"   \u26a0\ufe0f  Training times are much longer than initially estimated!\")\n",
    "        print(f\"   \ud83d\udca1 Consider starting with 2K samples to test, then scale up\")\n",
    "        print(f\"   \ud83d\udcb0 Budget ~$15-20 for 2K samples, $50+ for 10K samples\")\n",
    "\n",
    "else:\n",
    "    print(\"\u274c No CUDA GPU detected! This notebook requires GPU for training.\")\n",
    "    raise RuntimeError(\"GPU required for QLoRA training\")\n",
    "\n",
    "print(f\"\\n\u2705 Configuration set for {GPU_TYPE} with REALISTIC time estimates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178e022",
   "metadata": {
    "id": "6178e022"
   },
   "source": [
    "###  Service Authentication\n",
    "\n",
    "Setting up environment variables for Weights & Biases (W&B) and Hugging Face for experiment tracking and model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b5611a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1760019527092,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "80b5611a",
    "outputId": "53c2ddfa-b6f6-49d8-cbf1-86f45a8d815e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Environment variables set for W&B and Hugging Face\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set W&B environment variables\n",
    "# Replace with your actual W&B API Key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_KEY_HERE\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"jeffgong11235\"  # Replace with your W&B entity\n",
    "os.environ[\"WANDB_PROJECT\"] = \"hotpotqa-qlora\"\n",
    "os.environ[\"WANDB_RUN_GROUP\"] = \"deep-learning-rag\"\n",
    "\n",
    "# Set Hugging Face environment variables\n",
    "# Replace with your actual Hugging Face Token (if needed for private models)\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_your_token_here\"\n",
    "\n",
    "print(\"\u2705 Environment variables set for W&B and Hugging Face\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff0d492",
   "metadata": {
    "id": "cff0d492"
   },
   "source": [
    "###  Initialize Weights & Biases\n",
    "\n",
    "Logging into Weights & Biases and initializing a new run for tracking the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8645e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 3700,
     "status": "ok",
     "timestamp": 1760019530835,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "1c8645e9",
    "outputId": "135453a9-b480-4008-bf2f-0138e159ae67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd27 W&B Configuration:\n",
      "   Entity: jeffgong11235\n",
      "   Project: hotpotqa-qlora\n",
      "   Run Name: mistral-7b-qlora-rtx_a5000-1760019526\n",
      "   Group: deep-learning-rag\n",
      "\n",
      "\ud83d\udd10 Logging into Weights & Biases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjeffgong11235\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251009_141848-ruvp840x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jeffgong11235/hotpotqa-qlora/runs/ruvp840x' target=\"_blank\">mistral-7b-qlora-rtx_a5000-1760019526</a></strong> to <a href='https://wandb.ai/jeffgong11235/hotpotqa-qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jeffgong11235/hotpotqa-qlora' target=\"_blank\">https://wandb.ai/jeffgong11235/hotpotqa-qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jeffgong11235/hotpotqa-qlora/runs/ruvp840x' target=\"_blank\">https://wandb.ai/jeffgong11235/hotpotqa-qlora/runs/ruvp840x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 W&B initialized! Run URL: https://wandb.ai/jeffgong11235/hotpotqa-qlora/runs/ruvp840x\n"
     ]
    }
   ],
   "source": [
    "# W&B Configuration\n",
    "if 'GPU_TYPE' not in globals():\n",
    "  GPU_TYPE = 'CPU'\n",
    "if 'MAX_SEQ_LENGTH' not in globals():\n",
    "  MAX_SEQ_LENGTH = 1024\n",
    "if 'BATCH_SIZE' not in globals():\n",
    "  BATCH_SIZE = 1\n",
    "if 'GRAD_ACCUM_STEPS' not in globals():\n",
    "  GRAD_ACCUM_STEPS = 8\n",
    "WANDB_ENTITY = \"jeffgong11235\"  # Replace with your W&B entity\n",
    "WANDB_PROJECT = \"hotpotqa-qlora\"\n",
    "RUN_NAME = f\"mistral-7b-qlora-{GPU_TYPE.lower()}-{int(time.time())}\"\n",
    "GROUP = \"deep-learning-rag\"\n",
    "\n",
    "print(f\"\ud83d\udd27 W&B Configuration:\")\n",
    "print(f\"   Entity: {WANDB_ENTITY}\")\n",
    "print(f\"   Project: {WANDB_PROJECT}\")\n",
    "print(f\"   Run Name: {RUN_NAME}\")\n",
    "print(f\"   Group: {GROUP}\")\n",
    "\n",
    "# Login to W&B\n",
    "print(\"\\n\ud83d\udd10 Logging into Weights & Biases...\")\n",
    "wandb.login(key = \"Your key here\")  # Replace with your actual W&B API key\n",
    "\n",
    "# Initialize W&B run\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    name=RUN_NAME,\n",
    "    group=GROUP,\n",
    "    config={\n",
    "        \"base_model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"gpu_type\": GPU_TYPE,\n",
    "        \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"grad_accum_steps\": GRAD_ACCUM_STEPS,\n",
    "        \"lora_rank\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"epochs\": 2,\n",
    "        \"quantization\": \"4bit-nf4\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\u2705 W&B initialized! Run URL: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rl7aX8POyUAQ",
   "metadata": {
    "id": "Rl7aX8POyUAQ"
   },
   "source": [
    "###  Hugging face Authentication\n",
    "\n",
    "Logging into Hugging face for getting permission to use Hugging face models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161V-ZJNysUA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1760019530989,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "161V-ZJNysUA",
    "outputId": "ed205b45-ab4c-4fb1-8495-2ae377596d3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Log in to Hugging Face\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# It's recommended to store your HF token securely in Colab Secrets\n",
    "# and access it using userdata.get('HF_TOKEN')\n",
    "# For this example, we'll use the environment variable set in the previous cell.\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "if hf_token:\n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"\u2705 Successfully logged in to Hugging Face!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to log in to Hugging Face: {e}\")\n",
    "        print(\"   Please ensure your HF_TOKEN environment variable is set correctly.\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f HF_TOKEN environment variable not found. Skipping Hugging Face login.\")\n",
    "    print(\"   Some models may require authentication. Please set HF_TOKEN in environment variables or Colab Secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31957ad0",
   "metadata": {
    "id": "31957ad0"
   },
   "source": [
    "### Load and Investigate Dataset\n",
    "\n",
    "Loading the HotpotQA dataset and performing an initial investigation of its structure to understand how to process it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58f34f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b2e6911217ef4bbb8e164a1519f69d91",
      "f841fc538e1e477989c48eabc9f30246",
      "6b4da0ecb6eb45d693aa4236cde922cc",
      "698ff698b96f485fa51f305ca9d45218",
      "4764aef8050f4ff4872d7289e525097c",
      "f0444a5dc306443393ba2d383e02eb86",
      "485a6625f59149909c1288f50a6a796a",
      "ee6c1750eb744d5899aef6b15386bb36",
      "deee54bc243a407b8f508000c9086b98",
      "187a6a6288ca4bdba264a1d2ed89874c",
      "3fba0d9d196c45e297027f6e6085b763",
      "2909f357e4e0498b96e735071a2b72fd",
      "4171e5f724f8465a838211dc999bc5cf",
      "ec3daf2847104603b36d677f5e791e1f",
      "044ece2fc5894057921c5c3d931d31f3",
      "5aac047c359641ab92baaf95a99ceb4d",
      "b4596ce9ec834648896d56e39e60821a",
      "04e93e20c6f649f78a7dbcf66a2fb00b",
      "95af57ee1e804f85bfde6c350a193750",
      "cbcafa96eb9c4d119f3d71b01538ca00",
      "27e593352e8d4cf8979ddaa25566fb03",
      "ec976b5c1f5448e882998360593be166",
      "72b9562cb00341c08d71a4f9b813e0d8",
      "cd5077a206574c8a8fcbfbe88588358c",
      "67773175857d4d3abe5e5f6ee172fe2c",
      "962d2223b9f54554bf97f424156515c0",
      "96cf6240ea83430cb142b4cb20aee8a0",
      "39c2b0015fdd434687d68e211419ab8b",
      "59b21156ec7846d089f84b03d5740b4d",
      "d544667e1a4349a3b5d101c91546b8dc",
      "8fa6b91dc7dd44cd81a7108634853fb7",
      "17c033f53f2b482094180210cc621303",
      "b89ceba61b1d4f1e9202f93c88d91ca7",
      "4018b3b5fc444356a4954b1e4a4f5b08",
      "85ac65e52717443ba04947a77a61f131",
      "c037a04f65c9439d80f8b7d8654ac95c",
      "f1c4402da7994ca1a04f2783b7240890",
      "7c287fef7c73452abacae570c16684c6",
      "cee2a3249cc6401ebab75a8a2f191578",
      "a66ed9b6136f4ccc92605415103fa9c0",
      "d672863a0d494cbd92ad3ddcb161eb61",
      "8fc4f404697743d6bd9bb17a2d59417c",
      "634eda616a654f489fd58a6a3296aa70",
      "2215bbc48a8845878e336f0e93a7c18b",
      "1ec2b9929b4548d99aa981ac5ca67a5d",
      "8a9a6ee0c1b443d79c8bc2734b7b731e",
      "7ac61abe432f4bea90d106a6e4624873",
      "b84fee492fe041549927c1c9ceda470c",
      "fdf6db8980a94224a87f646f1222ff86",
      "56ac002fe0c24f8b81960a2a03bbb794",
      "2828185fc1fe41418db2b23adfc46cb4",
      "6b32657d3d3b4f589b7cbcbc22e3e905",
      "76261fcd41784089b133b81910fc29c2",
      "34748267fb164a64a74cdaf23510fd2b",
      "19d9aaa0d4f74837994d4ad10935b74d",
      "38408e50ddea46dca419ff77cee9dc32",
      "52d7d4ec01664f4b9e2f10ada5265376",
      "a37129fe3edd4682ba7baaf60d28d26a",
      "eb0f5bcd20df4e158c66c63de50a96e5",
      "2332f6013fa54ef0957511b0b8d011aa",
      "0d8b59acef194d93b73eb1ba54d42676",
      "f8b6638f7efe4ba6a8b399f989a79685",
      "815af36056bf412cb4ef1d64e0553a2f",
      "efa8482fb9f44bac804feda7b9b8dcb6",
      "fa63898fabf64f6884e53bf11fd55bfa",
      "14eef61174bf46d5a68f0d661b2c823b"
     ]
    },
    "executionInfo": {
     "elapsed": 9754,
     "status": "ok",
     "timestamp": 1760019540792,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "bb58f34f",
    "outputId": "1bcd66e2-60b8-4d69-d967-555fbbcf72a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d HOTPOTQA DATASET STRUCTURE INVESTIGATION\n",
      "============================================================\n",
      "Loading HotpotQA dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e6911217ef4bbb8e164a1519f69d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2909f357e4e0498b96e735071a2b72fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "distractor/train-00000-of-00002.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b9562cb00341c08d71a4f9b813e0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "distractor/train-00001-of-00002.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4018b3b5fc444356a4954b1e4a4f5b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "distractor/validation-00000-of-00001.par(\u2026):   0%|          | 0.00/27.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec2b9929b4548d99aa981ac5ca67a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38408e50ddea46dca419ff77cee9dc32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Dataset loaded: 90447 training examples\n",
      "\u2705 Dataset loaded: 7405 validation examples\n",
      "\n",
      "\ud83d\udccb COMPLETE SAMPLE STRUCTURE:\n",
      "============================================================\n",
      "\n",
      "\ud83d\udd0d FIELD: id\n",
      "   Type: str\n",
      "   Length: 24\n",
      "   Value: '5a7a06935542990198eaf050'\n",
      "\n",
      "\ud83d\udd0d FIELD: question\n",
      "   Type: str\n",
      "   Length: 70\n",
      "   Value: \"Which magazine was started first Arthur's Magazine or First for Women?\"\n",
      "\n",
      "\ud83d\udd0d FIELD: answer\n",
      "   Type: str\n",
      "   Length: 17\n",
      "   Value: \"Arthur's Magazine\"\n",
      "\n",
      "\ud83d\udd0d FIELD: type\n",
      "   Type: str\n",
      "   Length: 10\n",
      "   Value: 'comparison'\n",
      "\n",
      "\ud83d\udd0d FIELD: level\n",
      "   Type: str\n",
      "   Length: 6\n",
      "   Value: 'medium'\n",
      "\n",
      "\ud83d\udd0d FIELD: supporting_facts\n",
      "   Type: dict\n",
      "   Length: 2\n",
      "   Raw value type: <class 'dict'>\n",
      "   Dict keys: ['title', 'sent_id']\n",
      "   Key 'title': list, Length: 2\n",
      "     First few items: [\"Arthur's Magazine\", 'First for Women']\n",
      "   Key 'sent_id': list, Length: 2\n",
      "     First few items: [0, 0]\n",
      "\n",
      "\ud83d\udd0d FIELD: context\n",
      "   Type: dict\n",
      "   Length: 2\n",
      "   Raw value type: <class 'dict'>\n",
      "   Is dict: True\n",
      "   Dict keys: ['title', 'sentences']\n",
      "   Key 'title': list, Length: 10\n",
      "     First item: str - 'Radio City (Indian radio station)'\n",
      "   Key 'sentences': list, Length: 10\n",
      "     First item: list - [\"Radio City is India's first private FM radio station and was started on 3 July 2001.\", ' It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).', ' It plays Hindi, English and regional songs.', ' It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.', ' Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.', ' The Radio station currently plays a mix of Hindi and Regional music.', ' Abraham Thomas is the CEO of the company.']\n",
      "\n",
      "\ud83e\uddea PRACTICAL ACCESS TESTS:\n",
      "============================================================\n",
      "Testing context processing:\n",
      "  Context type: <class 'dict'>\n",
      "  Context keys: ['title', 'sentences']\n",
      "  Titles: <class 'list'>, Length: 10\n",
      "  Sentences: <class 'list'>, Length: 10\n",
      "  First title: Radio City (Indian radio station)\n",
      "  First sentences: [\"Radio City is India's first private FM radio station and was started on 3 July 2001.\", ' It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).', ' It plays Hindi, English and regional songs.', ' It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.', ' Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.', ' The Radio station currently plays a mix of Hindi and Regional music.', ' Abraham Thomas is the CEO of the company.']\n",
      "\n",
      "Testing supporting_facts processing:\n",
      "  Supporting facts type: <class 'dict'>\n",
      "  Supporting facts keys: ['title', 'sent_id']\n",
      "  Titles: [\"Arthur's Magazine\", 'First for Women']\n",
      "  Sentence IDs: [0, 0]\n",
      "\n",
      "\ud83d\udcca DATASET SIZE CONFIGURATION:\n",
      "==================================================\n",
      "\ud83c\udfaf RTX A5000 optimization: Using 2000 train, 400 val samples\n",
      "\n",
      "\ud83d\udcb0 COST ANALYSIS:\n",
      "   Training samples: 2,000 (2.2% of full dataset)\n",
      "   Steps per epoch: 250\n",
      "   Total steps: 500\n",
      "   Estimated time: 5.0 hours\n",
      "   Estimated cost: $2.50\n",
      "   \ud83d\udca1 Using subset for cost optimization\n",
      "\u2705 Working with: 2000 train, 400 validation\n",
      "\n",
      "\ud83d\udd27 STRUCTURE ANALYSIS COMPLETE!\n",
      "\ud83d\udccb Key findings:\n",
      "   - Context is a dict with 'title' and 'sentences' keys\n",
      "   - Supporting facts is a dict with 'title' and 'sent_id' keys\n",
      "   - Processing function needs to handle dict structure, not list structure\n"
     ]
    }
   ],
   "source": [
    "# Complete HotpotQA Structure Investigation\n",
    "print(\"\ud83d\udd0d HOTPOTQA DATASET STRUCTURE INVESTIGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'HOURLY_RATE' not in globals():\n",
    "  HOURLY_RATE = 0.50\n",
    "# Load dataset\n",
    "print(\"Loading HotpotQA dataset...\")\n",
    "dataset = load_dataset('hotpotqa/hotpot_qa', 'distractor')\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "print(f\"\u2705 Dataset loaded: {len(train_data)} training examples\")\n",
    "print(f\"\u2705 Dataset loaded: {len(validation_data)} validation examples\")\n",
    "\n",
    "# Get first example for detailed analysis\n",
    "sample = train_data[0]\n",
    "\n",
    "print(f\"\\n\ud83d\udccb COMPLETE SAMPLE STRUCTURE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze each field systematically\n",
    "for key, value in sample.items():\n",
    "    print(f\"\\n\ud83d\udd0d FIELD: {key}\")\n",
    "    print(f\"   Type: {type(value).__name__}\")\n",
    "\n",
    "    if hasattr(value, '__len__'):\n",
    "        try:\n",
    "            print(f\"   Length: {len(value)}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Special detailed handling for complex fields\n",
    "    if key == 'context':\n",
    "        print(f\"   Raw value type: {type(value)}\")\n",
    "        print(f\"   Is dict: {isinstance(value, dict)}\")\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"   Dict keys: {list(value.keys())}\")\n",
    "            for dict_key, dict_value in value.items():\n",
    "                print(f\"   Key '{dict_key}': {type(dict_value).__name__}, Length: {len(dict_value) if hasattr(dict_value, '__len__') else 'N/A'}\")\n",
    "                if hasattr(dict_value, '__len__') and len(dict_value) > 0:\n",
    "                    print(f\"     First item: {type(dict_value[0]).__name__} - {repr(dict_value[0])}\")\n",
    "\n",
    "    elif key == 'supporting_facts':\n",
    "        print(f\"   Raw value type: {type(value)}\")\n",
    "\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"   Dict keys: {list(value.keys())}\")\n",
    "            for dict_key, dict_value in value.items():\n",
    "                print(f\"   Key '{dict_key}': {type(dict_value).__name__}, Length: {len(dict_value) if hasattr(dict_value, '__len__') else 'N/A'}\")\n",
    "                if hasattr(dict_value, '__len__') and len(dict_value) > 0:\n",
    "                    print(f\"     First few items: {dict_value[:3]}\")\n",
    "\n",
    "    else:\n",
    "        # For simple fields\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            print(f\"   Value: {repr(value[:100])}...\")\n",
    "        else:\n",
    "            print(f\"   Value: {repr(value)}\")\n",
    "\n",
    "print(f\"\\n\ud83e\uddea PRACTICAL ACCESS TESTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test actual processing patterns\n",
    "context = sample['context']\n",
    "supporting_facts = sample['supporting_facts']\n",
    "\n",
    "print(f\"Testing context processing:\")\n",
    "print(f\"  Context type: {type(context)}\")\n",
    "if isinstance(context, dict):\n",
    "    print(f\"  Context keys: {list(context.keys())}\")\n",
    "    if 'title' in context and 'sentences' in context:\n",
    "        titles = context['title']\n",
    "        sentences = context['sentences']\n",
    "        print(f\"  Titles: {type(titles)}, Length: {len(titles)}\")\n",
    "        print(f\"  Sentences: {type(sentences)}, Length: {len(sentences)}\")\n",
    "        print(f\"  First title: {titles[0] if len(titles) > 0 else 'None'}\")\n",
    "        print(f\"  First sentences: {sentences[0] if len(sentences) > 0 else 'None'}\")\n",
    "\n",
    "print(f\"\\nTesting supporting_facts processing:\")\n",
    "print(f\"  Supporting facts type: {type(supporting_facts)}\")\n",
    "if isinstance(supporting_facts, dict):\n",
    "    print(f\"  Supporting facts keys: {list(supporting_facts.keys())}\")\n",
    "    if 'title' in supporting_facts and 'sent_id' in supporting_facts:\n",
    "        titles = supporting_facts['title']\n",
    "        sent_ids = supporting_facts['sent_id']\n",
    "        print(f\"  Titles: {titles}\")\n",
    "        print(f\"  Sentence IDs: {sent_ids}\")\n",
    "\n",
    "# Dataset size configuration - FIXED SPEED_FACTOR issue\n",
    "print(f\"\\n\ud83d\udcca DATASET SIZE CONFIGURATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# GPU-optimized subset for training\n",
    "if 'GPU_TYPE' in globals():\n",
    "    # Define SPEED_FACTOR based on GPU type\n",
    "    if GPU_TYPE == \"RTX_A5000\":\n",
    "        SPEED_FACTOR = 1.0\n",
    "        TRAIN_SIZE = 2000   # Cost: ~$2.00, Time: 4 hours\n",
    "        VAL_SIZE = 400\n",
    "        print(f\"\ud83c\udfaf RTX A5000 optimization: Using {TRAIN_SIZE} train, {VAL_SIZE} val samples\")\n",
    "\n",
    "    elif GPU_TYPE == \"RTX_4090\":\n",
    "        SPEED_FACTOR = 0.8\n",
    "        TRAIN_SIZE = 2000\n",
    "        VAL_SIZE = 400\n",
    "        print(f\"\ud83c\udfaf RTX 4090 optimization: Using {TRAIN_SIZE} train, {VAL_SIZE} val samples\")\n",
    "    else:\n",
    "        SPEED_FACTOR = 0.5\n",
    "        TRAIN_SIZE = 1000\n",
    "        VAL_SIZE = 200\n",
    "        print(f\"\ud83c\udfaf Conservative: Using {TRAIN_SIZE} train, {VAL_SIZE} val samples\")\n",
    "\n",
    "    # Cost analysis - FIXED with SPEED_FACTOR defined\n",
    "    steps_per_epoch = TRAIN_SIZE // (BATCH_SIZE * GRAD_ACCUM_STEPS)\n",
    "    total_steps = steps_per_epoch * 2  # 2 epochs\n",
    "    training_hours = total_steps / (100 * SPEED_FACTOR)  # 100 steps/hour baseline with speed factor\n",
    "    total_cost = training_hours * HOURLY_RATE\n",
    "\n",
    "    print(f\"\\n\ud83d\udcb0 COST ANALYSIS:\")\n",
    "    print(f\"   Training samples: {TRAIN_SIZE:,} ({TRAIN_SIZE/len(train_data)*100:.1f}% of full dataset)\")\n",
    "    print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"   Total steps: {total_steps}\")\n",
    "    print(f\"   Estimated time: {training_hours:.1f} hours\")\n",
    "    print(f\"   Estimated cost: ${total_cost:.2f}\")\n",
    "\n",
    "    if TRAIN_SIZE < 5000:\n",
    "        print(f\"   \ud83d\udca1 Using subset for cost optimization\")\n",
    "    elif TRAIN_SIZE < len(train_data):\n",
    "        print(f\"   \u2696\ufe0f Using partial dataset for balance of cost vs quality\")\n",
    "    else:\n",
    "        print(f\"   \ud83c\udfc6 Using full dataset for maximum quality\")\n",
    "\n",
    "    train_sample = train_data.shuffle(seed=42).select(range(min(TRAIN_SIZE, len(train_data))))\n",
    "    val_sample = validation_data.shuffle(seed=42).select(range(min(VAL_SIZE, len(validation_data))))\n",
    "    print(f\"\u2705 Working with: {len(train_sample)} train, {len(val_sample)} validation\")\n",
    "else:\n",
    "    # Fallback if GPU_TYPE not defined - FIXED with SPEED_FACTOR\n",
    "    SPEED_FACTOR = 0.5\n",
    "    TRAIN_SIZE = 2000\n",
    "    VAL_SIZE = 400\n",
    "    train_sample = train_data.shuffle(seed=42).select(range(TRAIN_SIZE))\n",
    "    val_sample = validation_data.shuffle(seed=42).select(range(VAL_SIZE))\n",
    "    print(f\"\u2705 Working with: {len(train_sample)} train, {len(val_sample)} validation\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 STRUCTURE ANALYSIS COMPLETE!\")\n",
    "print(f\"\ud83d\udccb Key findings:\")\n",
    "print(f\"   - Context is a dict with 'title' and 'sentences' keys\")\n",
    "print(f\"   - Supporting facts is a dict with 'title' and 'sent_id' keys\")\n",
    "print(f\"   - Processing function needs to handle dict structure, not list structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vb_j7VHmz0d_",
   "metadata": {
    "id": "vb_j7VHmz0d_"
   },
   "source": [
    "# Data processing, COT prompt preparation, code implementation for RAG on prompt-generation, evaluation,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xn5rJgIr1n_N",
   "metadata": {
    "id": "Xn5rJgIr1n_N"
   },
   "source": [
    "### Print 1 data points from train_sample and use the for creating chain of thought prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feVMLQFcjt5F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1760019540822,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "feVMLQFcjt5F",
    "outputId": "592bb7c0-e583-4045-99cd-80880b84fdca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 data points from train_sample:\n",
      "============================================================\n",
      "\n",
      "--- Example 1 ---\n",
      "  id: 5ae3cfe05542990afbd1e1e3\n",
      "  question: Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?\n",
      "  answer: Knox County Regional Airport\n",
      "  type: comparison\n",
      "  level: medium\n",
      "  supporting_facts: {'title': ['Sacramento International Airport', 'Knox County Regional Airport'], 'sent_id': [0, 0]}\n",
      "  context: {'title': ['Vinalhaven, Maine', 'Owls Head, Maine', 'North Haven, Maine', 'Downeast Flight 46', 'Northern California TRACON', 'Sacramento International Airport', 'Knox County Regional Airport', 'Matinicus Isle, Maine', 'Raleigh Executive Jetport', 'Lea County Regional Airport'], 'sentences': [['Vinalhaven is a town located on the larger of the two Fox Islands in Knox County, Maine, United States.', ' Vinalhaven is also used to refer to the Island itself.', ' The population was 1,165 at the 2010 census.', ' It is home to a thriving lobster fishery and hosts a summer colony.', ' Since there is no bridge to the island, Vinalhaven is accessible from Rockland via an approximately hour-and-fifteen-minute ferry ride across West Penobscot Bay, or by air taxi from Knox County Regional Airport.'], ['Owls Head is a town in Knox County, Maine, United States.', ' The population was 1,580 at the 2010 census.', ' A resort and fishing area, the community is home to the Knox County Regional Airport.', ' It includes the village of Ash Point.'], ['North Haven is a town in Knox County, Maine, United States, in Penobscot Bay.', ' The town is both a year-round island community and a prominent summer colony.', ' The population was 355 at the 2010 census.', ' North Haven is accessed by three-times daily ferry service from Rockland, or by air taxi from Knox County Regional Airport.'], [\"Downeast Airlines Flight 46 was a scheduled airline service in the United States from Boston's Logan International Airport to Rockland, Maine operated by Downeast Airlines.\", \" On May 30, 1979 a de Havilland Canada DHC-6 Twin Otter operating the flight crashed during a nonprecision approach to Rockland's Knox County Regional Airport.\", \" The cause of the accident was controlled flight into terrain (CFIT) after the failure of the flightcrew to stop the aircraft's descent below the minimum descent altitude for the nonprecision approach at Knox County airport.\", \" The investigation into the accident looked into the airline's corporate culture as a contributing factor to the crash; this was the first time an investigation took this approach to an air crash.\"], ['Northern California TRACON (NCT) (Terminal Radar Approach Control), or NorCal TRACON for short, is an air traffic control facility that provides safety alerts, separation, and sequencing of air traffic arriving, departing, and transiting the airspace and airports in Northern California.', ' Located in Rancho Cordova near Sacramento, NCT controls airspace over 19000 square miles, and serves Reno International Airport, Sacramento International Airport, San Jose International Airport, Oakland International Airport, and San Francisco International Airport, plus 19 other smaller airports with air traffic control towers.', ' NCT is the 3rd busiest TRACON in America.', \" NorCal TRACON is the step between local control (in an airport's control tower) and Air Route Traffic Control Center (ARTCC), in this case, Oakland Center (ICAO code: ZOA).\", ' San Francisco International Airport is the 2nd largest airport in California and the largest airport serving Northern California.'], ['Sacramento International Airport (IATA: SMF, ICAO: KSMF, FAA LID: SMF) is 10 mi northwest of downtown Sacramento, in Sacramento County, California.', ' It is run by the Sacramento County Airport System.', ' Southwest Airlines carries about half the airline passengers.'], ['Knox County Regional Airport (IATA: RKD, ICAO: KRKD, FAA LID: RKD) is a county owned, public use airport in Knox County, Maine, United States.', ' It is located three nautical miles (6 km) south of the central business district of Rockland, Maine.', ' The airport serves the residents of midcoast Maine with commercial and charter aviation services.', ' Scheduled airline service is subsidized by the Essential Air Service program.', \" It is also a major hub of freight and mail service to Maine's island communities including Matinicus, North Haven and Vinalhaven.\"], ['Matinicus Isle is an island plantation in Knox County, Maine, United States.', ' The island is located within Penobscot Bay about 20 miles east of the mainland coast and is accessible by ferry from Rockland or by air taxi from Knox County Regional Airport.', ' The plantation is both a year-round island community and a summer colony.', ' The population was 74 at the 2010 census.'], ['Raleigh Exec: The Raleigh Executive Jetport @ Sanford-Lee County or Raleigh Exec Jetport at Sanford-Lee CountyFAA Airport Master Record for TTA (Form 5010 ) (ICAO: KTTA,\\xa0FAA LID: TTA) is a public use airport located seven\\xa0nautical miles (8\\xa0mi, 13\\xa0km) northeast of the central business district of Sanford, a city in Lee County, North Carolina, United States.', ' It is owned by the Sanford-Lee County Regional Airport Authority and was previously known as Sanford-Lee County Regional Airport.', ' This airport is included in the National Plan of Integrated Airport Systems for 2011\u20132015, which categorized it as a \"reliever airport\" for Raleigh-Durham International Airport.'], ['Lea County Regional Airport (IATA: HOB,\\xa0ICAO: KHOB) (Lea County-Hobbs Airport) is four miles (6.4\\xa0km) west of Hobbs, in Lea County, New Mexico.', ' The airport covers 898 acre and has three runways.', \" It is an FAA certified commercial airport served by United Airlines' affiliate with daily regional flights.\", ' Lea County Regional Airport is the largest of the three airports owned and operated by Lea County Government.', ' Lea County also owns and operated two general aviation airports in Lovington and Jal, New Mexico.']]}\n",
      "    {'title': [\"Arthur's Magazine\", 'First for Women'], 'sent_id': [0, 0]}\n",
      "\n",
      "============================================================\n",
      "Finished displaying data points.\n"
     ]
    }
   ],
   "source": [
    "# Print 8 data points from train_sample and use the for creating chain of thought prompt\n",
    "print(\"Displaying 1 data points from train_sample:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure train_sample is available\n",
    "if 'train_sample' in globals():\n",
    "    num_examples_to_print = min(1, len(train_sample)) # Print up to 8 examples or fewer if dataset is smaller\n",
    "\n",
    "    for i in range(num_examples_to_print):\n",
    "        example = train_sample[i]\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        for key, value in example.items():\n",
    "            # Print only the content of each field\n",
    "            if isinstance(value, str):\n",
    "                print(f\"  {key}: {value}\")\n",
    "            elif isinstance(value, (list, dict)):\n",
    "                print(f\"  {key}: {repr(value)}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "\n",
    "\n",
    "        if isinstance(supporting_facts, dict):\n",
    "            print(f\"    {repr(supporting_facts)}\")\n",
    "        else:\n",
    "             print(f\"    Type: {type(supporting_facts).__name__} - {repr(supporting_facts)}\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Finished displaying data points.\")\n",
    "\n",
    "else:\n",
    "    print(\"train_sample not found. Please run the data processing cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ZF2Uha21ry-",
   "metadata": {
    "id": "_ZF2Uha21ry-"
   },
   "source": [
    "### Create the chain of thought prompt using data points printed from previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7agAtJS2Dyxk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1760019540872,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "7agAtJS2Dyxk",
    "outputId": "830da5fe-661f-43d3-ca25-54a873175396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Chain-of-Thought prompt instances...\n",
      "\n",
      "--- Processing Example 2 ---\n",
      "  Question: Peter Hobbs founded the company that is based in what town in Manchester?...\n",
      "\n",
      "  --- CoT Instance (JSON) ---\n",
      "{\n",
      "  \"instruction\": \"You are an evidence-grounded QA assistant. Choose the \\\"Supporting Facts\\\" from the \\\"Contexts\\\" given to you and filter out the irrelevant information from the Contexts. Using only the \\u201cSupporting Facts,\\u201d answer the question. Provide: answer \\u2014 the short final answer, reasoning \\u2014 a step-by-step explanation showing how you used the facts, and evidence \\u2014 a list of citations from the contexts you chose as \\\"Supporting Facts\\\".\\n    For instance if you choose the first and third sentence as citation from the context, evidence should be [1], [3]. If the facts are insufficient, set answer to \\u201cinsufficient information\\u201d.\\n     Please ensure that your answer follows this JSON format \\\"output\\\": {\\n    \\\"answer\\\": \\\"Failsworth\\\",\\n    \\\"reasoning\\\": [\\n      \\\"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\\\",\\n      \\\"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\\\",\\n      \\\"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\\\",\\n      \\\"Therefore, the company Peter Hobbs founded is based in Failsworth\\\"\\n    ],\\n    \\\"evidence\\\": [\\n      7,\\n      8\\n    ]\\n  }\",\n",
      "  \"input\": {\n",
      "    \"Question\": \"Peter Hobbs founded the company that is based in what town in Manchester?\",\n",
      "    \"Contexts\": [\n",
      "      \"[1] Title: Cains Brewery - Cains is a brewery in Liverpool, England, founded in 1858 by Robert Cain.\",\n",
      "      \"[2] Title: Cains Brewery -  The company merged with Peter Walker & Son in 1921 to form Walker Cains.\",\n",
      "      \"[3] Title: Cains Brewery -  Peter Walker & Son had a large brewery in Warrington so sold its Liverpool brewery to Higsons in 1923.\",\n",
      "      \"[4] Title: Cains Brewery -  Boddingtons of Manchester took over in 1985.\",\n",
      "      \"[5] Title: Cains Brewery -  In 1990 Whitbread acquired Boddington's brewing operations and closed the then Higsons Brewery in 1990.\",\n",
      "      \"[6] Title: Cains Brewery -  It was reopened by GB Breweries, who became part of Bryggerigruppen in 1991, and in 2002 was sold to Gardener-Shaw for \\u00a33.4 million.\",\n",
      "      \"[7] Title: Peter Hobbs (engineer) - Peter Wallace Hobbs (1916\\u20132008) was an English engineer, and businessman, who with Bill Russell formed the well-known electrical appliance company Russell Hobbs.\",\n",
      "      \"[8] Title: Russell Hobbs - Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England, United Kingdom.\",\n",
      "      \"[9] Title: Curzon Ashton F.C. - Curzon Ashton Football Club is a semi-professional football club based in the market town of Ashton-under-Lyne, Greater Manchester, England, that competes in the National League North, the sixth-highest division overall in the English football league system, and are members of the Manchester County Football Association.\",\n",
      "      \"[10] Title: Curzon Ashton F.C. -  Nicknamed \\\"the Nash\\\", the club was founded in 1963 and moved to its current stadium, Tameside Stadium, in 2005.\",\n",
      "      \"[11] Title: Manchester Liners - Manchester Liners was a cargo and passenger shipping company founded in 1898, based in Manchester, England.\",\n",
      "      \"[12] Title: Manchester Liners -  The line pioneered the regular passage of ocean-going vessels along the Manchester Ship Canal.\",\n",
      "      \"[13] Title: Manchester Liners -  Its main sphere of operation was the transatlantic shipping trade, but the company also operated services to the Mediterranean.\",\n",
      "      \"[14] Title: Manchester Liners -  All of the line's vessels were registered in the Port of Manchester, and many were lost to enemy action during the First and Second World Wars.\",\n",
      "      \"[15] Title: Dobson &amp; Barlow - Dobson and Barlow were manufacturers of textile machinery with works in Bolton, Greater Manchester.\",\n",
      "      \"[16] Title: Dobson &amp; Barlow -  Isaac Dobson (1767-1833) founded the company in 1790 and by 1850 Dobson in partnership with Peter Rothwell had premises in Blackhorse Street which produced mules for cotton spinning.\",\n",
      "      \"[17] Title: Dobson &amp; Barlow -  The company moved to a larger factory in Kay Street which had 1,600 workers in 1860.\",\n",
      "      \"[18] Title: The Flux Foundation - The Flux Foundation is a non-profit group based in the San Francisco Bay Area whose main objective is to build community through the creation of large-scale public art.\",\n",
      "      \"[19] Title: The Flux Foundation -  The group creates both public art and public artists.\",\n",
      "      \"[20] Title: The Flux Foundation -  It was founded on April 1, 2010 and was established as a California corporation on January 6, 2011, by Rebecca Anders, Jessica Hobbs, Peter (PK) Kimelman, Catherine Magee and Colinne Hemrich.\",\n",
      "      \"[21] Title: The Flux Foundation -  As of 2016 the Board of Directors consists of Kimelman, Hobbs, Magee, Paul Belger and Thwen Chaloemtiarana.\",\n",
      "      \"[22] Title: The Flux Foundation -  It is a \\\"public charity\\\" 501c(3) non-profit, supported by grants, public donations and the display of its artworks.\",\n",
      "      \"[23] Title: The Flux Foundation -  Its works are notable not only for their scale but interactivity with the audience relying on participation to create atmospheric effects.\",\n",
      "      \"[24] Title: The Flux Foundation -  The group draws upon Situationist and Fluxus ideas of creating spectacle to establish social connections as an effect of the artwork.\",\n",
      "      \"[25] Title: The Flux Foundation -  This \\\"community creation\\\" is mirrored in the pieces' creation by a large-number of volunteers who themselves create new social networks.\",\n",
      "      \"[26] Title: The Flux Foundation -  The Foundation also provides mentorship and fiscal sponsorship to other large-scale artists.\",\n",
      "      \"[27] Title: The Flux Foundation -  Flux is administratively based in San Francisco, while its studios are located at American Steel Studios in West Oakland, California.\",\n",
      "      \"[28] Title: Manchester Sport and Leisure Trust - Manchester Sport and Leisure Trust is a non-profit organisation which manages sport and leisure venues in the City of Manchester, United Kingdom.\",\n",
      "      \"[29] Title: Manchester Sport and Leisure Trust -  MSLT was founded in 1997 and is a company limited by guarantee with charitable status with a turnover of \\u00a312.5m. MSLT is based at the Sportcity site.\",\n",
      "      \"[30] Title: ITV Granada - ITV Granada (formerly Granada Television; informally Granada) is the Channel 3 regional service for North West England.\",\n",
      "      \"[31] Title: ITV Granada -  The licence for the region has been held by ITV Broadcasting Limited since November 2008.\",\n",
      "      \"[32] Title: ITV Granada -  It is the largest independent television-franchise producing company in the UK, accounting for 25% of the total broadcasting output of the ITV network.\",\n",
      "      \"[33] Title: ITV Granada -  It had been held by Granada Television, which was founded by Sidney Bernstein and based at Granada Studios on Quay Street in Manchester since its inception.\",\n",
      "      \"[34] Title: ITV Granada -  This was the only surviving company of the original four Independent Television Authority franchisees from 1954; Granada Media Group (parent company of Granada Television) merged with Carlton Communications to form ITV plc in 2004.\",\n",
      "      \"[35] Title: ITV Granada -  It covers Cheshire, Greater Manchester, Lancashire, Merseyside, northwestern Derbyshire, part of Cumbria and North Yorkshire.\",\n",
      "      \"[36] Title: ITV Granada -  On 15 July 2009, the Isle of Man was transferred to ITV Granada from ITV Border (even though the Isle of Man is a British Crown Dependency and is not part of the United Kingdom).\",\n",
      "      \"[37] Title: Peter Hesketh-Fleetwood - Sir Peter Hesketh-Fleetwood, 1st Baronet, (9 May 1801\\u00a0\\u2013 12 April 1866) was an English landowner, developer and Member of Parliament, who founded the town of Fleetwood, in Lancashire, England.\",\n",
      "      \"[38] Title: Peter Hesketh-Fleetwood -  Born Peter Hesketh, he changed his name by Royal assent to Hesketh-Fleetwood, incorporating the name of his ancestors, and was later created Baronet Fleetwood.\",\n",
      "      \"[39] Title: Peter Hesketh-Fleetwood -  Predeceased by an older brother, he inherited estates in west Lancashire in 1824.\",\n",
      "      \"[40] Title: Peter Hesketh-Fleetwood -  Inspired by the transport developments of the early 19th century, he decided to bring the railway to the Lancashire coast and develop a holiday resort and port.\",\n",
      "      \"[41] Title: Peter Hesketh-Fleetwood -  He hired architect Decimus Burton to design his new town, which he named Fleetwood; construction began in 1836.\",\n",
      "      \"[42] Title: Peter Hesketh-Fleetwood -  Hesketh-Fleetwood was instrumental in the formation of the Preston and Wyre Railway Company and with his financial support, a railway line was built between Preston and Fleetwood which opened in 1840.\"\n",
      "    ]\n",
      "  },\n",
      "  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
      "      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
      "      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
      "      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "      7,\n",
      "      8\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "  --- Supporting Facts ---\n",
      "    Title: 'Peter Hobbs (engineer)', Sentence ID: 0, Text: 'Peter Wallace Hobbs (1916\u20132008) was an English engineer, and businessman, who with Bill Russell form...'\n",
      "    Title: 'Russell Hobbs', Sentence ID: 0, Text: 'Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, Eng...'\n",
      "cot instance:  {'instruction': 'You are an evidence-grounded QA assistant. Choose the \"Supporting Facts\" from the \"Contexts\" given to you and filter out the irrelevant information from the Contexts. Using only the \u201cSupporting Facts,\u201d answer the question. Provide: answer \u2014 the short final answer, reasoning \u2014 a step-by-step explanation showing how you used the facts, and evidence \u2014 a list of citations from the contexts you chose as \"Supporting Facts\".\\n    For instance if you choose the first and third sentence as citation from the context, evidence should be [1], [3]. If the facts are insufficient, set answer to \u201cinsufficient information\u201d.\\n     Please ensure that your answer follows this JSON format \"output\": {\\n    \"answer\": \"Failsworth\",\\n    \"reasoning\": [\\n      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\\n      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\\n      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\\n      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\\n    ],\\n    \"evidence\": [\\n      7,\\n      8\\n    ]\\n  }', 'input': {'Question': 'Peter Hobbs founded the company that is based in what town in Manchester?', 'Contexts': ['[1] Title: Cains Brewery - Cains is a brewery in Liverpool, England, founded in 1858 by Robert Cain.', '[2] Title: Cains Brewery -  The company merged with Peter Walker & Son in 1921 to form Walker Cains.', '[3] Title: Cains Brewery -  Peter Walker & Son had a large brewery in Warrington so sold its Liverpool brewery to Higsons in 1923.', '[4] Title: Cains Brewery -  Boddingtons of Manchester took over in 1985.', \"[5] Title: Cains Brewery -  In 1990 Whitbread acquired Boddington's brewing operations and closed the then Higsons Brewery in 1990.\", '[6] Title: Cains Brewery -  It was reopened by GB Breweries, who became part of Bryggerigruppen in 1991, and in 2002 was sold to Gardener-Shaw for \u00a33.4 million.', '[7] Title: Peter Hobbs (engineer) - Peter Wallace Hobbs (1916\u20132008) was an English engineer, and businessman, who with Bill Russell formed the well-known electrical appliance company Russell Hobbs.', '[8] Title: Russell Hobbs - Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England, United Kingdom.', '[9] Title: Curzon Ashton F.C. - Curzon Ashton Football Club is a semi-professional football club based in the market town of Ashton-under-Lyne, Greater Manchester, England, that competes in the National League North, the sixth-highest division overall in the English football league system, and are members of the Manchester County Football Association.', '[10] Title: Curzon Ashton F.C. -  Nicknamed \"the Nash\", the club was founded in 1963 and moved to its current stadium, Tameside Stadium, in 2005.', '[11] Title: Manchester Liners - Manchester Liners was a cargo and passenger shipping company founded in 1898, based in Manchester, England.', '[12] Title: Manchester Liners -  The line pioneered the regular passage of ocean-going vessels along the Manchester Ship Canal.', '[13] Title: Manchester Liners -  Its main sphere of operation was the transatlantic shipping trade, but the company also operated services to the Mediterranean.', \"[14] Title: Manchester Liners -  All of the line's vessels were registered in the Port of Manchester, and many were lost to enemy action during the First and Second World Wars.\", '[15] Title: Dobson &amp; Barlow - Dobson and Barlow were manufacturers of textile machinery with works in Bolton, Greater Manchester.', '[16] Title: Dobson &amp; Barlow -  Isaac Dobson (1767-1833) founded the company in 1790 and by 1850 Dobson in partnership with Peter Rothwell had premises in Blackhorse Street which produced mules for cotton spinning.', '[17] Title: Dobson &amp; Barlow -  The company moved to a larger factory in Kay Street which had 1,600 workers in 1860.', '[18] Title: The Flux Foundation - The Flux Foundation is a non-profit group based in the San Francisco Bay Area whose main objective is to build community through the creation of large-scale public art.', '[19] Title: The Flux Foundation -  The group creates both public art and public artists.', '[20] Title: The Flux Foundation -  It was founded on April 1, 2010 and was established as a California corporation on January 6, 2011, by Rebecca Anders, Jessica Hobbs, Peter (PK) Kimelman, Catherine Magee and Colinne Hemrich.', '[21] Title: The Flux Foundation -  As of 2016 the Board of Directors consists of Kimelman, Hobbs, Magee, Paul Belger and Thwen Chaloemtiarana.', '[22] Title: The Flux Foundation -  It is a \"public charity\" 501c(3) non-profit, supported by grants, public donations and the display of its artworks.', '[23] Title: The Flux Foundation -  Its works are notable not only for their scale but interactivity with the audience relying on participation to create atmospheric effects.', '[24] Title: The Flux Foundation -  The group draws upon Situationist and Fluxus ideas of creating spectacle to establish social connections as an effect of the artwork.', '[25] Title: The Flux Foundation -  This \"community creation\" is mirrored in the pieces\\' creation by a large-number of volunteers who themselves create new social networks.', '[26] Title: The Flux Foundation -  The Foundation also provides mentorship and fiscal sponsorship to other large-scale artists.', '[27] Title: The Flux Foundation -  Flux is administratively based in San Francisco, while its studios are located at American Steel Studios in West Oakland, California.', '[28] Title: Manchester Sport and Leisure Trust - Manchester Sport and Leisure Trust is a non-profit organisation which manages sport and leisure venues in the City of Manchester, United Kingdom.', '[29] Title: Manchester Sport and Leisure Trust -  MSLT was founded in 1997 and is a company limited by guarantee with charitable status with a turnover of \u00a312.5m. MSLT is based at the Sportcity site.', '[30] Title: ITV Granada - ITV Granada (formerly Granada Television; informally Granada) is the Channel 3 regional service for North West England.', '[31] Title: ITV Granada -  The licence for the region has been held by ITV Broadcasting Limited since November 2008.', '[32] Title: ITV Granada -  It is the largest independent television-franchise producing company in the UK, accounting for 25% of the total broadcasting output of the ITV network.', '[33] Title: ITV Granada -  It had been held by Granada Television, which was founded by Sidney Bernstein and based at Granada Studios on Quay Street in Manchester since its inception.', '[34] Title: ITV Granada -  This was the only surviving company of the original four Independent Television Authority franchisees from 1954; Granada Media Group (parent company of Granada Television) merged with Carlton Communications to form ITV plc in 2004.', '[35] Title: ITV Granada -  It covers Cheshire, Greater Manchester, Lancashire, Merseyside, northwestern Derbyshire, part of Cumbria and North Yorkshire.', '[36] Title: ITV Granada -  On 15 July 2009, the Isle of Man was transferred to ITV Granada from ITV Border (even though the Isle of Man is a British Crown Dependency and is not part of the United Kingdom).', '[37] Title: Peter Hesketh-Fleetwood - Sir Peter Hesketh-Fleetwood, 1st Baronet, (9 May 1801\\xa0\u2013 12 April 1866) was an English landowner, developer and Member of Parliament, who founded the town of Fleetwood, in Lancashire, England.', '[38] Title: Peter Hesketh-Fleetwood -  Born Peter Hesketh, he changed his name by Royal assent to Hesketh-Fleetwood, incorporating the name of his ancestors, and was later created Baronet Fleetwood.', '[39] Title: Peter Hesketh-Fleetwood -  Predeceased by an older brother, he inherited estates in west Lancashire in 1824.', '[40] Title: Peter Hesketh-Fleetwood -  Inspired by the transport developments of the early 19th century, he decided to bring the railway to the Lancashire coast and develop a holiday resort and port.', '[41] Title: Peter Hesketh-Fleetwood -  He hired architect Decimus Burton to design his new town, which he named Fleetwood; construction began in 1836.', '[42] Title: Peter Hesketh-Fleetwood -  Hesketh-Fleetwood was instrumental in the formation of the Preston and Wyre Railway Company and with his financial support, a railway line was built between Preston and Fleetwood which opened in 1840.']}, 'output': {'answer': 'Failsworth', 'reasoning': ['From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell', 'From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England', 'Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth', 'Therefore, the company Peter Hobbs founded is based in Failsworth'], 'evidence': [7, 8]}}\n",
      "\n",
      "\u2705 Generated 1 Chain-of-Thought prompt instances.\n",
      "\n",
      " Here is the chain of thought exemplars\n",
      "\ud83d\udcbe Saved generated exemplars to 'chain_of_thought_prompt.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re # Import re for parsing citations\n",
    "\n",
    "# Ensure train_sample is available from previous cells\n",
    "# Create the chain of thought prompt.\n",
    "# Since we want to control the output of LLM and standardize it, for instance, we want LLM to provide citations in desired format which is a structured output(e.g. json),\n",
    "#in the prompt we present the instructions, chain of thought instance to be in structured format.\n",
    "\n",
    "# --- Provided reasoning steps and citations for the first 3 examples ---\n",
    "# For the three examples we chose, we use Claude 4 to generate the reasoning process and stored the reasoning_steps, citations in lists\n",
    "\n",
    "prepared_train_sample_indexs = [0,1,2]\n",
    "\n",
    "prepared_reasoning_steps = [\n",
    "    [\n",
    "        \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California\",\n",
    "        \"From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States\",\n",
    "        \"Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox County Regional Airport is in Maine\",\n",
    "        \"Therefore, Knox County Regional Airport is the airport located in Maine\"\n",
    "    ],\n",
    "    [\n",
    "        \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
    "        \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
    "        \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
    "        \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
    "    ],\n",
    "    [\n",
    "        \"From evidence [22]: Austrolebias bellottii is a species of fish that lives in the basins of the Paran\u00e1 River and Uruguay River\",\n",
    "        \"From evidence [24]: The Uruguay River flows from north to south and forms parts of the boundaries of Brazil, Argentina, and Uruguay\",\n",
    "        \"Since Austrolebias bellottii are found in the Uruguay River basin, and the Uruguay River flows from north to south\",\n",
    "        \"Therefore, the river flows from north to south\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "prepared_citations = [\n",
    "    \"[23], [26]\",\n",
    "    \"[7], [8]\",\n",
    "    \"[22], [24]\"\n",
    "]\n",
    "\n",
    "choosen_indices = [1]\n",
    "\n",
    "provided_train_sample_indexs = [prepared_train_sample_indexs[i] for i in choosen_indices]\n",
    "provided_reasoning_steps = [prepared_reasoning_steps[i] for i in choosen_indices]\n",
    "provided_citations = [prepared_citations[1] for i in choosen_indices]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "if 'train_sample' not in globals() or len(train_sample) == 0:\n",
    "    print(\"\u274c train_sample not found or is empty. Please run the data loading and processing cells first.\")\n",
    "else:\n",
    "    print(\"Generating Chain-of-Thought prompt instances...\")\n",
    "\n",
    "    cot_exemplars = []\n",
    "\n",
    "\n",
    "\n",
    "    for i, reasoning_step, provided_citation in zip(provided_train_sample_indexs ,provided_reasoning_steps, provided_citations):\n",
    "        example = train_sample[i]\n",
    "\n",
    "        # Print header for the example\n",
    "        print(f\"\\n--- Processing Example {i+1} ---\")\n",
    "        print(f\"  Question: {example.get('question', '')[:100]}...\")\n",
    "\n",
    "        # Format the context as a list of strings, each including title and sentence\n",
    "        context_list = []\n",
    "        linear_index_counter = 1 # Start counter for linear index\n",
    "        context_sentences_map = {} # Map (title, sent_id) to actual sentence text\n",
    "        if isinstance(example.get('context'), dict):\n",
    "            titles = example['context'].get('title', [])\n",
    "            sentences_lists = example['context'].get('sentences', [])\n",
    "\n",
    "            # Create a mapping from (title, sent_id) to linear_index for validation\n",
    "            title_sentence_map = {}\n",
    "            current_linear_index = 1\n",
    "            for title_idx, (title, sentences) in enumerate(zip(titles, sentences_lists)):\n",
    "                 if isinstance(sentences, list):\n",
    "                      for sent_idx, sentence in enumerate(sentences):\n",
    "                          context_list.append(f\"[{current_linear_index}] Title: {title} - {sentence}\")\n",
    "                          title_sentence_map[(title, sent_idx)] = current_linear_index\n",
    "                          context_sentences_map[(title, sent_idx)] = sentence # Store sentence text\n",
    "                          current_linear_index += 1\n",
    "                 else:\n",
    "                      context_list.append(f\"[{current_linear_index}] Title: {title} - {str(sentences)}\")\n",
    "                      title_sentence_map[(title, 0)] = current_linear_index # Assuming single sentence per title if not list\n",
    "                      context_sentences_map[(title, 0)] = str(sentences) # Store sentence text\n",
    "                      current_linear_index += 1\n",
    "\n",
    "        context_for_pydantic = context_list # Use the list of strings for Contexts\n",
    "\n",
    "\n",
    "        # Determine reasoning and evidence based on index (using provided for first 3)\n",
    "\n",
    "          # Use the provided reasoning and parse the provided citations\n",
    "        reasoning_for_exemplar = reasoning_step\n",
    "\n",
    "        # Parse provided citations string like \"[1], [3]\"\n",
    "        citation_indices = []\n",
    "        citation_string = provided_citation\n",
    "        try:\n",
    "            # Find all numbers within brackets\n",
    "            found_citations = re.findall(r'\\[(\\d+)\\]', citation_string)\n",
    "            citation_indices = [int(c) for c in found_citations]\n",
    "\n",
    "            # Optional: Add validation against ground truth supporting facts linear index\n",
    "            # This requires mapping ground truth supporting facts to linear indices\n",
    "            # based on the `title_sentence_map` created earlier.\n",
    "\n",
    "            # Get ground truth supporting facts from the example\n",
    "            gold_sf_titles = example.get('supporting_facts', {}).get('title', [])\n",
    "            gold_sf_sent_ids = example.get('supporting_facts', {}).get('sent_id', [])\n",
    "            gold_linear_indices = set()\n",
    "\n",
    "            for sf_title, sf_sent_id in zip(gold_sf_titles, gold_sf_sent_ids):\n",
    "                if (sf_title, sf_sent_id) in title_sentence_map:\n",
    "                      gold_linear_indices.add(title_sentence_map[(sf_title, sf_sent_id)])\n",
    "\n",
    "            # Check if provided citations match gold citations\n",
    "            provided_indices_set = set(citation_indices)\n",
    "            if provided_indices_set != gold_linear_indices:\n",
    "                print(f\"\u26a0\ufe0f Warning: Provided citations {provided_indices_set} for example {i+1} do not exactly match ground truth supporting facts {gold_linear_indices}.\")\n",
    "                # Decide whether to use provided or gold. For now, using provided as requested.\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error parsing provided citations '{citation_string}' for example {i+1}: {e}. Using empty list.\")\n",
    "            citation_indices = []\n",
    "\n",
    "\n",
    "        evidence_for_exemplar = citation_indices # Use parsed integer list\n",
    "\n",
    "\n",
    "\n",
    "        # Create the prompt instance structure\n",
    "        cot_instance = {\n",
    "            \"instruction\": \"\"\"You are an evidence-grounded QA assistant. Choose the \"Supporting Facts\" from the \"Contexts\" given to you and filter out the irrelevant information from the Contexts. Using only the \u201cSupporting Facts,\u201d answer the question. Provide: answer \u2014 the short final answer, reasoning \u2014 a step-by-step explanation showing how you used the facts, and evidence \u2014 a list of citations from the contexts you chose as \"Supporting Facts\".\n",
    "    For instance if you choose the first and third sentence as citation from the context, evidence should be [1], [3]. If the facts are insufficient, set answer to \u201cinsufficient information\u201d.\n",
    "     Please ensure that your answer follows this JSON format \"output\": {\n",
    "    \"answer\": \"Failsworth\",\n",
    "    \"reasoning\": [\n",
    "      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
    "      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
    "      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
    "      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
    "    ],\n",
    "    \"evidence\": [\n",
    "      7,\n",
    "      8\n",
    "    ]\n",
    "  }\"\"\",\n",
    "            \"input\": {\n",
    "                \"Question\": example.get('question', ''),\n",
    "                \"Contexts\": context_for_pydantic # Use the list of strings for Contexts\n",
    "            },\n",
    "            # Use the determined reasoning and evidence\n",
    "            \"output\": {\n",
    "                \"answer\": example.get('answer', 'insufficient information'),\n",
    "                \"reasoning\": reasoning_for_exemplar,\n",
    "                \"evidence\": evidence_for_exemplar # Use the determined list of integers\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Print the cot_instance first, then print the supporting facts, THEN append\n",
    "        print(f\"\\n  --- CoT Instance (JSON) ---\")\n",
    "        print(json.dumps(cot_instance, indent=2))\n",
    "\n",
    "        print(f\"\\n  --- Supporting Facts ---\")\n",
    "        supporting_facts = example.get('supporting_facts', {})\n",
    "        if isinstance(supporting_facts, dict) and 'title' in supporting_facts and 'sent_id' in supporting_facts:\n",
    "            sf_titles = supporting_facts['title']\n",
    "            sf_sent_ids = supporting_facts['sent_id']\n",
    "            for sf_title, sf_sent_id in zip(sf_titles, sf_sent_ids):\n",
    "                sentence_text = context_sentences_map.get((sf_title, sf_sent_id), \"Sentence not found\")\n",
    "                print(f\"    Title: '{sf_title}', Sentence ID: {sf_sent_id}, Text: '{sentence_text[:100]}...'\")\n",
    "        else:\n",
    "             print(f\"    Raw Supporting Facts: {repr(supporting_facts)}\")\n",
    "\n",
    "        # Append the cot_instance to the list after printing\n",
    "        print('cot instance: ', cot_instance)\n",
    "        cot_exemplars.append(cot_instance)\n",
    "\n",
    "\n",
    "    # # Print the generated JSON structure (full list)\n",
    "    # print(f\"\\n--- Full CoT Exemplars List ---\")\n",
    "    # print(json.dumps(cot_exemplars, indent=2))\n",
    "\n",
    "    print(f\"\\n\u2705 Generated {len(cot_exemplars)} Chain-of-Thought prompt instances.\")\n",
    "    print(f\"\\n Here is the chain of thought exemplars\")\n",
    "    # Optionally, save this to a file\n",
    "    output_filename = \"chain_of_thought_prompt.json\"\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(cot_exemplars, f, indent=2)\n",
    "    print(f\"\ud83d\udcbe Saved generated exemplars to '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WMqKXRdGgYsu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1760019540917,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "WMqKXRdGgYsu",
    "outputId": "601305bd-f729-4a1f-f912-1fd39e4503ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Debugging Linear Index Calculation for Evidence\n",
      "============================================================\n",
      "Testing linear index calculation on 3 examples:\n",
      "\n",
      "--- Debugging Example 1 ---\n",
      "Question: Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?...\n",
      "\n",
      "Supporting Facts (2 total):\n",
      "  SF 1: Title='Sacramento International Airport', Sentence ID=0\n",
      "    Calculated Linear Index (0-based): 22\n",
      "    Fetched Sentence: 'Sacramento International Airport (IATA: SMF, ICAO: KSMF, FAA LID: SMF) is 10 mi northwest of downtow...'\n",
      "    Original Sentence from SF: 'Sacramento International Airport (IATA: SMF, ICAO: KSMF, FAA LID: SMF) is 10 mi northwest of downtow...'\n",
      "    \u2705 Verification Successful: Fetched sentence matches original.\n",
      "  SF 2: Title='Knox County Regional Airport', Sentence ID=0\n",
      "    Calculated Linear Index (0-based): 25\n",
      "    Fetched Sentence: 'Knox County Regional Airport (IATA: RKD, ICAO: KRKD, FAA LID: RKD) is a county owned, public use air...'\n",
      "    Original Sentence from SF: 'Knox County Regional Airport (IATA: RKD, ICAO: KRKD, FAA LID: RKD) is a county owned, public use air...'\n",
      "    \u2705 Verification Successful: Fetched sentence matches original.\n",
      "\n",
      "--- Debugging Example 2 ---\n",
      "Question: Peter Hobbs founded the company that is based in what town in Manchester?...\n",
      "\n",
      "Supporting Facts (2 total):\n",
      "  SF 1: Title='Peter Hobbs (engineer)', Sentence ID=0\n",
      "    Calculated Linear Index (0-based): 6\n",
      "    Fetched Sentence: 'Peter Wallace Hobbs (1916\u20132008) was an English engineer, and businessman, who with Bill Russell form...'\n",
      "    Original Sentence from SF: 'Peter Wallace Hobbs (1916\u20132008) was an English engineer, and businessman, who with Bill Russell form...'\n",
      "    \u2705 Verification Successful: Fetched sentence matches original.\n",
      "  SF 2: Title='Russell Hobbs', Sentence ID=0\n",
      "    Calculated Linear Index (0-based): 7\n",
      "    Fetched Sentence: 'Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, Eng...'\n",
      "    Original Sentence from SF: 'Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, Eng...'\n",
      "    \u2705 Verification Successful: Fetched sentence matches original.\n",
      "\n",
      "--- Debugging Example 3 ---\n",
      "Question: What direction does the river that Austrolebias bellotti are found in flow?...\n",
      "\n",
      "Supporting Facts (2 total):\n",
      "  SF 1: Title='Austrolebias bellottii', Sentence ID=0\n",
      "    Calculated Linear Index (0-based): 21\n",
      "    Fetched Sentence: 'Austrolebias bellottii is a species of fish that lives in the basins of the Paran\u00e1 River and Uruguay...'\n",
      "    Original Sentence from SF: 'Austrolebias bellottii is a species of fish that lives in the basins of the Paran\u00e1 River and Uruguay...'\n",
      "    \u2705 Verification Successful: Fetched sentence matches original.\n",
      "  SF 2: Title='Uruguay River', Sentence ID=1\n",
      "    Calculated Linear Index (0-based): 23\n",
      "    Fetched Sentence: ' It flows from north to south and forms parts of the boundaries of Brazil, Argentina, and Uruguay, s...'\n",
      "    Original Sentence from SF: ' It flows from north to south and forms parts of the boundaries of Brazil, Argentina, and Uruguay, s...'\n",
      "    \u2705 Verification Successful: Fetched sentence matches original.\n",
      "\n",
      "============================================================\n",
      "\ud83d\udd0d Debugging complete.\n"
     ]
    }
   ],
   "source": [
    "# Debugging the linear index calculation for evidence\n",
    "import json\n",
    "\n",
    "print(\"\ud83d\udd0d Debugging Linear Index Calculation for Evidence\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure train_sample is available from previous cells\n",
    "if 'train_sample' not in globals() or len(train_sample) == 0:\n",
    "    print(\"\u274c train_sample not found or is empty. Please run the data loading and processing cells first.\")\n",
    "else:\n",
    "    # Use a few examples for debugging\n",
    "    num_debug_examples = min(3, len(train_sample))\n",
    "    debug_examples = train_sample.select(range(num_debug_examples))\n",
    "\n",
    "    print(f\"Testing linear index calculation on {len(debug_examples)} examples:\")\n",
    "\n",
    "    for i, example in enumerate(debug_examples):\n",
    "        print(f\"\\n--- Debugging Example {i+1} ---\")\n",
    "        question = example.get('question', '')\n",
    "        print(f\"Question: {question[:100]}...\")\n",
    "\n",
    "        context_data = example.get('context', {})\n",
    "        supporting_facts_data = example.get('supporting_facts', {})\n",
    "\n",
    "        if not isinstance(context_data, dict) or not isinstance(supporting_facts_data, dict):\n",
    "            print(\"\u26a0\ufe0f Skipping example: Context or Supporting Facts not in expected dict format.\")\n",
    "            continue\n",
    "\n",
    "        context_titles = context_data.get('title', [])\n",
    "        context_sentences_lists = context_data.get('sentences', [])\n",
    "        sf_titles = supporting_facts_data.get('title', [])\n",
    "        sf_sent_ids = supporting_facts_data.get('sent_id', [])\n",
    "\n",
    "        # Flatten the context sentences to easily access by linear index\n",
    "        flat_context_sentences = [sent for sublist in context_sentences_lists for sent in sublist]\n",
    "\n",
    "        print(f\"\\nSupporting Facts ({len(sf_titles)} total):\")\n",
    "        for j, (sf_title, sf_sent_id) in enumerate(zip(sf_titles, sf_sent_ids)):\n",
    "            print(f\"  SF {j+1}: Title='{sf_title}', Sentence ID={sf_sent_id}\")\n",
    "\n",
    "            try:\n",
    "                title_index = context_titles.index(sf_title)\n",
    "\n",
    "                if title_index < len(context_sentences_lists) and sf_sent_id < len(context_sentences_lists[title_index]):\n",
    "                    # Calculate the linear index\n",
    "                    linear_index = sum(len(context_sentences_lists[k]) for k in range(title_index)) + sf_sent_id\n",
    "\n",
    "                    # Fetch sentence using calculated linear index\n",
    "                    fetched_sentence = flat_context_sentences[linear_index]\n",
    "\n",
    "                    # Get the original sentence from supporting facts (for comparison)\n",
    "                    original_sentence_from_sf = context_sentences_lists[title_index][sf_sent_id]\n",
    "\n",
    "\n",
    "                    print(f\"    Calculated Linear Index (0-based): {linear_index}\")\n",
    "                    print(f\"    Fetched Sentence: '{fetched_sentence[:100]}...'\")\n",
    "                    print(f\"    Original Sentence from SF: '{original_sentence_from_sf[:100]}...'\")\n",
    "\n",
    "                    # Compare fetched sentence with original sentence from context\n",
    "                    if fetched_sentence == original_sentence_from_sf:\n",
    "                        print(\"    \u2705 Verification Successful: Fetched sentence matches original.\")\n",
    "                    else:\n",
    "                        print(\"    \u274c Verification Failed: Fetched sentence DOES NOT match original!\")\n",
    "                        print(f\"      Fetched: {fetched_sentence}\")\n",
    "                        print(f\"      Original: {original_sentence_from_sf}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"    \u26a0\ufe0f Skipping SF {j+1}: Sentence ID {sf_sent_id} out of bounds for title '{sf_title}' (has {len(context_sentences_lists[title_index])} sentences).\")\n",
    "\n",
    "            except ValueError:\n",
    "                print(f\"    \u26a0\ufe0f Skipping SF {j+1}: Title '{sf_title}' not found in context titles.\")\n",
    "            except IndexError:\n",
    "                 print(f\"    \u26a0\ufe0f Skipping SF {j+1}: Linear index {linear_index} out of bounds for flattened context ({len(flat_context_sentences)} sentences).\")\n",
    "            except Exception as e:\n",
    "                print(f\"    \u274c An unexpected error occurred for SF {j+1}: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"\ud83d\udd0d Debugging complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qy-Mae8s2yEX",
   "metadata": {
    "id": "qy-Mae8s2yEX"
   },
   "source": [
    "## Structural Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mweoUA6dgqRq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1760019540970,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "mweoUA6dgqRq",
    "outputId": "f7eee71e-1d11-4858-d459-82671f80afbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udc1b DEBUG: Starting test...\n",
      "\ud83d\udcdd Testing input creation...\n",
      "\u2705 Input creation successful\n",
      "\ud83d\udd0d Testing parse_and_validate_response...\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'citations'])\n",
      "\u2705 QAOutput created successfully!\n",
      "\n",
      "\ud83d\udccb RESULTS:\n",
      "Answer: Second Battle of St Albans\n",
      "Citations: [2, 3]\n",
      "Reasoning type: <class 'list'>\n",
      "Reasoning: [\"I need to find information about Sir Thomas Kyriell's execution and which battle it followed.\", \"From [2], I can see that 'He was executed after the Second Battle of St Albans.'\", \"From [3], I can confirm that 'The Second Battle of St Albans was a battle of the English Wars of the Roses, fought on 17 February 1461.'\", 'This directly answers the question about which battle from the Wars of the Roses preceded his execution.']\n",
      "\ud83d\udd0d Counting steps in reasoning type: <class 'list'>\n",
      "\ud83d\udd0d Reasoning content: [\"I need to find information about Sir Thomas Kyriell's execution and which battle it followed.\", \"F...\n",
      "Number of reasoning steps: 4\n",
      "\n",
      "\ud83e\uddea Testing with JSON string input...\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'citations'])\n",
      "\u2705 JSON string parsing successful!\n",
      "Answer from JSON string: Second Battle of St Albans\n",
      "Citations from JSON string: [2, 3]\n",
      "Reasoning from JSON string: [\"I need to find information about Sir Thomas Kyriell's execution and which battle it followed.\", \"From [2], I can see that 'He was executed after the Second Battle of St Albans.'\", \"From [3], I can confirm that 'The Second Battle of St Albans was a battle of the English Wars of the Roses, fought on 17 February 1461.'\", 'This directly answers the question about which battle from the Wars of the Roses preceded his execution.']\n",
      "\n",
      "\ud83e\uddea Testing with invalid citation...\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'citations'])\n",
      "\u274c Parsing error: 1 validation error for QAOutput\n",
      "citations.0\n",
      "  Value error, The citation 5 is not in the contexts. Expected range 1-3 [type=value_error, input_value=5, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\u274c Error: name 'fallback_parse' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-3625582184.py\", line 173, in parse_and_validate_response\n",
      "    return QAOutput(**parsed)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/main.py\", line 253, in __init__\n",
      "    'A custom validator is returning a value other than `self`.\\n'\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for QAOutput\n",
      "citations.0\n",
      "  Value error, The citation 5 is not in the contexts. Expected range 1-3 [type=value_error, input_value=5, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-3625582184.py\", line 320, in test_qa_system\n",
      "    result3 = parse_and_validate_response(invalid_response, mock_contexts)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipython-input-3625582184.py\", line 183, in parse_and_validate_response\n",
      "    return fallback_parse(str(raw_response), contexts)\n",
      "           ^^^^^^^^^^^^^^\n",
      "NameError: name 'fallback_parse' is not defined\n"
     ]
    }
   ],
   "source": [
    "#This code cell provides data validation via Pydantic package.\n",
    "\n",
    "#The Pydantic package provides schema based data modeling such that,\n",
    "\n",
    "#we could ensure, structural input and output to the large language model follows a designed schema\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field, ValidationError, validator\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# =============================================\n",
    "\n",
    "# PYDANTIC DATA MODELS (matching your CoT format)\n",
    "\n",
    "# =============================================\n",
    "\n",
    "\n",
    "\n",
    "class QAInput(BaseModel):\n",
    "\n",
    "    \"\"\"Input structure matching your CoT format\"\"\"\n",
    "\n",
    "    Question: str\n",
    "\n",
    "    Contexts: List[str]\n",
    "\n",
    "\n",
    "\n",
    "class QAOutput(BaseModel):\n",
    "\n",
    "    \"\"\"Output structure matching your CoT format\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"Short final answer\")\n",
    "\n",
    "    reasoning: Union[str, List[str]] = Field(description=\"Step-by-step reasoning\")\n",
    "\n",
    "    citations: List[int] = Field(description=\"Citations like 1, 2\")\n",
    "\n",
    "\n",
    "\n",
    "    @validator('citations', each_item=True)\n",
    "\n",
    "    def validate_citations(cls, v, values):\n",
    "\n",
    "\n",
    "\n",
    "        # Get num_contexts from the class-level variable we'll set\n",
    "\n",
    "        num_contexts = getattr(cls, '_num_contexts', 0)\n",
    "\n",
    "        if num_contexts <= 0:\n",
    "\n",
    "            return v\n",
    "\n",
    "\n",
    "\n",
    "        if v <= 0 or v > num_contexts:\n",
    "\n",
    "            raise ValueError(f'The citation {v} is not in the contexts. Expected range 1-{num_contexts}')\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "\n",
    "    def get_reasoning_steps_count(self) -> int:\n",
    "\n",
    "        \"\"\"Count the number of reasoning steps\"\"\"\n",
    "\n",
    "        print(f\"\ud83d\udd0d Counting steps in reasoning type: {type(self.reasoning)}\")\n",
    "\n",
    "        print(f\"\ud83d\udd0d Reasoning content: {str(self.reasoning)[:100]}...\")\n",
    "\n",
    "\n",
    "\n",
    "        if not self.reasoning:\n",
    "\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(self.reasoning, str):\n",
    "\n",
    "            steps = re.findall(r'(?:^\\d+\\.|^-|^\u2022)', self.reasoning, re.MULTILINE)\n",
    "\n",
    "            step_count = len(steps) if steps else 1\n",
    "\n",
    "            print(f\"\ud83d\udd0d Found {step_count} steps\")\n",
    "\n",
    "            return step_count\n",
    "\n",
    "        elif isinstance(self.reasoning, list):\n",
    "\n",
    "            return len(self.reasoning)\n",
    "\n",
    "        else:\n",
    "\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "def parse_and_validate_response(raw_response: Union[str, dict], contexts: List[str]) -> QAOutput:\n",
    "\n",
    "    \"\"\"Parse and validate response wiwth Pydantic\n",
    "\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        raw_response: Either a JSON string or a dictionary containing the response\n",
    "\n",
    "        contexts: List of context strings for validation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Set the number of contexts for validation\n",
    "\n",
    "        QAOutput._num_contexts = len(contexts)\n",
    "\n",
    "\n",
    "\n",
    "        # Handle both string and dict inputs\n",
    "\n",
    "        if isinstance(raw_response, dict):\n",
    "\n",
    "            parsed = raw_response\n",
    "\n",
    "        elif isinstance(raw_response, str):\n",
    "\n",
    "            # Try to extract JSON from response string\n",
    "\n",
    "            json_match = re.search(r'\\{.*\\}', raw_response, re.DOTALL)\n",
    "\n",
    "            if json_match:\n",
    "\n",
    "                json_str = json_match.group()\n",
    "\n",
    "                parsed = json.loads(json_str)\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Use fallback parsing for non-JSON strings\n",
    "\n",
    "                print('The answer is not in the format of dict or json, using fallback parse')\n",
    "\n",
    "                print('The answer is in the format of: ', type(raw_response))\n",
    "\n",
    "                return fallback_parse(raw_response, contexts)\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"Unsupported input type: {type(raw_response)}\")\n",
    "\n",
    "\n",
    "\n",
    "        print(f'\ud83d\udc1b DEBUG: Parsed keys: {parsed.keys()}')\n",
    "\n",
    "        return QAOutput(**parsed)\n",
    "\n",
    "\n",
    "\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "\n",
    "        print(f\"\u274c Parsing error: {e}\")\n",
    "\n",
    "        print(\"\ud83d\udd04 Falling back to fallback parser...\")\n",
    "\n",
    "        return fallback_parse(str(raw_response), contexts)\n",
    "\n",
    "\n",
    "def test_qa_system():\n",
    "\n",
    "    \"\"\"Test the QA system with debugging\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Mock response as dictionary (like your actual output)\n",
    "\n",
    "    mock_response = {\n",
    "\n",
    "        \"answer\": \"Second Battle of St Albans\",\n",
    "\n",
    "        \"reasoning\": [\n",
    "\n",
    "            \"I need to find information about Sir Thomas Kyriell's execution and which battle it followed.\",\n",
    "\n",
    "            \"From [2], I can see that 'He was executed after the Second Battle of St Albans.'\",\n",
    "\n",
    "            \"From [3], I can confirm that 'The Second Battle of St Albans was a battle of the English Wars of the Roses, fought on 17 February 1461.'\",\n",
    "\n",
    "            \"This directly answers the question about which battle from the Wars of the Roses preceded his execution.\"\n",
    "\n",
    "        ],\n",
    "\n",
    "        \"citations\": [2, 3]\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Mock contexts\n",
    "\n",
    "    mock_contexts = [\n",
    "\n",
    "        \"[1] Title: Sir Thomas Kyriell - Sir Thomas Kyriell (1396\u20131461) was an English soldier of the Hundred Years' War and the opening of the Wars of the Roses.\",\n",
    "\n",
    "        \"[2] Title: Sir Thomas Kyriell - He was executed after the Second Battle of St Albans.\",\n",
    "\n",
    "        \"[3] Title: Second Battle of St Albans - The Second Battle of St Albans was a battle of the English Wars of the Roses, fought on 17 February 1461, at St Albans.\"\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        print(f\"\ud83d\udc1b DEBUG: Starting test...\")\n",
    "\n",
    "        print(f\"\ud83d\udcdd Testing input creation...\")\n",
    "\n",
    "\n",
    "\n",
    "        test_input = QAInput(\n",
    "\n",
    "            Question=\"Sir Thomas Kyriell was executed after which battle from the Wars of the Roses?\",\n",
    "\n",
    "            Contexts=mock_contexts\n",
    "\n",
    "        )\n",
    "\n",
    "        print(f\"\u2705 Input creation successful\")\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"\ud83d\udd0d Testing parse_and_validate_response...\")\n",
    "\n",
    "        # Test with dictionary input\n",
    "\n",
    "        result = parse_and_validate_response(mock_response, mock_contexts)\n",
    "\n",
    "        print(f\"\u2705 QAOutput created successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n\ud83d\udccb RESULTS:\")\n",
    "\n",
    "        print(f\"Answer: {result.answer}\")\n",
    "\n",
    "        print(f\"Citations: {result.citations}\")\n",
    "\n",
    "        print(f\"Reasoning type: {type(result.reasoning)}\")\n",
    "\n",
    "        print(f\"Reasoning: {result.reasoning}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Test the method\n",
    "\n",
    "        if hasattr(result, 'get_reasoning_steps_count'):\n",
    "\n",
    "            steps_count = result.get_reasoning_steps_count()\n",
    "\n",
    "            print(f\"Number of reasoning steps: {steps_count}\")\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"\u274c Method get_reasoning_steps_count not found!\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n\ud83e\uddea Testing with JSON string input...\")\n",
    "\n",
    "        # Test with JSON string input\n",
    "\n",
    "        json_string = json.dumps(mock_response)\n",
    "\n",
    "        result2 = parse_and_validate_response(json_string, mock_contexts)\n",
    "\n",
    "        print(f\"\u2705 JSON string parsing successful!\")\n",
    "\n",
    "        print(f\"Answer from JSON string: {result2.answer}\")\n",
    "\n",
    "        print(f\"Citations from JSON string: {result2.citations}\")\n",
    "\n",
    "        print(f\"Reasoning from JSON string: {result2.reasoning}\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n\ud83e\uddea Testing with invalid citation...\")\n",
    "\n",
    "        # Test validation with invalid citation\n",
    "\n",
    "        invalid_response = {\n",
    "\n",
    "            \"answer\": \"Test\",\n",
    "\n",
    "            \"reasoning\": [\"Test reasoning\"],\n",
    "\n",
    "            \"citations\": [5]  # Invalid - only 3 contexts available\n",
    "\n",
    "        }\n",
    "\n",
    "        try:\n",
    "\n",
    "            result3 = parse_and_validate_response(invalid_response, mock_contexts)\n",
    "\n",
    "            print(\"\u274c Should have failed validation!\")\n",
    "\n",
    "        except ValidationError as ve:\n",
    "\n",
    "            print(f\"\u2705 Validation correctly caught invalid citation: {ve}\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n\u2705 All tests completed successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"\u274c Error: {e}\")\n",
    "\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "\n",
    "# Alternative approach using a context manager for cleaner validation\n",
    "\n",
    "class ValidationContext:\n",
    "\n",
    "    \"\"\"Context manager to set validation parameters\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, num_contexts: int):\n",
    "\n",
    "        self.num_contexts = num_contexts\n",
    "\n",
    "\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        QAOutput._num_contexts = self.num_contexts\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "\n",
    "        if hasattr(QAOutput, '_num_contexts'):\n",
    "\n",
    "            delattr(QAOutput, '_num_contexts')\n",
    "\n",
    "\n",
    "\n",
    "def parse_and_validate_with_context(raw_response: Union[str, dict], contexts: List[str]) -> QAOutput:\n",
    "\n",
    "    \"\"\"Alternative version using context manager\"\"\"\n",
    "\n",
    "    with ValidationContext(len(contexts)):\n",
    "\n",
    "        if isinstance(raw_response, dict):\n",
    "\n",
    "            return QAOutput(**raw_response)\n",
    "\n",
    "        elif isinstance(raw_response, str):\n",
    "\n",
    "            json_match = re.search(r'\\{.*\\}', raw_response, re.DOTALL)\n",
    "\n",
    "            if json_match:\n",
    "\n",
    "                parsed = json.loads(json_match.group())\n",
    "\n",
    "                return QAOutput(**parsed)\n",
    "\n",
    "            else:\n",
    "\n",
    "                return fallback_parse(raw_response, contexts)\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"Unsupported input type: {type(raw_response)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_qa_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cqom15y6lt4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1760019541036,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "cqom15y6lt4",
    "outputId": "66f6dfdb-eb70-4350-f93d-ce9c04f2a608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Extractive reasoning helper functions loaded successfully!\n",
      "\ud83d\udcdd Functions available: split_into_sentences, find_sentence_containing_answer, generate_extractive_reasoning\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# EXTRACTIVE REASONING HELPER FUNCTIONS\n",
    "# =============================================\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into sentences using simple regex.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to split\n",
    "\n",
    "    Returns:\n",
    "        List of sentences\n",
    "    \"\"\"\n",
    "    # Simple sentence splitter - splits on period, exclamation, question mark followed by space\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def find_sentence_containing_answer(passage_text: str, answer: str, question: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Find the sentence in passage that best supports the answer.\n",
    "\n",
    "    Uses keyword overlap scoring to find the most relevant sentence.\n",
    "\n",
    "    Args:\n",
    "        passage_text: The full passage text\n",
    "        answer: The answer string to look for\n",
    "        question: Optional question for additional context\n",
    "\n",
    "    Returns:\n",
    "        The most relevant sentence from the passage\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = split_into_sentences(passage_text)\n",
    "\n",
    "    if not sentences:\n",
    "        # Fallback: return first 150 chars if no sentences found\n",
    "        return passage_text[:150].strip() + (\"...\" if len(passage_text) > 150 else \"\")\n",
    "\n",
    "    # Prepare keywords for scoring\n",
    "    answer_words = set(answer.lower().split())\n",
    "    question_words = set(question.lower().split()) if question else set()\n",
    "    # Remove common stop words from question\n",
    "    stop_words = {'what', 'where', 'when', 'who', 'which', 'how', 'is', 'are', 'was', 'were',\n",
    "                  'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'that', 'this'}\n",
    "    question_words = question_words - stop_words\n",
    "\n",
    "    # Score each sentence\n",
    "    best_sent = sentences[0]  # Default to first sentence\n",
    "    best_score = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_lower = sent.lower()\n",
    "\n",
    "        # Count keyword matches (weight answer keywords higher)\n",
    "        answer_overlap = sum(1 for word in answer_words if word in sent_lower)\n",
    "        question_overlap = sum(1 for word in question_words if word in sent_lower)\n",
    "\n",
    "        # Score: answer keywords worth 2 points, question keywords worth 1 point\n",
    "        score = answer_overlap * 2 + question_overlap\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_sent = sent\n",
    "\n",
    "    return best_sent.strip()\n",
    "\n",
    "\n",
    "def generate_extractive_reasoning(\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    selected_passages: List[Dict],\n",
    "    evidence_indices: List[int]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate natural reasoning by extracting relevant sentences from passages.\n",
    "\n",
    "    This function:\n",
    "    1. Extracts the most relevant sentence from each evidence passage\n",
    "    2. Connects them with natural discourse markers\n",
    "    3. Embeds citations where evidence is used\n",
    "    4. Adds a conclusion\n",
    "\n",
    "    Args:\n",
    "        question: The question being answered\n",
    "        answer: The correct answer\n",
    "        selected_passages: List of passage dicts with 'title' and 'text' keys\n",
    "        evidence_indices: List of 1-indexed passage numbers that support the answer\n",
    "\n",
    "    Returns:\n",
    "        Natural reasoning text with embedded citations (30-100 tokens)\n",
    "    \"\"\"\n",
    "    if not evidence_indices or answer == \"insufficient context\":\n",
    "        return \"Based on the available evidence, I cannot determine a definitive answer to this question.\"\n",
    "\n",
    "    # Extract key sentences from each evidence passage\n",
    "    evidence_sents = []\n",
    "    for idx in evidence_indices:\n",
    "        if 1 <= idx <= len(selected_passages):\n",
    "            passage = selected_passages[idx - 1]  # Convert to 0-indexed\n",
    "            # Extract most relevant sentence\n",
    "            key_sent = find_sentence_containing_answer(\n",
    "                passage['text'],\n",
    "                answer,\n",
    "                question\n",
    "            )\n",
    "            evidence_sents.append((idx, key_sent))\n",
    "\n",
    "    if not evidence_sents:\n",
    "        return f\"The answer is {answer}.\"\n",
    "\n",
    "    # Build natural reasoning with discourse connectors\n",
    "    reasoning_parts = []\n",
    "\n",
    "    # Opening: Frame the task\n",
    "    reasoning_parts.append(\"To answer this question,\")\n",
    "\n",
    "    # Middle: Present evidence with natural connectors\n",
    "    if len(evidence_sents) == 1:\n",
    "        idx, sent = evidence_sents[0]\n",
    "        reasoning_parts.append(f\"evidence [{idx}] shows that {sent}\")\n",
    "\n",
    "    elif len(evidence_sents) == 2:\n",
    "        idx1, sent1 = evidence_sents[0]\n",
    "        idx2, sent2 = evidence_sents[1]\n",
    "        reasoning_parts.append(f\"evidence [{idx1}] shows that {sent1},\")\n",
    "        reasoning_parts.append(f\"and evidence [{idx2}] indicates that {sent2}.\")\n",
    "\n",
    "    else:\n",
    "        # 3+ pieces of evidence\n",
    "        for i, (idx, sent) in enumerate(evidence_sents):\n",
    "            if i == 0:\n",
    "                reasoning_parts.append(f\"evidence [{idx}] shows that {sent},\")\n",
    "            elif i < len(evidence_sents) - 1:\n",
    "                reasoning_parts.append(f\"evidence [{idx}] indicates that {sent},\")\n",
    "            else:\n",
    "                reasoning_parts.append(f\"and evidence [{idx}] states that {sent}.\")\n",
    "\n",
    "    # Conclusion: Connect to final answer\n",
    "    if len(evidence_sents) > 1:\n",
    "        # Multiple evidence pieces - show synthesis\n",
    "        citation_list = \", \".join([f\"[{idx}]\" for idx, _ in evidence_sents])\n",
    "        reasoning_parts.append(f\"Based on {citation_list}, the answer is {answer}.\")\n",
    "    else:\n",
    "        reasoning_parts.append(f\"Therefore, the answer is {answer}.\")\n",
    "\n",
    "    # Join all parts\n",
    "    reasoning_text = \" \".join(reasoning_parts)\n",
    "\n",
    "    return reasoning_text\n",
    "\n",
    "\n",
    "print(\"\u2705 Extractive reasoning helper functions loaded successfully!\")\n",
    "print(\"\ud83d\udcdd Functions available: split_into_sentences, find_sentence_containing_answer, generate_extractive_reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gltxTFq04Une",
   "metadata": {
    "id": "gltxTFq04Une"
   },
   "source": [
    "## Prompt template building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9tbt6ub6o8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1760019541063,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "9tbt6ub6o8",
    "outputId": "0b58df43-650d-4af1-ddf5-fbc206e5919a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Successfully loaded 1 CoT exemplars from 'chain_of_thought_prompt.json'\n",
      "loaded cot exemplars:  [{'instruction': 'You are an evidence-grounded QA assistant. Choose the \"Supporting Facts\" from the \"Contexts\" given to you and filter out the irrelevant information from the Contexts. Using only the \u201cSupporting Facts,\u201d answer the question. Provide: answer \u2014 the short final answer, reasoning \u2014 a step-by-step explanation showing how you used the facts, and evidence \u2014 a list of citations from the contexts you chose as \"Supporting Facts\".\\n    For instance if you choose the first and third sentence as citation from the context, evidence should be [1], [3]. If the facts are insufficient, set answer to \u201cinsufficient information\u201d.\\n     Please ensure that your answer follows this JSON format \"output\": {\\n    \"answer\": \"Failsworth\",\\n    \"reasoning\": [\\n      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\\n      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\\n      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\\n      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\\n    ],\\n    \"evidence\": [\\n      7,\\n      8\\n    ]\\n  }', 'input': {'Question': 'Peter Hobbs founded the company that is based in what town in Manchester?', 'Contexts': ['[1] Title: Cains Brewery - Cains is a brewery in Liverpool, England, founded in 1858 by Robert Cain.', '[2] Title: Cains Brewery -  The company merged with Peter Walker & Son in 1921 to form Walker Cains.', '[3] Title: Cains Brewery -  Peter Walker & Son had a large brewery in Warrington so sold its Liverpool brewery to Higsons in 1923.', '[4] Title: Cains Brewery -  Boddingtons of Manchester took over in 1985.', \"[5] Title: Cains Brewery -  In 1990 Whitbread acquired Boddington's brewing operations and closed the then Higsons Brewery in 1990.\", '[6] Title: Cains Brewery -  It was reopened by GB Breweries, who became part of Bryggerigruppen in 1991, and in 2002 was sold to Gardener-Shaw for \u00a33.4 million.', '[7] Title: Peter Hobbs (engineer) - Peter Wallace Hobbs (1916\u20132008) was an English engineer, and businessman, who with Bill Russell formed the well-known electrical appliance company Russell Hobbs.', '[8] Title: Russell Hobbs - Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England, United Kingdom.', '[9] Title: Curzon Ashton F.C. - Curzon Ashton Football Club is a semi-professional football club based in the market town of Ashton-under-Lyne, Greater Manchester, England, that competes in the National League North, the sixth-highest division overall in the English football league system, and are members of the Manchester County Football Association.', '[10] Title: Curzon Ashton F.C. -  Nicknamed \"the Nash\", the club was founded in 1963 and moved to its current stadium, Tameside Stadium, in 2005.', '[11] Title: Manchester Liners - Manchester Liners was a cargo and passenger shipping company founded in 1898, based in Manchester, England.', '[12] Title: Manchester Liners -  The line pioneered the regular passage of ocean-going vessels along the Manchester Ship Canal.', '[13] Title: Manchester Liners -  Its main sphere of operation was the transatlantic shipping trade, but the company also operated services to the Mediterranean.', \"[14] Title: Manchester Liners -  All of the line's vessels were registered in the Port of Manchester, and many were lost to enemy action during the First and Second World Wars.\", '[15] Title: Dobson &amp; Barlow - Dobson and Barlow were manufacturers of textile machinery with works in Bolton, Greater Manchester.', '[16] Title: Dobson &amp; Barlow -  Isaac Dobson (1767-1833) founded the company in 1790 and by 1850 Dobson in partnership with Peter Rothwell had premises in Blackhorse Street which produced mules for cotton spinning.', '[17] Title: Dobson &amp; Barlow -  The company moved to a larger factory in Kay Street which had 1,600 workers in 1860.', '[18] Title: The Flux Foundation - The Flux Foundation is a non-profit group based in the San Francisco Bay Area whose main objective is to build community through the creation of large-scale public art.', '[19] Title: The Flux Foundation -  The group creates both public art and public artists.', '[20] Title: The Flux Foundation -  It was founded on April 1, 2010 and was established as a California corporation on January 6, 2011, by Rebecca Anders, Jessica Hobbs, Peter (PK) Kimelman, Catherine Magee and Colinne Hemrich.', '[21] Title: The Flux Foundation -  As of 2016 the Board of Directors consists of Kimelman, Hobbs, Magee, Paul Belger and Thwen Chaloemtiarana.', '[22] Title: The Flux Foundation -  It is a \"public charity\" 501c(3) non-profit, supported by grants, public donations and the display of its artworks.', '[23] Title: The Flux Foundation -  Its works are notable not only for their scale but interactivity with the audience relying on participation to create atmospheric effects.', '[24] Title: The Flux Foundation -  The group draws upon Situationist and Fluxus ideas of creating spectacle to establish social connections as an effect of the artwork.', '[25] Title: The Flux Foundation -  This \"community creation\" is mirrored in the pieces\\' creation by a large-number of volunteers who themselves create new social networks.', '[26] Title: The Flux Foundation -  The Foundation also provides mentorship and fiscal sponsorship to other large-scale artists.', '[27] Title: The Flux Foundation -  Flux is administratively based in San Francisco, while its studios are located at American Steel Studios in West Oakland, California.', '[28] Title: Manchester Sport and Leisure Trust - Manchester Sport and Leisure Trust is a non-profit organisation which manages sport and leisure venues in the City of Manchester, United Kingdom.', '[29] Title: Manchester Sport and Leisure Trust -  MSLT was founded in 1997 and is a company limited by guarantee with charitable status with a turnover of \u00a312.5m. MSLT is based at the Sportcity site.', '[30] Title: ITV Granada - ITV Granada (formerly Granada Television; informally Granada) is the Channel 3 regional service for North West England.', '[31] Title: ITV Granada -  The licence for the region has been held by ITV Broadcasting Limited since November 2008.', '[32] Title: ITV Granada -  It is the largest independent television-franchise producing company in the UK, accounting for 25% of the total broadcasting output of the ITV network.', '[33] Title: ITV Granada -  It had been held by Granada Television, which was founded by Sidney Bernstein and based at Granada Studios on Quay Street in Manchester since its inception.', '[34] Title: ITV Granada -  This was the only surviving company of the original four Independent Television Authority franchisees from 1954; Granada Media Group (parent company of Granada Television) merged with Carlton Communications to form ITV plc in 2004.', '[35] Title: ITV Granada -  It covers Cheshire, Greater Manchester, Lancashire, Merseyside, northwestern Derbyshire, part of Cumbria and North Yorkshire.', '[36] Title: ITV Granada -  On 15 July 2009, the Isle of Man was transferred to ITV Granada from ITV Border (even though the Isle of Man is a British Crown Dependency and is not part of the United Kingdom).', '[37] Title: Peter Hesketh-Fleetwood - Sir Peter Hesketh-Fleetwood, 1st Baronet, (9 May 1801\\xa0\u2013 12 April 1866) was an English landowner, developer and Member of Parliament, who founded the town of Fleetwood, in Lancashire, England.', '[38] Title: Peter Hesketh-Fleetwood -  Born Peter Hesketh, he changed his name by Royal assent to Hesketh-Fleetwood, incorporating the name of his ancestors, and was later created Baronet Fleetwood.', '[39] Title: Peter Hesketh-Fleetwood -  Predeceased by an older brother, he inherited estates in west Lancashire in 1824.', '[40] Title: Peter Hesketh-Fleetwood -  Inspired by the transport developments of the early 19th century, he decided to bring the railway to the Lancashire coast and develop a holiday resort and port.', '[41] Title: Peter Hesketh-Fleetwood -  He hired architect Decimus Burton to design his new town, which he named Fleetwood; construction began in 1836.', '[42] Title: Peter Hesketh-Fleetwood -  Hesketh-Fleetwood was instrumental in the formation of the Preston and Wyre Railway Company and with his financial support, a railway line was built between Preston and Fleetwood which opened in 1840.']}, 'output': {'answer': 'Failsworth', 'reasoning': ['From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell', 'From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England', 'Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth', 'Therefore, the company Peter Hobbs founded is based in Failsworth'], 'evidence': [7, 8]}}]\n",
      "\n",
      "Structure of a single exemplar:\n",
      "instruction: You are an evidence-grounded QA assistant. Choose the \"Supporting Facts\" from the \"Contexts\" given to you and filter out the irrelevant information from the Contexts. Using only the \u201cSupporting Facts,\u201d answer the question. Provide: answer \u2014 the short final answer, reasoning \u2014 a step-by-step explanation showing how you used the facts, and evidence \u2014 a list of citations from the contexts you chose as \"Supporting Facts\".\n",
      "    For instance if you choose the first and third sentence as citation from the context, evidence should be [1], [3]. If the facts are insufficient, set answer to \u201cinsufficient information\u201d.\n",
      "     Please ensure that your answer follows this JSON format \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
      "      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
      "      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
      "      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "      7,\n",
      "      8\n",
      "    ]\n",
      "  }\n",
      "input: {'Question': 'Peter Hobbs founded the company that is based in what town in Manchester?', 'Contexts': ['[1] Title: Cains Brewery - Cains is a brewery in Liverpool, England, founded in 1858 by Robert Cain.', '[2] Title: Cains Brewery -  The company merged with Peter Walker & Son in 1921 to form Walker Cains.', '[3] Title: Cains Brewery -  Peter Walker & Son had a large brewery in Warrington so sold its Liverpool brewery to Higsons in 1923.', '[4] Title: Cains Brewery -  Boddingtons of Manchester took over in 1985.', \"[5] Title: Cains Brewery -  In 1990 Whitbread acquired Boddington's brewing operations and closed the then Higsons Brewery in 1990.\", '[6] Title: Cains Brewery -  It was reopened by GB Breweries, who became part of Bryggerigruppen in 1991, and in 2002 was sold to Gardener-Shaw for \u00a33.4 million.', '[7] Title: Peter Hobbs (engineer) - Peter Wallace Hobbs (1916\u20132008) was an English engineer, and businessman, who with Bill Russell formed the well-known electrical appliance company Russell Hobbs.', '[8] Title: Russell Hobbs - Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England, United Kingdom.', '[9] Title: Curzon Ashton F.C. - Curzon Ashton Football Club is a semi-professional football club based in the market town of Ashton-under-Lyne, Greater Manchester, England, that competes in the National League North, the sixth-highest division overall in the English football league system, and are members of the Manchester County Football Association.', '[10] Title: Curzon Ashton F.C. -  Nicknamed \"the Nash\", the club was founded in 1963 and moved to its current stadium, Tameside Stadium, in 2005.', '[11] Title: Manchester Liners - Manchester Liners was a cargo and passenger shipping company founded in 1898, based in Manchester, England.', '[12] Title: Manchester Liners -  The line pioneered the regular passage of ocean-going vessels along the Manchester Ship Canal.', '[13] Title: Manchester Liners -  Its main sphere of operation was the transatlantic shipping trade, but the company also operated services to the Mediterranean.', \"[14] Title: Manchester Liners -  All of the line's vessels were registered in the Port of Manchester, and many were lost to enemy action during the First and Second World Wars.\", '[15] Title: Dobson &amp; Barlow - Dobson and Barlow were manufacturers of textile machinery with works in Bolton, Greater Manchester.', '[16] Title: Dobson &amp; Barlow -  Isaac Dobson (1767-1833) founded the company in 1790 and by 1850 Dobson in partnership with Peter Rothwell had premises in Blackhorse Street which produced mules for cotton spinning.', '[17] Title: Dobson &amp; Barlow -  The company moved to a larger factory in Kay Street which had 1,600 workers in 1860.', '[18] Title: The Flux Foundation - The Flux Foundation is a non-profit group based in the San Francisco Bay Area whose main objective is to build community through the creation of large-scale public art.', '[19] Title: The Flux Foundation -  The group creates both public art and public artists.', '[20] Title: The Flux Foundation -  It was founded on April 1, 2010 and was established as a California corporation on January 6, 2011, by Rebecca Anders, Jessica Hobbs, Peter (PK) Kimelman, Catherine Magee and Colinne Hemrich.', '[21] Title: The Flux Foundation -  As of 2016 the Board of Directors consists of Kimelman, Hobbs, Magee, Paul Belger and Thwen Chaloemtiarana.', '[22] Title: The Flux Foundation -  It is a \"public charity\" 501c(3) non-profit, supported by grants, public donations and the display of its artworks.', '[23] Title: The Flux Foundation -  Its works are notable not only for their scale but interactivity with the audience relying on participation to create atmospheric effects.', '[24] Title: The Flux Foundation -  The group draws upon Situationist and Fluxus ideas of creating spectacle to establish social connections as an effect of the artwork.', '[25] Title: The Flux Foundation -  This \"community creation\" is mirrored in the pieces\\' creation by a large-number of volunteers who themselves create new social networks.', '[26] Title: The Flux Foundation -  The Foundation also provides mentorship and fiscal sponsorship to other large-scale artists.', '[27] Title: The Flux Foundation -  Flux is administratively based in San Francisco, while its studios are located at American Steel Studios in West Oakland, California.', '[28] Title: Manchester Sport and Leisure Trust - Manchester Sport and Leisure Trust is a non-profit organisation which manages sport and leisure venues in the City of Manchester, United Kingdom.', '[29] Title: Manchester Sport and Leisure Trust -  MSLT was founded in 1997 and is a company limited by guarantee with charitable status with a turnover of \u00a312.5m. MSLT is based at the Sportcity site.', '[30] Title: ITV Granada - ITV Granada (formerly Granada Television; informally Granada) is the Channel 3 regional service for North West England.', '[31] Title: ITV Granada -  The licence for the region has been held by ITV Broadcasting Limited since November 2008.', '[32] Title: ITV Granada -  It is the largest independent television-franchise producing company in the UK, accounting for 25% of the total broadcasting output of the ITV network.', '[33] Title: ITV Granada -  It had been held by Granada Television, which was founded by Sidney Bernstein and based at Granada Studios on Quay Street in Manchester since its inception.', '[34] Title: ITV Granada -  This was the only surviving company of the original four Independent Television Authority franchisees from 1954; Granada Media Group (parent company of Granada Television) merged with Carlton Communications to form ITV plc in 2004.', '[35] Title: ITV Granada -  It covers Cheshire, Greater Manchester, Lancashire, Merseyside, northwestern Derbyshire, part of Cumbria and North Yorkshire.', '[36] Title: ITV Granada -  On 15 July 2009, the Isle of Man was transferred to ITV Granada from ITV Border (even though the Isle of Man is a British Crown Dependency and is not part of the United Kingdom).', '[37] Title: Peter Hesketh-Fleetwood - Sir Peter Hesketh-Fleetwood, 1st Baronet, (9 May 1801\\xa0\u2013 12 April 1866) was an English landowner, developer and Member of Parliament, who founded the town of Fleetwood, in Lancashire, England.', '[38] Title: Peter Hesketh-Fleetwood -  Born Peter Hesketh, he changed his name by Royal assent to Hesketh-Fleetwood, incorporating the name of his ancestors, and was later created Baronet Fleetwood.', '[39] Title: Peter Hesketh-Fleetwood -  Predeceased by an older brother, he inherited estates in west Lancashire in 1824.', '[40] Title: Peter Hesketh-Fleetwood -  Inspired by the transport developments of the early 19th century, he decided to bring the railway to the Lancashire coast and develop a holiday resort and port.', '[41] Title: Peter Hesketh-Fleetwood -  He hired architect Decimus Burton to design his new town, which he named Fleetwood; construction began in 1836.', '[42] Title: Peter Hesketh-Fleetwood -  Hesketh-Fleetwood was instrumental in the formation of the Preston and Wyre Railway Company and with his financial support, a railway line was built between Preston and Fleetwood which opened in 1840.']}\n",
      "output: {'answer': 'Failsworth', 'reasoning': ['From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell', 'From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England', 'Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth', 'Therefore, the company Peter Hobbs founded is based in Failsworth'], 'evidence': [7, 8]}\n",
      "\n",
      "\u2705 Prompt template building code updated and executed.\n"
     ]
    }
   ],
   "source": [
    "# Data processing functions with curriculum learning\n",
    "from typing import List, Dict\n",
    "from pydantic import BaseModel, Field, ValidationError, validator\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "\n",
    "\n",
    "# Define instruction and load CoT exemplars for RAG prompting\n",
    "instruction = \"\"\"Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
    " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
    "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
    "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
    "    \"answer\": \"Failsworth\",\n",
    "    \"reasoning\": [\n",
    "      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
    "      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
    "      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
    "      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
    "    ],\n",
    "    \"evidence\": [\n",
    "      7,\n",
    "      8\n",
    "    ]\n",
    "  }\n",
    "    Please give the direct answer for this case, for answer you dont need to show reasoning, reasoning goes to field \"reasoning\".\n",
    "\"\"\"\n",
    "\n",
    "# Load the saved cot exemplar in json format\n",
    "cot_exemplar_file = \"chain_of_thought_prompt.json\"\n",
    "loaded_cot_exemplars = []\n",
    "try:\n",
    "    with open(cot_exemplar_file, 'r') as f:\n",
    "        loaded_cot_exemplars = json.load(f)\n",
    "    print(f\"\u2705 Successfully loaded {len(loaded_cot_exemplars)} CoT exemplars from '{cot_exemplar_file}'\")\n",
    "    print(f\"loaded cot exemplars: \", loaded_cot_exemplars)\n",
    "    # Demonstrate the structure of a single exemplar\n",
    "    if loaded_cot_exemplars:\n",
    "        demonstrate_example = loaded_cot_exemplars[0]\n",
    "        print(\"\\nStructure of a single exemplar:\")\n",
    "        for key, value in demonstrate_example.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\u274c Error: CoT exemplar file '{cot_exemplar_file}' not found. Please run the cell to save it first.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\u274c Error decoding JSON from '{cot_exemplar_file}': {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c An unexpected error occurred while loading '{cot_exemplar_file}': {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to format a single CoT exemplar into a string for the prompt\n",
    "def format_cot_exemplar_for_prompt(exemplar_data: Dict) -> str:\n",
    "    \"\"\"Formats a single loaded JSON exemplar into a string for the prompt.\"\"\"\n",
    "    # This structure should match the desired display within the prompt\n",
    "    # Example based on the JSON structure:\n",
    "    input_data = exemplar_data.get(\"input\", {})\n",
    "\n",
    "    # Use QAInput model for strict validation of the input structure\n",
    "    try:\n",
    "        validated_input = QAInput(**input_data)\n",
    "        # print(f\"\ud83d\udc1b DEBUG: Input validated successfully with QAInput.\") # Keep debug output minimal\n",
    "    except ValidationError as e:\n",
    "        print(f\"\u274c Input validation failed for exemplar: {e}\")\n",
    "        # Handle validation error - perhaps skip this exemplar or log a warning\n",
    "        # For now, we'll proceed with the raw data but log the failure\n",
    "        validated_input = input_data # Use raw data if validation fails\n",
    "\n",
    "\n",
    "    # Access validated data or raw data if validation failed\n",
    "    # Format the data into a string for the prompt\n",
    "    # Ensure contexts is a list before joining\n",
    "    contexts_list = getattr(validated_input, 'Contexts', input_data.get('Contexts', []))\n",
    "    if not isinstance(contexts_list, list):\n",
    "        contexts_list = [] # Ensure it's a list if validation failed or data is malformed\n",
    "\n",
    "    formatted_input = f\"Question: {getattr(validated_input, 'Question', input_data.get('Question', ''))}\\nContexts: {'\\n'.join(contexts_list)}\"\n",
    "\n",
    "\n",
    "    output_data = exemplar_data.get(\"output\", {})\n",
    "    # Note: We are NOT validating output here, only formatting it for the prompt string\n",
    "    formatted_output_reasoning = \"\\n\".join(output_data.get(\"reasoning\", []))\n",
    "    formatted_output_answer = output_data.get(\"answer\", \"insufficient information\")\n",
    "    # Note: We are NOT formatting evidence here for the prompt string as it's part of the output JSON later\n",
    "\n",
    "\n",
    "    # Construct the example in a way the model can follow, mirroring the intended CoT format\n",
    "    # The prompt format itself will NOT be a JSON object, but a string that contains structured examples\n",
    "    return f\"\"\"\n",
    "[Exemplar]\n",
    "Instruction: {exemplar_data.get(\"instruction\", \"\").strip()}\n",
    "Input: {formatted_input.strip()}\n",
    "Output:\n",
    "Reasoning:\n",
    "{formatted_output_reasoning.strip()}\n",
    "Answer: {formatted_output_answer.strip()}\n",
    "[/Exemplar]\n",
    "\"\"\"\n",
    "\n",
    "# Function to create the main prompt template\n",
    "def create_prompt_template(question: str, passages: List[Dict], building_prompts: Dict, include_answer: bool = True, answer: str = \"\") -> str:\n",
    "  \"\"\"Create standardized prompt template for HotpotQA multihop reasoning\n",
    "  For now we do not consider batching.\n",
    "  Adheres to Mistral-7B-Instruct-v0.2 format: <s>[INST] Instruction [/INST] Model response\n",
    "  Includes optional Chain-of-Thought exemplar after the main instruction.\n",
    "  \"\"\"\n",
    "\n",
    "  # Format evidence section\n",
    "  evidence_lines = []\n",
    "  for i, passage in enumerate(passages, 1):\n",
    "    title = passage.get('title', f'Passage {i}')\n",
    "    text = passage.get('text', passage.get('passage', ''))\n",
    "    evidence_lines.append(f\"[{i}] Title: {title} - {text}\") # Include Title in evidence format\n",
    "  evidence_text = \"\\n\".join(evidence_lines)\n",
    "\n",
    "  # Get the formatted CoT exemplars and main instruction\n",
    "  main_instruction = building_prompts.get('instruction', '').strip()\n",
    "  cot_exemplar_string = building_prompts.get('cot_exemplar', '').strip()\n",
    "\n",
    "  # Build the instruction part for the model\n",
    "  # Include CoT exemplar *after* the main instruction and before Q&A\n",
    "  instruction_text = f\"{cot_exemplar_string}\"\n",
    "\n",
    "\n",
    "  # Build the full prompt with Mistral-Instruct format\n",
    "  prompt = f\"{instruction_text.strip()}<s>[INST]  \\n\\n{main_instruction}\\n\\n Now lets keep previous exemplar and instruction in mind but fully focused on solving following question by deducing from the evidences given to you only. [Question]: {question}\\n[Evidence]: {evidence_text} [/INST]\"\n",
    "\n",
    "  # Append the expected output format for training\n",
    "  prompt += \"\\nOutput:\" # Add \"Output:\" header before the answer part\n",
    "\n",
    "  if include_answer:\n",
    "    prompt += f\"\\n{answer}</s>\" # Append the answer for training, on a new line\n",
    "\n",
    "  return prompt\n",
    "\n",
    "# Combine loaded exemplars into a single string for the prompt\n",
    "# This string will be passed as building_prompts['cot_exemplar']\n",
    "if loaded_cot_exemplars:\n",
    "    cot_exemplar_string_for_prompt = \"\\n\\n\".join([format_cot_exemplar_for_prompt(ex) for ex in loaded_cot_exemplars])\n",
    "else:\n",
    "    cot_exemplar_string_for_prompt = \"\"\n",
    "\n",
    "# Create the building_prompts dictionary to pass to create_prompt_template\n",
    "cot_exemplar_string_for_prompt = ''\n",
    "building_prompts_rag = {'instruction': instruction, 'cot_exemplar': cot_exemplar_string_for_prompt}\n",
    "\n",
    "print(\"\\n\u2705 Prompt template building code updated and executed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c304f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1760019541079,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "053c304f",
    "outputId": "836461ee-f803-4f0d-925b-66c167776056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of instruction: 1363\n",
      "Length of CoT exemplars string: 0\n",
      "Total length of prompt template (instruction + exemplars): 1363\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of the combined instruction and CoT exemplars\n",
    "if 'building_prompts_rag' in globals():\n",
    "    instruction_length = len(building_prompts_rag.get('instruction', ''))\n",
    "    cot_exemplar_length = len(building_prompts_rag.get('cot_exemplar', ''))\n",
    "    total_prompt_template_length = instruction_length + cot_exemplar_length\n",
    "    print(f\"Length of instruction: {instruction_length}\")\n",
    "    print(f\"Length of CoT exemplars string: {cot_exemplar_length}\")\n",
    "    print(f\"Total length of prompt template (instruction + exemplars): {total_prompt_template_length}\")\n",
    "else:\n",
    "    print(\"building_prompts_rag not found. Please run the relevant cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4mtomZT-7Fu",
   "metadata": {
    "id": "j4mtomZT-7Fu"
   },
   "source": [
    "##Load tokenizer eval func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n_AkfLuR_Btx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "a4e6e88de34749c7a10da51e7ac7057e",
      "b14d4c10af1f436da42fedd91676b943",
      "281ddd4688ad454e8d2cf7fa118406d0",
      "cb32a67aea674945b107ec8c114bbe76",
      "a03f4788e6af4e0dab5b02361ee00655",
      "85421197a0f7408a899cfb296a39ed8b",
      "7720c5396332436f85f593241e1955a2",
      "083383565c284f7bb71f045efa337cd4",
      "79dbe78107474dc28bc60a3fa5fea04e",
      "8b2d0a2daf394531b8506f7080bf719e",
      "6f36eeeb291846a7b6479382f11aa0fa",
      "413160e06dfc47598c79d7871bf552ea",
      "bbd58975e5774330802e46950eb59a9e",
      "26dd87a3420640ec8b0ea4340949b36f",
      "b9b03cd8015d472f92928b29714581a7",
      "505b22c0710845e4a5627454cdd2c0a0",
      "aa4c0504edf44137ad2c93949741a03d",
      "22f01198bd9a4a78b56e8c48b978ba37",
      "47548c8559464ae1aeec4836f9a84def",
      "cdf35d781fd446058421b2c19000df03",
      "0e3c62526f7740b1af7e45bc8c0c72fc",
      "2e110498596345228afc933e0692b9c3",
      "07b9e0c1fd5b4169b342e6a05766caf2",
      "40bea4c4d16f486ebc02e1f2fe08648f",
      "81c2a555d761422bbcd5ec905cd4a955",
      "c5af8b913c1d4f20835bb6b247d95374",
      "8fa7ee8a8f744804bdda4711d866c998",
      "164af62aa685453bbb95511f65b6842b",
      "4cd721c36321453da4ddcf8c688720e1",
      "7d209fda5a6446499539d1d7203f5faf",
      "3119963fe9d14d249896d0f705b91f01",
      "36eb02379ec9430d83eb522601d99a7f",
      "b8b76a3d07004320b81747587e2a4f85",
      "6ccb751588da46569f9322368b5eb3c4",
      "71f19f72f2f549e8b61d80a3935fffe5",
      "7e25296404824c128702f2d24100b242",
      "f9dd8777920b477282429d6ce9bd4429",
      "3b48c0cf2c2f4a5b81af800b8d20232b",
      "560bb55736aa424c936bd8c68605bc22",
      "702b5111780f4d7cbc0721f0377f868a",
      "55db34967bd841ef93d14add79018799",
      "b2a5165e0f2e4a238d051a3a40bd99b2",
      "82fb7e5fd4d14b5a86e361ad1c04d2f7",
      "7e46a4c0ee0b4367a188e4036772595c"
     ]
    },
    "executionInfo": {
     "elapsed": 2250,
     "status": "ok",
     "timestamp": 1760019543330,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "n_AkfLuR_Btx",
    "outputId": "656c71a3-d5c4-4cb9-befc-99626ba0c92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e6e88de34749c7a10da51e7ac7057e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413160e06dfc47598c79d7871bf552ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b9e0c1fd5b4169b342e6a05766caf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccb751588da46569f9322368b5eb3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "CACHE_DIR = \"/workspace/models\" if os.path.exists(\"/workspace\") else \"./models\"\n",
    "print(\"\ud83d\udd04 Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q0B5x0vHPsXc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1760019543385,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "q0B5x0vHPsXc",
    "outputId": "a3bfc7f0-84f1-4bde-cdbd-78fc456b85cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: instruction \n",
      " value: \n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
      "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
      "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
      "      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
      "      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
      "      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "      7,\n",
      "      8\n",
      "    ]\n",
      "  }\n",
      "    Please give the direct answer for this case, for answer you dont need to show reasoning, reasoning goes to field \"reasoning\".\n",
      "\n",
      "key: cot_exemplar \n",
      " value: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in building_prompts_rag.items():\n",
    "  print(f\"key: {k} \\n value: \\n{v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7B3bsSVz--6_",
   "metadata": {
    "id": "7B3bsSVz--6_"
   },
   "source": [
    "## Training data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhwHI_3P-2QG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4053,
     "status": "ok",
     "timestamp": 1760019547441,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "xhwHI_3P-2QG",
    "outputId": "46c133fb-b71a-4923-e34f-7b47920d15cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Processing HotpotQA data for training with EXTRACTIVE REASONING...\n",
      "i == 1 DEBUG (Structured Output with Extractive Reasoning)\n",
      "question Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?\n",
      "selected_passages (subset):\n",
      "  [1] North Haven, Maine: North Haven is a town in Knox County, Maine, United States, in Penobscot Bay.  The town is both a ye...\n",
      "  [2] Vinalhaven, Maine: Vinalhaven is a town located on the larger of the two Fox Islands in Knox County, Maine, United Stat...\n",
      "  [3] Lea County Regional Airport: Lea County Regional Airport (IATA: HOB,\u00a0ICAO: KHOB) (Lea County-Hobbs Airport) is four miles (6.4\u00a0km...\n",
      "  [4] Sacramento International Airport: Sacramento International Airport (IATA: SMF, ICAO: KSMF, FAA LID: SMF) is 10 mi northwest of downtow...\n",
      "  [5] Downeast Flight 46: Downeast Airlines Flight 46 was a scheduled airline service in the United States from Boston's Logan...\n",
      "  ...and 3 more passages\n",
      "predicted_output (JSON):\n",
      " {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox County Regional Airport is in Maine Therefore, Knox County Regional Airport is the airport located in Maine\",\n",
      "  \"answer\": \"Knox County Regional Airport\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "input_text (first 400 chars):\n",
      " <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "target_text (JSON string):\n",
      " {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox Count...\n",
      "full_text (first 800 chars):\n",
      " <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
      "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
      "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]...\n",
      "i == 1 DEBUG (Structured Output with Extractive Reasoning)\n",
      "question Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?\n",
      "selected_passages (subset):\n",
      "  [1] North Haven, Maine: North Haven is a town in Knox County, Maine, United States, in Penobscot Bay.  The town is both a ye...\n",
      "  [2] Matinicus Isle, Maine: Matinicus Isle is an island plantation in Knox County, Maine, United States.  The island is located ...\n",
      "  [3] Knox County Regional Airport: Knox County Regional Airport (IATA: RKD, ICAO: KRKD, FAA LID: RKD) is a county owned, public use air...\n",
      "  [4] Vinalhaven, Maine: Vinalhaven is a town located on the larger of the two Fox Islands in Knox County, Maine, United Stat...\n",
      "  [5] Owls Head, Maine: Owls Head is a town in Knox County, Maine, United States.  The population was 1,580 at the 2010 cens...\n",
      "  ...and 3 more passages\n",
      "predicted_output (JSON):\n",
      " {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox County Regional Airport is in Maine Therefore, Knox County Regional Airport is the airport located in Maine\",\n",
      "  \"answer\": \"Knox County Regional Airport\",\n",
      "  \"citations\": [\n",
      "    3,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "input_text (first 400 chars):\n",
      " <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "target_text (JSON string):\n",
      " {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox Count...\n",
      "full_text (first 800 chars):\n",
      " <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
      "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
      "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]...\n",
      "i == 1 DEBUG (Structured Output with Extractive Reasoning)\n",
      "question What nationality was Oliver Reed's character in the film Royal Flash?\n",
      "selected_passages (subset):\n",
      "  [1] Royal Flash (film): Royal Flash is a 1975 film based on George MacDonald Fraser's second Flashman novel, \"Royal Flash\". ...\n",
      "  [2] Lion of the Desert: Lion of the Desert is a 1981 Libyan historical action film starring Anthony Quinn as Libyan tribal l...\n",
      "  [3] Ivan Dragomiloff: Ivan Dragomiloff is a fictional character, the chairman of \"The Assassination Bureau, Ltd\" in the bo...\n",
      "  [4] The Duke of Hamilton: The Duke of Hamilton was one of the oldest pubs in London, situated in Hampstead.  It was a popular ...\n",
      "  [5] Otto von Bismarck: Otto Eduard Leopold, Prince of Bismarck, Duke of Lauenburg (1 April 1815 \u2013 30 July 1898), known as O...\n",
      "  ...and 3 more passages\n",
      "predicted_output (JSON):\n",
      " {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox County Regional Airport is in Maine Therefore, Knox County Regional Airport is the airport located in Maine\",\n",
      "  \"answer\": \"Prussian\",\n",
      "  \"citations\": [\n",
      "    1,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "input_text (first 400 chars):\n",
      " <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "target_text (JSON string):\n",
      " {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox Count...\n",
      "full_text (first 800 chars):\n",
      " <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
      "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
      "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]...\n",
      "\u2705 Data processed successfully with EXTRACTIVE REASONING:\n",
      "   Curriculum training: 2000 examples\n",
      "   Realistic training: 2000 examples\n",
      "   Evaluation: 400 examples\n",
      "\n",
      "\ud83d\udcdd Sample training example (with Extractive Reasoning):\n",
      "Question: Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?\n",
      "Answer (JSON string): {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox County Regional Airport is in Maine Therefore, Knox County Regional Airport is the airport located in Maine\",\n",
      "  \"answer\": \"Knox County Regional Airport\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "Has gold context: True\n",
      "\n",
      "\ud83d\udccb Input text (first 400 chars):\n",
      "<s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "\ud83d\udccb Full text (first 800 chars):\n",
      "<s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
      "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
      "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]...\n",
      "\n",
      "\u2705 All data processed and logged to W&B!\n"
     ]
    }
   ],
   "source": [
    "def process_hotpotqa_for_training(examples, building_prompts: Dict, curriculum_epoch: bool = True, generate_reasoning: bool = False):\n",
    "    \"\"\"\n",
    "    Process HotpotQA examples into training format with structured JSON output.\n",
    "    Uses extractive reasoning generation with embedded citations.\n",
    "    \"\"\"\n",
    "    processed_examples = []\n",
    "\n",
    "    i = 0\n",
    "    # Create a mapping from example ID to its index in the original dataset for exemplar handling\n",
    "    example_id_to_idx = {ex['id']: idx for idx, ex in enumerate(examples)}\n",
    "\n",
    "    for example in examples:\n",
    "        i += 1\n",
    "        question = example['question']\n",
    "        answer = example['answer']\n",
    "        context_data = example['context']\n",
    "        supporting_facts_data = example['supporting_facts']\n",
    "\n",
    "        # Create passage list with titles and text\n",
    "        passages = []\n",
    "        gold_passages = []\n",
    "\n",
    "        # STEP 1: Extract gold titles from supporting facts\n",
    "        gold_facts = set() # Use set of (title, sent_id) tuples\n",
    "        gold_titles = set()\n",
    "\n",
    "\n",
    "        try:\n",
    "            if isinstance(supporting_facts_data, dict):\n",
    "                # Dict structure: {'title': [...], 'sent_id': [...]}\n",
    "                if 'title' in supporting_facts_data and 'sent_id' in supporting_facts_data:\n",
    "                    for title, sent_id in zip(supporting_facts_data['title'], supporting_facts_data['sent_id']):\n",
    "                        gold_facts.add((title, sent_id))\n",
    "                        gold_titles.add(title)\n",
    "            # Removed handling for list structure as systematic investigation shows it's a dict\n",
    "        except Exception as e:\n",
    "            # Removed visualization print for this error\n",
    "            pass\n",
    "\n",
    "\n",
    "        # STEP 2: Process context to extract passages and map to original (title, sent_id)\n",
    "        # Also create a map from (title, sent_id) to its sentence text\n",
    "        context_map = {} # Map (title, sent_id) to sentence text\n",
    "        passage_list_flat = [] # List of strings: \"[idx] Title: ... - Sentence...\"\n",
    "        linear_index_counter = 1\n",
    "        passage_info_list = [] # List of {title: ..., text: ...}\n",
    "\n",
    "\n",
    "        try:\n",
    "            assert isinstance(context_data, dict)\n",
    "            # HuggingFace dict structure: {'title': [...], 'sentences': [...]}\n",
    "            if 'title' in context_data and 'sentences' in context_data:\n",
    "                titles = context_data['title']\n",
    "                sentences_lists = context_data['sentences']\n",
    "\n",
    "                for title, sentences in zip(titles, sentences_lists):\n",
    "                    if isinstance(sentences, list):\n",
    "                        full_passage_text = \" \".join(sentences)\n",
    "                        passage_info_list.append({\"title\": title, \"text\": full_passage_text}) # Store full passage text\n",
    "\n",
    "                        for sent_idx, sentence in enumerate(sentences):\n",
    "                            context_map[(title, sent_idx)] = sentence # Map fact to sentence text\n",
    "                            passage_list_flat.append(f\"[{linear_index_counter}] Title: {title} - {sentence}\") # Flattened for prompt\n",
    "                            linear_index_counter += 1\n",
    "                    else:\n",
    "                         # Handle cases where sentences is not a list (shouldn't happen based on investigation, but robustness)\n",
    "                         full_passage_text = str(sentences)\n",
    "                         passage_info_list.append({\"title\": title, \"text\": full_passage_text}) # Store full passage text\n",
    "                         context_map[(title, 0)] = full_passage_text # Map fact to sentence text\n",
    "                         passage_list_flat.append(f\"[{linear_index_counter}] Title: {title} - {full_passage_text}\") # Flattened for prompt\n",
    "                         linear_index_counter += 1\n",
    "\n",
    "            # Populate passages and gold_passages lists for selection\n",
    "            passages = passage_info_list\n",
    "            gold_passages = [p for p in passages if p['title'] in gold_titles] # Simple check by title for now\n",
    "\n",
    "        except Exception as e:\n",
    "            # Removed visualization print for this error\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Skip if we couldn't process any passages\n",
    "        if len(passages) == 0:\n",
    "            # Removed visualization print for this warning\n",
    "            continue\n",
    "\n",
    "        # STEP 3: Curriculum learning strategy\n",
    "        if curriculum_epoch and len(gold_passages) >= 2:\n",
    "            # Curriculum: Start with all gold passages + distractors up to 8\n",
    "            selected_passages = gold_passages.copy()\n",
    "            distractors = [p for p in passages if p not in gold_passages]\n",
    "            import random\n",
    "            random.shuffle(distractors)\n",
    "            # Ensure we don't exceed 8 total passages\n",
    "            selected_passages.extend(distractors[:max(0, 8 - len(selected_passages))])\n",
    "            # Shuffle the selected passages so gold ones aren't always first in the prompt\n",
    "            random.shuffle(selected_passages)\n",
    "\n",
    "        else:\n",
    "            # Standard: Random selection\n",
    "            import random\n",
    "            random.shuffle(passages)\n",
    "            selected_passages = passages[:8]\n",
    "\n",
    "            # Check if we have enough gold context in the randomly selected passages\n",
    "            selected_titles = set(p['title'] for p in selected_passages)\n",
    "            if len(selected_titles.intersection(gold_titles)) < 2 and answer != \"insufficient context\":\n",
    "                # If the gold answer exists but insufficient gold context is present in selected passages\n",
    "                # The target output should reflect insufficient context\n",
    "                answer = \"insufficient context\"\n",
    "\n",
    "\n",
    "        # Ensure selected_passages are present for prompt creation\n",
    "        if not selected_passages:\n",
    "             # If somehow no passages were selected, skip this example\n",
    "             continue\n",
    "\n",
    "        # STEP 4: Prepare structured output (answer, reasoning, citations) - UPDATED FOR EXTRACTIVE REASONING\n",
    "        predicted_output = {\"reasoning\": \"\", \"answer\": answer, \"citations\": []} # Initialize structured output\n",
    "\n",
    "        if answer != \"insufficient context\":\n",
    "            # Find indices of selected passages corresponding to gold facts\n",
    "            selected_passage_titles = [p['title'] for p in selected_passages]\n",
    "            selected_passage_texts = [p['text'] for p in selected_passages] # Store full text for matching\n",
    "\n",
    "            # Build citations list (indices in selected_passages, 1-based)\n",
    "            citation_indices = set()\n",
    "\n",
    "            # Map gold facts to indices in the *selected* passages\n",
    "            for gold_title, gold_sent_id in gold_facts:\n",
    "                 # Find index of the passage with this gold_title in selected_passages\n",
    "                 try:\n",
    "                     # Find all indices where the title matches\n",
    "                     matching_indices_in_selected = [idx for idx, p in enumerate(selected_passages, 1) if p['title'] == gold_title]\n",
    "\n",
    "                     if matching_indices_in_selected:\n",
    "                         # Add matching passage indices to citations\n",
    "                         citation_indices.update(matching_indices_in_selected)\n",
    "\n",
    "                 except ValueError:\n",
    "                     # Gold title not found in selected passages - shouldn't happen if curriculum=True and >=2 gold\n",
    "                     pass # Or log a warning\n",
    "\n",
    "            # Ensure unique and sorted citations (indices in selected_passages)\n",
    "            predicted_output[\"citations\"] = sorted(list(citation_indices))\n",
    "\n",
    "            # Build reasoning using extractive approach\n",
    "            original_idx = example_id_to_idx.get(example['id'])\n",
    "\n",
    "            # Access prepared data for exemplars\n",
    "            if 'prepared_train_sample_indexs' in globals() and 'prepared_reasoning_steps' in globals():\n",
    "                try:\n",
    "                    # Find the position of the current example's original index within the prepared indices\n",
    "                    exemplar_position = prepared_train_sample_indexs.index(original_idx)\n",
    "                    # If found, use the pre-defined reasoning (convert list to string if needed)\n",
    "                    exemplar_reasoning = prepared_reasoning_steps[exemplar_position]\n",
    "                    if isinstance(exemplar_reasoning, list):\n",
    "                        # Convert list of reasoning steps to natural paragraph\n",
    "                        predicted_output[\"reasoning\"] = \" \".join(exemplar_reasoning)\n",
    "                    else:\n",
    "                        predicted_output[\"reasoning\"] = exemplar_reasoning\n",
    "                except ValueError:\n",
    "                    # Not a prepared exemplar, generate extractive reasoning if requested\n",
    "                    if generate_reasoning and predicted_output[\"citations\"]:\n",
    "                        # Generate natural extractive reasoning with embedded citations\n",
    "                        predicted_output[\"reasoning\"] = generate_extractive_reasoning(\n",
    "                            question=question,\n",
    "                            answer=answer,\n",
    "                            selected_passages=selected_passages,\n",
    "                            evidence_indices=predicted_output[\"citations\"]\n",
    "                        )\n",
    "                    elif predicted_output[\"citations\"] and not generate_reasoning:\n",
    "                        # If citations exist but no reasoning generation requested, add placeholder\n",
    "                        citation_list = \", \".join([f\"[{idx}]\" for idx in predicted_output[\"citations\"]])\n",
    "                        predicted_output[\"reasoning\"] = f\"Relevant evidence found in passages {citation_list}.\"\n",
    "                    else:\n",
    "                        # If no citations, reasoning is empty\n",
    "                        predicted_output[\"reasoning\"] = \"\"\n",
    "            else:\n",
    "                 # If prepared data is not available, generate reasoning or use placeholder\n",
    "                 if generate_reasoning and predicted_output[\"citations\"]:\n",
    "                     predicted_output[\"reasoning\"] = generate_extractive_reasoning(\n",
    "                         question=question,\n",
    "                         answer=answer,\n",
    "                         selected_passages=selected_passages,\n",
    "                         evidence_indices=predicted_output[\"citations\"]\n",
    "                     )\n",
    "                 elif predicted_output[\"citations\"]:\n",
    "                     citation_list = \", \".join([f\"[{idx}]\" for idx in predicted_output[\"citations\"]])\n",
    "                     predicted_output[\"reasoning\"] = f\"Relevant evidence found in passages {citation_list}.\"\n",
    "                 else:\n",
    "                     predicted_output[\"reasoning\"] = \"\"\n",
    "\n",
    "\n",
    "        else:\n",
    "            # If answer is insufficient context, citations and reasoning should be empty\n",
    "            predicted_output[\"citations\"] = []\n",
    "            predicted_output[\"reasoning\"] = \"Based on the available evidence, I cannot determine a definitive answer to this question.\"\n",
    "\n",
    "\n",
    "        # STEP 5: Create training example with structured JSON output as target\n",
    "        # Serialize the output dictionary to a JSON string\n",
    "        try:\n",
    "            output_json_string = json.dumps(predicted_output, indent=2)\n",
    "            # Ensure the JSON string follows the desired format for the model output\n",
    "            # The model should output just the JSON object after [/INST]\\nOutput:\\n\n",
    "            target_text = output_json_string\n",
    "\n",
    "            # The full_text is the prompt + the target_text\n",
    "            # Corrected variable name from building_prompts to building_prompts_rag\n",
    "            prompt = create_prompt_template(question, selected_passages, building_prompts, include_answer=False) # Create prompt without the old answer format\n",
    "            full_text = prompt + \"\\n\" + target_text # Combine prompt and the new JSON target\n",
    "\n",
    "\n",
    "            if i == 1:\n",
    "              print('i == 1 DEBUG (Structured Output with Extractive Reasoning)')\n",
    "              print('question', question)\n",
    "              # Print only titles and first 100 chars of text for passages\n",
    "              print('selected_passages (subset):')\n",
    "              for idx, p in enumerate(selected_passages[:5]): # Print max 5 passages for brevity\n",
    "                  print(f\"  [{idx+1}] {p.get('title', 'N/A')}: {p.get('text', '')[:100]}...\")\n",
    "              if len(selected_passages) > 5:\n",
    "                   print(f\"  ...and {len(selected_passages)-5} more passages\")\n",
    "              print('predicted_output (JSON):\\n', json.dumps(predicted_output, indent=2))\n",
    "              print('input_text (first 400 chars):\\n', prompt[:400] + \"...\")\n",
    "              print('target_text (JSON string):\\n', target_text[:400] + \"...\")\n",
    "              print('full_text (first 800 chars):\\n', full_text[:800] + \"...\")\n",
    "\n",
    "\n",
    "            processed_examples.append({\n",
    "                \"question\": question,\n",
    "                \"passages\": selected_passages, # Keep passages for potential later use\n",
    "                \"answer\": target_text, # Store the JSON string as the 'answer' for consistency with old code expecting 'answer' in eval dataset\n",
    "                \"input_text\": prompt,\n",
    "                \"target_text\": target_text, # The JSON string is the target\n",
    "                \"full_text\": full_text, # Prompt + JSON string\n",
    "                \"has_gold_context\": len(gold_passages) >= 2 # Keep track of gold context availability\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error creating JSON output for example {i}: {e}\")\n",
    "            # Skip this example if JSON creation fails\n",
    "            continue\n",
    "\n",
    "\n",
    "    return Dataset.from_list(processed_examples)\n",
    "\n",
    "# Process training data with curriculum learning - USING NEW STRUCTURED OUTPUT WITH EXTRACTIVE REASONING\n",
    "print(\"\ud83d\udcca Processing HotpotQA data for training with EXTRACTIVE REASONING...\")\n",
    "\n",
    "# Ensure building_prompts_rag is defined before calling this function\n",
    "# Ensure prepared_train_sample_indexs and prepared_reasoning_steps are available\n",
    "# They are defined in cell 7agAtJS2Dyxk. Make sure that cell is run first.\n",
    "if 'building_prompts_rag' in globals() and 'prepared_train_sample_indexs' in globals() and 'prepared_reasoning_steps' in globals():\n",
    "  # Pass building_prompts_rag and enable reasoning generation for non-exemplars\n",
    "  train_dataset_curriculum = process_hotpotqa_for_training(train_sample, building_prompts_rag, curriculum_epoch=True, generate_reasoning=True)\n",
    "  train_dataset_realistic = process_hotpotqa_for_training(train_sample, building_prompts_rag, curriculum_epoch=False, generate_reasoning=True)\n",
    "\n",
    "  # Evaluation data (realistic setting) - also with structured output as target for evaluation logic\n",
    "  # The evaluation logic needs to be updated to parse this JSON output\n",
    "  eval_dataset = process_hotpotqa_for_training(val_sample, building_prompts_rag, curriculum_epoch=False, generate_reasoning=False) # No need to generate reasoning for eval targets\n",
    "\n",
    "  print(f\"\u2705 Data processed successfully with EXTRACTIVE REASONING:\")\n",
    "  print(f\"   Curriculum training: {len(train_dataset_curriculum)} examples\")\n",
    "  print(f\"   Realistic training: {len(train_dataset_realistic)} examples\")\n",
    "  print(f\"   Evaluation: {len(eval_dataset)} examples\")\n",
    "\n",
    "  # Show sample\n",
    "  if len(train_dataset_curriculum) > 0:\n",
    "      sample = train_dataset_curriculum[0]\n",
    "      print(f\"\\n\ud83d\udcdd Sample training example (with Extractive Reasoning):\")\n",
    "      print(f\"Question: {sample['question']}\")\n",
    "      print(f\"Answer (JSON string): {sample['answer']}\") # This is the JSON string\n",
    "      print(f\"Has gold context: {sample['has_gold_context']}\")\n",
    "      print(f\"\\n\ud83d\udccb Input text (first 400 chars):\")\n",
    "      print(sample['input_text'][:400] + \"...\")\n",
    "      print(f\"\\n\ud83d\udccb Full text (first 800 chars):\")\n",
    "      print(sample['full_text'][:800] + \"...\")\n",
    "\n",
    "  else:\n",
    "      print(\"\u26a0\ufe0f No examples processed successfully - investigate data structure further\")\n",
    "\n",
    "  # Log dataset statistics to W&B (only if we have data)\n",
    "  if len(train_dataset_curriculum) > 0 and 'wandb' in globals() and wandb.run:\n",
    "      wandb.log({\n",
    "          \"train_curriculum_size\": len(train_dataset_curriculum),\n",
    "          \"train_realistic_size\": len(train_dataset_realistic),\n",
    "          \"eval_size\": len(eval_dataset),\n",
    "          \"gold_context_rate_curriculum\": sum(ex['has_gold_context'] for ex in train_dataset_curriculum) / len(train_dataset_curriculum),\n",
    "          \"gold_context_rate_realistic\": sum(ex['has_gold_context'] for ex in train_dataset_realistic) / len(train_dataset_realistic)\n",
    "      })\n",
    "      print(f\"\\n\u2705 All data processed and logged to W&B!\")\n",
    "  elif len(train_dataset_curriculum) > 0:\n",
    "      print(f\"\\n\u26a0\ufe0f wandb not initialized. Dataset statistics not logged.\")\n",
    "  else:\n",
    "      print(f\"\\n\u274c No data processed - check the structure investigation output above\")\n",
    "\n",
    "else:\n",
    "  print(\"\u274c Required variables (building_prompts_rag, prepared_train_sample_indexs, prepared_reasoning_steps) are not defined. Please run the necessary cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KlIQcmWXch9v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1760019547660,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "KlIQcmWXch9v",
    "outputId": "89265f2c-2e1c-4b6a-a963-f25f7311dabe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "#compute the percentage of insufficient context in the training dataset\n",
    "print(type(train_dataset_curriculum))\n",
    "train_dataset = train_dataset_curriculum.to_list()\n",
    "print(len(train_dataset))\n",
    "num_samples = len(train_dataset)\n",
    "num_insufficient_context = sum(1 for ex in train_dataset if ex['has_gold_context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V5Q2x9im5Ub6",
   "metadata": {
    "id": "V5Q2x9im5Ub6"
   },
   "source": [
    "## Eval Function, Wandb training integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A3VmYOE6ajv2",
   "metadata": {
    "id": "A3VmYOE6ajv2"
   },
   "source": [
    "### Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dx94rbupa5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1760019547723,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "3dx94rbupa5",
    "outputId": "e22a50d6-4352-47b8-e565-cc6aa77414c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Comprehensive evaluation with ROBUST TENSOR HANDLING and UTILITY FUNCTIONS ready!\n",
      "\ud83d\udcca Features:\n",
      "   - Handles all tensor formats (logits, token IDs, numpy, lists)\n",
      "   - Detailed debugging output for tensor analysis\n",
      "   - Graceful error handling with full context\n",
      "   - HotpotQA-specific metrics (F1, EM, Citation Accuracy)\n",
      "   - Includes generate_answer and evaluate_model_on_dataset for flexible evaluation\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive HotpotQA Evaluator with Robust Tensor Handling and Utility Functions\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import os\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# Core ML libraries (should work on cloud platforms)\n",
    "\n",
    "from transformers import (\n",
    "\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "\n",
    "    TrainingArguments, Trainer, TrainerCallback, TrainerState\n",
    "\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "# Define necessary variables if not already defined (for standalone execution)\n",
    "\n",
    "if 'MAX_SEQ_LENGTH' not in globals():\n",
    "\n",
    "    MAX_SEQ_LENGTH = 1000 # Default value\n",
    "\n",
    "\n",
    "class HotpotQAEvaluator:\n",
    "\n",
    "    \"\"\"Comprehensive evaluator for HotpotQA multihop reasoning\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def normalize_answer(self, text):\n",
    "\n",
    "        \"\"\"Normalize answer text for comparison\"\"\"\n",
    "\n",
    "        import re\n",
    "\n",
    "        import string\n",
    "\n",
    "\n",
    "\n",
    "        # Convert to lowercase\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "\n",
    "\n",
    "        # Remove articles\n",
    "\n",
    "        text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "\n",
    "\n",
    "        # Remove punctuation\n",
    "\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "\n",
    "        # Remove extra whitespace\n",
    "\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "    def answer_f1_score(self, prediction, ground_truth):\n",
    "\n",
    "        \"\"\"Calculate F1 score between prediction and ground truth\"\"\"\n",
    "\n",
    "        from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "        pred_tokens = self.normalize_answer(prediction).split()\n",
    "\n",
    "        gold_tokens = self.normalize_answer(ground_truth).split()\n",
    "\n",
    "\n",
    "\n",
    "        if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "\n",
    "            return 1.0\n",
    "\n",
    "        if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "\n",
    "        common_tokens = Counter(pred_tokens) & Counter(gold_tokens)\n",
    "\n",
    "        num_same = sum(common_tokens.values())\n",
    "\n",
    "\n",
    "\n",
    "        if num_same == 0:\n",
    "\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "\n",
    "        precision = num_same / len(pred_tokens)\n",
    "\n",
    "        recall = num_same / len(gold_tokens)\n",
    "\n",
    "\n",
    "\n",
    "        return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "\n",
    "\n",
    "    def answer_exact_match(self, prediction, ground_truth):\n",
    "\n",
    "        \"\"\"Calculate exact match score\"\"\"\n",
    "\n",
    "        return float(self.normalize_answer(prediction) == self.normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize evaluator\n",
    "\n",
    "evaluator = HotpotQAEvaluator()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_predictions_to_token_ids(predictions):\n",
    "\n",
    "    \"\"\"Robust conversion of any prediction format to token IDs with detailed debugging\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\n\ud83d\udd0d TENSOR CONVERSION DEBUG:\")\n",
    "\n",
    "    print(f\"   Input type: {type(predictions)}\")\n",
    "\n",
    "    print(f\"   Input class: {predictions.__class__.__name__}\")\n",
    "\n",
    "\n",
    "\n",
    "    if hasattr(predictions, 'shape'):\n",
    "\n",
    "        print(f\"   Shape: {predictions.shape}\")\n",
    "\n",
    "    elif hasattr(predictions, '__len__'):\n",
    "\n",
    "        print(f\"   Length: {len(predictions)}\")\n",
    "\n",
    "\n",
    "\n",
    "    if hasattr(predictions, 'dtype'):\n",
    "\n",
    "        print(f\"   Dtype: {predictions.dtype}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Sample first few values for inspection\n",
    "\n",
    "    if isinstance(predictions, (list, tuple)):\n",
    "\n",
    "        print(f\"   First element type: {type(predictions[0])}\")\n",
    "\n",
    "        if hasattr(predictions[0], 'shape'):\n",
    "\n",
    "            print(f\"   First element shape: {predictions[0].shape}\")\n",
    "\n",
    "        elif hasattr(predictions[0], '__len__'):\n",
    "\n",
    "            print(f\"   First element length: {len(predictions[0])}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Show actual values (first few)\n",
    "\n",
    "        if hasattr(predictions[0], '__iter__') and not isinstance(predictions[0], str):\n",
    "\n",
    "            try:\n",
    "\n",
    "                sample_vals = list(predictions[0])[:3] if len(predictions[0]) > 0 else []\n",
    "\n",
    "                print(f\"   Sample values from first element: {sample_vals}\")\n",
    "\n",
    "            except:\n",
    "\n",
    "                print(f\"   Could not extract sample values\")\n",
    "\n",
    "\n",
    "\n",
    "    elif hasattr(predictions, 'flatten'):\n",
    "\n",
    "        try:\n",
    "\n",
    "            flat_sample = predictions.flatten()[:3].tolist()\n",
    "\n",
    "            print(f\"   Sample flattened values: {flat_sample}\")\n",
    "\n",
    "        except:\n",
    "\n",
    "            print(f\"   Could not flatten for sampling\")\n",
    "\n",
    "\n",
    "\n",
    "    # Now attempt conversion\n",
    "\n",
    "    print(f\"   \ud83d\udd27 Attempting conversion...\")\n",
    "\n",
    "\n",
    "\n",
    "    # Case 1: Already token IDs (integers)\n",
    "\n",
    "    if hasattr(predictions, 'dtype') and predictions.dtype in [torch.int32, torch.int64, torch.long]:\n",
    "\n",
    "        print(f\"   \u2705 Already token IDs (integers)\")\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "    # Case 2: Logits (floats) - need argmax\n",
    "\n",
    "    if hasattr(predictions, 'dtype') and predictions.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n",
    "\n",
    "        print(f\"   \ud83c\udfaf Converting logits (floats) using argmax\")\n",
    "\n",
    "        if len(predictions.shape) == 3:  # [batch, seq_len, vocab_size]\n",
    "\n",
    "            print(f\"   \ud83d\udcca 3D tensor [batch, seq_len, vocab_size] -> argmax on dim=-1\")\n",
    "\n",
    "            result = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "            print(f\"   \u2705 Converted to shape: {result.shape}\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        elif len(predictions.shape) == 2:  # Already [batch, seq_len]\n",
    "\n",
    "            print(f\"   \ud83d\udcca 2D tensor [batch, seq_len] -> converting to long\")\n",
    "\n",
    "            result = predictions.long()\n",
    "\n",
    "            print(f\"   \u2705 Converted to dtype: {result.dtype}\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"   \u26a0\ufe0f Unexpected tensor shape: {predictions.shape}\")\n",
    "\n",
    "            result = predictions.long()\n",
    "\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "    # Case 3: Numpy arrays\n",
    "\n",
    "    if isinstance(predictions, np.ndarray):\n",
    "\n",
    "        print(f\"   \ud83d\udd22 Converting numpy array\")\n",
    "\n",
    "        if predictions.dtype in [np.float16, np.float32, np.float64]:\n",
    "\n",
    "            print(f\"   \ud83c\udfaf Numpy float array\")\n",
    "\n",
    "            if len(predictions.shape) == 3:\n",
    "\n",
    "                print(f\"   \ud83d\udcca 3D numpy array -> argmax on axis=-1\")\n",
    "\n",
    "                result = torch.tensor(np.argmax(predictions, axis=-1))\n",
    "\n",
    "                print(f\"   \u2705 Converted to torch tensor shape: {result.shape}\")\n",
    "\n",
    "                return result\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"   \ud83d\udcca Converting numpy float to torch long\")\n",
    "\n",
    "                result = torch.tensor(predictions).long()\n",
    "\n",
    "                return result\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"   \ud83d\udcca Converting numpy int to torch long\")\n",
    "\n",
    "            result = torch.tensor(predictions).long()\n",
    "\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "    # Case 4: Nested lists\n",
    "\n",
    "    if isinstance(predictions, list):\n",
    "\n",
    "        print(f\"   \ud83d\udcdd Processing list input\")\n",
    "\n",
    "        if len(predictions) > 0:\n",
    "\n",
    "            if isinstance(predictions[0], list):\n",
    "\n",
    "                print(f\"   \ud83d\udcca Nested list structure\")\n",
    "\n",
    "                try:\n",
    "\n",
    "                    tensor = torch.tensor(predictions)\n",
    "\n",
    "                    print(f\"   \ud83d\udd04 Converted to tensor: {tensor.shape}, dtype: {tensor.dtype}\")\n",
    "\n",
    "                    if tensor.dtype in [torch.float16, torch.float32]:\n",
    "\n",
    "                        if len(tensor.shape) == 3:\n",
    "\n",
    "                            print(f\"   \ud83c\udfaf 3D float tensor -> argmax\")\n",
    "\n",
    "                            return torch.argmax(tensor, dim=-1)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            print(f\"   \ud83d\udd04 Converting float tensor to long\")\n",
    "\n",
    "                            return tensor.long()\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        print(f\"   \u2705 Already integer tensor\")\n",
    "\n",
    "                        return tensor.long()\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f\"   \u26a0\ufe0f Tensor conversion failed: {e}\")\n",
    "\n",
    "                    # Fallback: flatten\n",
    "\n",
    "                    print(f\"   \ud83d\udd04 Attempting flatten fallback\")\n",
    "\n",
    "                    flat = [item for sublist in predictions for item in sublist]\n",
    "\n",
    "                    result = torch.tensor(flat).long()\n",
    "\n",
    "                    print(f\"   \u2705 Flattened result shape: {result.shape}\")\n",
    "\n",
    "                    return result\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"   \ud83d\udcca Simple list -> tensor\")\n",
    "\n",
    "                result = torch.tensor(predictions).long()\n",
    "\n",
    "                print(f\"   \u2705 Converted shape: {result.shape}\")\n",
    "\n",
    "                return result\n",
    "\n",
    "\n",
    "\n",
    "    # Fallback: try to convert directly\n",
    "\n",
    "    print(f\"   \ud83c\udd98 Using fallback conversion\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        result = torch.tensor(predictions).long()\n",
    "\n",
    "        print(f\"   \u2705 Fallback successful: {result.shape}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"   \u274c Fallback failed: {e}\")\n",
    "\n",
    "        raise e\n",
    "\n",
    "\n",
    "\n",
    "### This function is used in inference demo or debugging model performance with quick question answering\n",
    "def generate_answer(question: str, passages: List[Dict], building_prompt:Dict, model_to_use, max_new_tokens: int = 1000) -> str:\n",
    "\n",
    "    \"\"\"Generate answer using specified model\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Create prompt\n",
    "\n",
    "    prompt = create_prompt_template(question, passages, building_prompt ,include_answer=False)\n",
    "\n",
    "    print(f\"Prompt length: {len(prompt)}\")\n",
    "\n",
    "    # Tokenize\n",
    "\n",
    "    inputs = tokenizer(\n",
    "\n",
    "        prompt,\n",
    "\n",
    "        return_tensors=\"pt\",\n",
    "\n",
    "        truncation=True,\n",
    "\n",
    "        max_length=MAX_SEQ_LENGTH - max_new_tokens\n",
    "\n",
    "    ).to(model_to_use.device)\n",
    "\n",
    "    print('the maximum sequence length is: ',MAX_SEQ_LENGTH)\n",
    "\n",
    "    # Generate\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model_to_use.generate(\n",
    "\n",
    "            **inputs,\n",
    "\n",
    "            max_new_tokens=max_new_tokens,\n",
    "\n",
    "            temperature=0.1,\n",
    "\n",
    "            do_sample=True,\n",
    "\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Decode response (only new tokens)\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Validate structure output of response using the dedicated function\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Create a list of context strings in the format expected by parse_and_validate_response\n",
    "\n",
    "        context_strings = [f\"[{i+1}] Title: {p.get('title', '')} - {p.get('text', '')}\" for i, p in enumerate(passages)]\n",
    "\n",
    "        print('the response type is: ', type(response))\n",
    "\n",
    "        validated_response = parse_and_validate_response(response, context_strings)\n",
    "\n",
    "        # If you need the raw string response for later steps, return that.\n",
    "\n",
    "        # If you need the validated object, return validated_response.\n",
    "\n",
    "        # For now, returning the original string response as the rest of the code expects it.\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"\u274c Response validation failed: {e}\")\n",
    "\n",
    "        # Handle validation failure - maybe return an error message or the raw response\n",
    "\n",
    "        # For now, just print the error and continue, returning the raw response.\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #print the length of the generated answer\n",
    "\n",
    "    print(f\"Generated answer length: {len(response)}\") # Use raw response length for consistency\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data collator for instruction tuning\n",
    "\n",
    "class HotpotQADataCollator:\n",
    "\n",
    "    \"\"\"Custom data collator for HotpotQA instruction tuning\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, tokenizer, max_length: int = 2048):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.visualization_count = 0 # Add visualization counter\n",
    "\n",
    "        self.max_visualization_prints = 3 # Limit prints\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, examples: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        # Extract full text (input + target)\n",
    "\n",
    "        texts = [ex['full_text'] for ex in examples]\n",
    "\n",
    "\n",
    "\n",
    "        # Tokenize\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "\n",
    "            texts,\n",
    "\n",
    "            truncation=True,\n",
    "\n",
    "            padding=True,\n",
    "\n",
    "            max_length=self.max_length,\n",
    "\n",
    "            return_tensors=\"pt\"\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Create labels (same as input_ids, but with -100 for padding)\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "\n",
    "\n",
    "        # Mask padding tokens in labels\n",
    "\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "\n",
    "\n",
    "        # For instruction tuning, mask the input part and only train on answer\n",
    "\n",
    "        for i, example in enumerate(examples):\n",
    "\n",
    "            input_text = example['input_text']\n",
    "\n",
    "            # Tokenize input_text separately to get its length in tokens\n",
    "\n",
    "            input_ids_input_text = self.tokenizer(input_text, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "            input_length = len(input_ids_input_text)\n",
    "\n",
    "\n",
    "\n",
    "            # Mask input tokens in labels (only train on answer)\n",
    "\n",
    "            if input_length < len(labels[i]):\n",
    "\n",
    "                labels[i][:input_length] = -100\n",
    "\n",
    "\n",
    "\n",
    "            # Visualization prints for the first few examples in the batch\n",
    "\n",
    "            if self.visualization_count < self.max_visualization_prints:\n",
    "\n",
    "                print(f\"\\n--- Example {self.visualization_count+1} (HotpotQADataCollator) ---\")\n",
    "\n",
    "                print(f\"  Full Text (first 400 chars): {example['full_text'][:400]}...\")\n",
    "\n",
    "                print(f\"  Input Text Length (tokens): {input_length}\")\n",
    "\n",
    "                print(f\"  Tokenized Input IDs (first 20): {batch['input_ids'][i][:20].tolist()}\")\n",
    "\n",
    "                print(f\"  Labels Before Masking Input (first 20): {batch['input_ids'][i][:20].tolist()}\") # Same as input_ids\n",
    "\n",
    "                print(f\"  Labels After Masking Input (first 20): {labels[i][:20].tolist()}\")\n",
    "\n",
    "                # Find first non -100 label to show where target starts\n",
    "\n",
    "                first_target_token_idx = (labels[i] != -100).nonzero(as_tuple=True)[0][0] if (labels[i] != -100).any() else -1\n",
    "\n",
    "                print(f\"  First Target Token Index in Labels: {first_target_token_idx}\")\n",
    "\n",
    "                # Show a snippet around the masking boundary\n",
    "\n",
    "                snippet_start = max(0, input_length - 5)\n",
    "\n",
    "                snippet_end = min(len(labels[i]), input_length + 5)\n",
    "\n",
    "                print(f\"  Labels around input_length {input_length} (indices {snippet_start}-{snippet_end-1}): {labels[i][snippet_start:snippet_end].tolist()}\")\n",
    "\n",
    "\n",
    "\n",
    "                self.visualization_count += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "# Create data collator\n",
    "\n",
    "data_collator = HotpotQADataCollator(tokenizer, max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\u2705 Comprehensive evaluation with ROBUST TENSOR HANDLING and UTILITY FUNCTIONS ready!\")\n",
    "\n",
    "print(\"\ud83d\udcca Features:\")\n",
    "\n",
    "print(\"   - Handles all tensor formats (logits, token IDs, numpy, lists)\")\n",
    "\n",
    "print(\"   - Detailed debugging output for tensor analysis\")\n",
    "\n",
    "print(\"   - Graceful error handling with full context\")\n",
    "\n",
    "print(\"   - HotpotQA-specific metrics (F1, EM, Citation Accuracy)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p399m1taayr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1760019547780,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "p399m1taayr",
    "outputId": "483e8883-f6d4-409b-b2de-8c0ff189744e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Unified evaluation function loaded successfully!\n",
      "\ud83d\udd27 FIXES APPLIED:\n",
      "   \u2022 Added example_idx parameter to extract_answer_and_citations\n",
      "   \u2022 Added flush=True to all debug prints for proper tqdm alignment\n",
      "   \u2022 Fixed 'insufficient context' scoring: EM=1.0, F1=1.0 when both match\n",
      "   \u2022 Display now shows extracted answer instead of raw response\n"
     ]
    }
   ],
   "source": [
    "# Unified Evaluation Function for Comprehensive Model Assessment\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Any\n",
    "import sys\n",
    "\n",
    "def evaluate_model_comprehensive(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    evaluator,\n",
    "    model_name: str = \"Model\",\n",
    "    max_examples: Optional[int] = None,\n",
    "    use_rag_prompting: bool = True,\n",
    "    verbose_level: str = \"summary\",  # \"all\", \"sample\", \"summary\"\n",
    "    wandb_prefix: Optional[str] = None,\n",
    "    building_prompts: Optional[Dict] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Unified evaluation function for both baseline and fine-tuned models.\n",
    "\n",
    "    Added example_idx to extract_answer_and_citations calls\n",
    "    Added separator prints before extraction for better debugging\n",
    "    Fixed \"insufficient context\" EM/F1 scoring\n",
    "    Display now shows extracted answer, not raw response\n",
    "\n",
    "    Args:\n",
    "        model: Model to evaluate (base or fine-tuned)\n",
    "        tokenizer: Tokenizer\n",
    "        eval_dataset: Dataset to evaluate on\n",
    "        evaluator: HotpotQAEvaluator instance\n",
    "        model_name: Name for logging\n",
    "        max_examples: Max examples to evaluate (None = all)\n",
    "        use_rag_prompting: If True, use RAG prompts; if False, use direct JSON format\n",
    "        verbose_level: \"all\" (print every example), \"sample\" (first 5), \"summary\" (final only)\n",
    "        wandb_prefix: Prefix for W&B metrics (e.g., \"baseline_rag\" or \"final_eval\")\n",
    "        building_prompts: Prompt template dict (required if use_rag_prompting=True)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with comprehensive metrics\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Select dataset subset if specified\n",
    "    if max_examples:\n",
    "        eval_subset = eval_dataset.select(range(min(max_examples, len(eval_dataset))))\n",
    "    else:\n",
    "        eval_subset = eval_dataset\n",
    "\n",
    "    # Metrics tracking\n",
    "    f1_scores = []\n",
    "    em_scores = []\n",
    "    citation_precisions = []\n",
    "    citation_recalls = []\n",
    "    citation_f1s = []\n",
    "\n",
    "    # Insufficient context tracking\n",
    "    insufficient_context_count = 0\n",
    "    insufficient_context_correct = 0\n",
    "    per_example_results = []\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\ud83d\udd0d Evaluating {model_name} on {len(eval_subset)} examples...\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    for idx, example in enumerate(tqdm(eval_subset, desc=f\"Evaluating {model_name}\")):\n",
    "        try:\n",
    "            question = example.get('question', '')\n",
    "            passages = example.get('passages', [])\n",
    "\n",
    "            # Create input based on prompting strategy\n",
    "            if use_rag_prompting:\n",
    "                if building_prompts is None:\n",
    "                    raise ValueError(\"building_prompts required when use_rag_prompting=True\")\n",
    "                input_text = create_prompt_template(question, passages, building_prompts, include_answer=False)\n",
    "            else:\n",
    "                # Use direct input_text from dataset (for fine-tuned model)\n",
    "                input_text = example.get('input_text', '')\n",
    "\n",
    "            # Generate prediction\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=300,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "            # FIXED: Print separator BEFORE extraction with flush\n",
    "            if verbose_level == \"all\" or (verbose_level == \"sample\" and idx < 5):\n",
    "                print(f\"\\n{'='*60}\", flush=True)\n",
    "                print(f\"--- Example {idx + 1} ---\", flush=True)\n",
    "                print(f\"Question: {question[:100]}...\", flush=True)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            # FIXED: Extract answer and citations from response with example_idx\n",
    "            pred_answer, pred_citations = extract_answer_and_citations(response, example_idx=idx)\n",
    "\n",
    "            # Parse ground truth\n",
    "            gt_text = example.get('answer', '{}')\n",
    "            gold_answer, gold_citations = extract_answer_and_citations(gt_text, example_idx=idx)\n",
    "\n",
    "            # Compute metrics\n",
    "            f1 = evaluator.answer_f1_score(pred_answer, gold_answer)\n",
    "            em = evaluator.answer_exact_match(pred_answer, gold_answer)\n",
    "\n",
    "            # FIXED: Special handling for \"insufficient context\" cases\n",
    "            # When both answers are \"insufficient context\", the F1 and EM should be 1.0\n",
    "            is_insufficient = gold_answer.lower().strip() == 'insufficient context'\n",
    "            pred_insufficient = pred_answer.lower().strip() == 'insufficient context'\n",
    "\n",
    "            if is_insufficient and pred_insufficient:\n",
    "                # Both correctly identified as insufficient context\n",
    "                # This is a PERFECT match, so override the scores\n",
    "                f1 = 1.0\n",
    "                em = 1.0\n",
    "\n",
    "            # Citation metrics\n",
    "            if gold_citations:\n",
    "                pred_set = set(pred_citations)\n",
    "                gold_set = set(gold_citations)\n",
    "\n",
    "                if pred_set:\n",
    "                    citation_precision = len(pred_set & gold_set) / len(pred_set)\n",
    "                else:\n",
    "                    citation_precision = 0.0\n",
    "\n",
    "                citation_recall = len(pred_set & gold_set) / len(gold_set)\n",
    "\n",
    "                if citation_precision + citation_recall > 0:\n",
    "                    citation_f1 = 2 * citation_precision * citation_recall / (citation_precision + citation_recall)\n",
    "                else:\n",
    "                    citation_f1 = 0.0\n",
    "            else:\n",
    "                # No gold citations (insufficient context case)\n",
    "                # If model also predicts no citations, that's perfect (1.0)\n",
    "                # If model predicts citations when there shouldn't be any, that's wrong (0.0)\n",
    "                citation_precision = 1.0 if not pred_citations else 0.0\n",
    "                citation_recall = 1.0\n",
    "                citation_f1 = 1.0 if not pred_citations else 0.0\n",
    "\n",
    "            # Insufficient context tracking\n",
    "            if is_insufficient:\n",
    "                insufficient_context_count += 1\n",
    "                if pred_insufficient:\n",
    "                    insufficient_context_correct += 1\n",
    "\n",
    "            # Store results\n",
    "            f1_scores.append(f1)\n",
    "            em_scores.append(em)\n",
    "            citation_precisions.append(citation_precision)\n",
    "            citation_recalls.append(citation_recall)\n",
    "            citation_f1s.append(citation_f1)\n",
    "\n",
    "            per_example_results.append({\n",
    "                'question': question,\n",
    "                'predicted_answer': pred_answer,\n",
    "                'gold_answer': gold_answer,\n",
    "                'predicted_citations': pred_citations,\n",
    "                'gold_citations': gold_citations,\n",
    "                'f1': f1,\n",
    "                'em': em,\n",
    "                'citation_precision': citation_precision,\n",
    "                'citation_recall': citation_recall,\n",
    "                'citation_f1': citation_f1\n",
    "            })\n",
    "\n",
    "            # FIXED: Verbose output with extracted answers (not raw response)\n",
    "            if verbose_level == \"all\" or (verbose_level == \"sample\" and idx < 5):\n",
    "                print(f\"Predicted: {pred_answer}\", flush=True)\n",
    "                print(f\"Gold: {gold_answer}\", flush=True)\n",
    "                print(f\"Pred Citations: {pred_citations}\", flush=True)\n",
    "                print(f\"Gold Citations: {gold_citations}\", flush=True)\n",
    "                print(f\"F1: {f1:.3f}, EM: {em:.3f}, Citation F1: {citation_f1:.3f}\", flush=True)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n\u26a0\ufe0f  Error on example {idx}: {str(e)}\", flush=True)\n",
    "            continue\n",
    "\n",
    "    # Compute final metrics\n",
    "    results = {\n",
    "        'em': np.mean(em_scores) if em_scores else 0.0,\n",
    "        'f1': np.mean(f1_scores) if f1_scores else 0.0,\n",
    "        'citation_precision': np.mean(citation_precisions) if citation_precisions else 0.0,\n",
    "        'citation_recall': np.mean(citation_recalls) if citation_recalls else 0.0,\n",
    "        'citation_f1': np.mean(citation_f1s) if citation_f1s else 0.0,\n",
    "        'insufficient_context_rate': insufficient_context_correct / insufficient_context_count if insufficient_context_count > 0 else 0.0,\n",
    "        'insufficient_context_total': insufficient_context_count,\n",
    "        'insufficient_context_correct': insufficient_context_correct,\n",
    "        'total_examples': len(per_example_results),\n",
    "        'per_example_results': per_example_results\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\ud83d\udcca {model_name.upper()} - EVALUATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Examples: {results['total_examples']}\")\n",
    "    print(f\"Exact Match (EM): {results['em']:.3f}\")\n",
    "    print(f\"F1 Score: {results['f1']:.3f}\")\n",
    "    print(f\"Citation Precision: {results['citation_precision']:.3f}\")\n",
    "    print(f\"Citation Recall: {results['citation_recall']:.3f}\")\n",
    "    print(f\"Citation F1: {results['citation_f1']:.3f}\")\n",
    "    print(f\"Insufficient Context Detection: {results['insufficient_context_rate']:.1%} ({results['insufficient_context_correct']}/{results['insufficient_context_total']})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Log to W&B\n",
    "    if wandb_prefix and wandb.run:\n",
    "        wandb.log({\n",
    "            f\"{wandb_prefix}_em\": results['em'],\n",
    "            f\"{wandb_prefix}_f1\": results['f1'],\n",
    "            f\"{wandb_prefix}_citation_precision\": results['citation_precision'],\n",
    "            f\"{wandb_prefix}_citation_recall\": results['citation_recall'],\n",
    "            f\"{wandb_prefix}_citation_f1\": results['citation_f1'],\n",
    "            f\"{wandb_prefix}_insufficient_context_rate\": results['insufficient_context_rate'],\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"\u2705 Unified evaluation function loaded successfully!\")\n",
    "print(\"\ud83d\udd27 FIXES APPLIED:\")\n",
    "print(\"   \u2022 Added example_idx parameter to extract_answer_and_citations\")\n",
    "print(\"   \u2022 Added flush=True to all debug prints for proper tqdm alignment\")\n",
    "print(\"   \u2022 Fixed 'insufficient context' scoring: EM=1.0, F1=1.0 when both match\")\n",
    "print(\"   \u2022 Display now shows extracted answer instead of raw response\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6vrr8tsj5xj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1760019547828,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "6vrr8tsj5xj",
    "outputId": "bd25adf8-dd09-4bc5-da46-c150480904b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd27 Testing the REVISED extract_answer_and_citations()...\n",
      "\n",
      "--- Testing 'evidence' field (the actual issue!) ---\n",
      "\ud83d\udd0d Extracting from: {   \"answer\": \"Kurt Weill\",   \"reasoning\": [\"Step 1\", \"Step 2\"],   \"evidence\": [7, 1] }...\n",
      "\u2705 JSON parse OK: answer='Kurt Weill...', citations=[1, 7]\n",
      "   Answer: 'Kurt Weill'\n",
      "   Citations: [1, 7]\n",
      "\u2705 PASS\n",
      "\n",
      "--- Testing 'citations' field ---\n",
      "\ud83d\udd0d Extracting from: {\"answer\": \"Gimme Shelter\", \"citations\": [1, 7]}...\n",
      "\u2705 JSON parse OK: answer='Gimme Shelter...', citations=[1, 7]\n",
      "\u2705 PASS\n",
      "\n",
      "--- Testing answer as list ---\n",
      "\ud83d\udd0d Extracting from: {\"answer\": [\"Item 1\", \"Item 2\"], \"citations\": [6]}...\n",
      "\u2705 JSON parse OK: answer='Item 1, Item 2...', citations=[6]\n",
      "\u2705 PASS\n",
      "\n",
      "\u2705 REVISED parsing functions now handle both 'citations' and 'evidence' fields!\n",
      "\ud83d\udcdd Reduced debug verbosity for cleaner evaluation output\n",
      "\ud83d\udd27 Added flush=True to fix output buffering with tqdm progress bar\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd27 CRITICAL EVALUATION FIXES - Run this cell to fix all evaluation bugs!\n",
    "# This cell overrides the buggy functions in Cell 34 and Cell 22\n",
    "# FIXED: Now handles both \"citations\" and \"evidence\" fields\n",
    "# FIXED: Added flush=True to fix output buffering issues\n",
    "# FIXED: Added example_idx parameter for better debugging\n",
    "\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Dict\n",
    "\n",
    "def extract_answer_and_citations(generated_text: str, example_idx: int = None) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Extract answer and citations from generated text, prioritizing JSON parsing.\n",
    "\n",
    "    Handles malformed JSON, extra text after JSON, and attempts text fallback.\n",
    "    Also handles answer potentially being a list in the model output.\n",
    "    FIXED: Now checks both \"citations\" and \"evidence\" fields.\n",
    "    FIXED: Added example_idx for better debugging and flush=True for output alignment.\n",
    "\n",
    "    Args:\n",
    "        generated_text: Model output string\n",
    "        example_idx: Optional example index for debugging output\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (answer: str, citations: List[int])\n",
    "    \"\"\"\n",
    "    # Reduce verbosity - only show first 200 chars\n",
    "    debug_text = generated_text[:200].replace('\\n', ' ')\n",
    "    prefix = f\"[Ex {example_idx}] \" if example_idx is not None else \"\"\n",
    "    print(f\"{prefix}\ud83d\udd0d Extracting from: {debug_text}...\", flush=True)\n",
    "\n",
    "    try:\n",
    "        # Method 1: Try strict JSON parse first (best case)\n",
    "        parsed = json.loads(generated_text)\n",
    "        answer = parsed.get('answer', '')\n",
    "        # Handle answer being a list or other type\n",
    "        if isinstance(answer, list):\n",
    "             answer = \", \".join(str(a) for a in answer) # Join list elements into a string\n",
    "        elif not isinstance(answer, str):\n",
    "             answer = str(answer) # Convert other types to string\n",
    "        answer = answer.strip() # Apply strip after ensuring it's a string\n",
    "\n",
    "        # FIXED: Check both 'citations' and 'evidence' fields\n",
    "        citations = parsed.get('citations', parsed.get('evidence', []))\n",
    "        # Ensure citations are integers and unique\n",
    "        citations = sorted(list(set(int(c) for c in citations if isinstance(c, (int, str)) and str(c).isdigit())))\n",
    "\n",
    "        print(f\"{prefix}\u2705 JSON parse OK: answer='{answer[:50]}...', citations={citations}\", flush=True)\n",
    "        return answer, citations\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError, TypeError) as e:\n",
    "        print(f\"{prefix}\u26a0\ufe0f  JSON parse failed: {str(e)[:50]}. Trying fallback...\", flush=True)\n",
    "\n",
    "        # Method 2: Try to find the JSON-like part and parse it\n",
    "        json_match = re.search(r'\\{.*\\}', generated_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_substring = json_match.group(0) # Get the matched substring\n",
    "            print(f\"{prefix}\ud83d\udd0d Found JSON substring, parsing...\", flush=True)\n",
    "            try:\n",
    "                parsed_substring = json.loads(json_substring)\n",
    "                answer = parsed_substring.get('answer', '')\n",
    "                # Handle answer being a list or other type in substring\n",
    "                if isinstance(answer, list):\n",
    "                    answer = \", \".join(str(a) for a in answer)\n",
    "                elif not isinstance(answer, str):\n",
    "                    answer = str(answer)\n",
    "                answer = answer.strip() # Apply strip after ensuring it's a string\n",
    "\n",
    "                # FIXED: Check both 'citations' and 'evidence' fields\n",
    "                citations = parsed_substring.get('citations', parsed_substring.get('evidence', []))\n",
    "                citations = sorted(list(set(int(c) for c in citations if isinstance(c, (int, str)) and str(c).isdigit())))\n",
    "                print(f\"{prefix}\u2705 Substring parse OK: answer='{answer[:50]}...', citations={citations}\", flush=True)\n",
    "                return answer, citations\n",
    "            except (json.JSONDecodeError, ValueError, TypeError) as sub_e:\n",
    "                 print(f\"{prefix}\u26a0\ufe0f  Substring parse failed. Using regex...\", flush=True)\n",
    "\n",
    "        # Method 3: Fallback to regex on raw text (less robust but handles some cases)\n",
    "        print(f\"{prefix}\ud83d\udd04 Using regex fallback...\", flush=True)\n",
    "\n",
    "        # Attempt to find answer using regex\n",
    "        answer_match = re.search(r'\"answer\"\\s*:\\s*(\"([^\"]*)\"|\\[.*?\\])', generated_text, re.DOTALL) # Added capture for list\n",
    "        if answer_match:\n",
    "            # Check if it's a string or a list match\n",
    "            if answer_match.group(2): # String match\n",
    "                 answer = answer_match.group(2) # Don't strip yet\n",
    "            elif answer_match.group(1): # List match (capture group 1 contains the list string)\n",
    "                 list_str = answer_match.group(1) # Don't strip yet\n",
    "                 try:\n",
    "                     # Attempt to parse the list string\n",
    "                     answer_list = json.loads(list_str)\n",
    "                     if isinstance(answer_list, list):\n",
    "                          answer = \", \".join(str(a) for a in answer_list)\n",
    "                     else:\n",
    "                          answer = str(answer_list) # Should ideally be a list\n",
    "                 except:\n",
    "                      answer = list_str # If list parsing fails, just use the string representation\n",
    "            else:\n",
    "                 answer = generated_text[:100] # Default if regex finds something unexpected, don't strip yet\n",
    "        else:\n",
    "            # Fallback if no \"answer\": \"...\" or \"answer\": [...] found\n",
    "            # Try to extract the first reasonable looking text chunk\n",
    "            answer = generated_text.split('\\n')[0] # Take the first line, don't strip yet\n",
    "            if len(answer) > 100: answer = answer[:100] + \"...\"\n",
    "\n",
    "        answer = answer.strip() # Apply strip after all possibilities\n",
    "\n",
    "        # FIXED: Look for both \"citations\" and \"evidence\" arrays using regex\n",
    "        citations_match = re.search(r'\"citations\"\\s*:\\s*\\[([\\d,\\s]+)\\]', generated_text)\n",
    "        if not citations_match:\n",
    "            # Try \"evidence\" if \"citations\" not found\n",
    "            citations_match = re.search(r'\"evidence\"\\s*:\\s*\\[([\\d,\\s]+)\\]', generated_text)\n",
    "\n",
    "        citations = []\n",
    "        if citations_match:\n",
    "            citations_str = citations_match.group(1)\n",
    "            citations = sorted(list(set(int(c.strip()) for c in citations_str.split(',') if c.strip().isdigit())))\n",
    "\n",
    "        print(f\"{prefix}\ud83d\udcdd Regex result: answer='{answer[:50]}...', citations={citations}\", flush=True)\n",
    "\n",
    "        return answer, citations\n",
    "\n",
    "\n",
    "def fallback_parse(raw_response: str, contexts: List[str]) -> QAOutput:\n",
    "    \"\"\"\n",
    "    Fallback parser for malformed responses within the QAOutput structure.\n",
    "    This is called by parse_and_validate_response when the full JSON parse fails.\n",
    "\n",
    "    FIXED: Attempts JSON parse first, then uses regex fallback.\n",
    "    Ensures reasoning is a string.\n",
    "    Now handles both \"citations\" and \"evidence\" fields.\n",
    "\n",
    "    Args:\n",
    "        raw_response: Raw model output string\n",
    "        contexts: List of context passages (for validation)\n",
    "\n",
    "    Returns:\n",
    "        QAOutput object with answer, reasoning (str), and citations\n",
    "    \"\"\"\n",
    "    print(\"\ud83d\udd04 fallback_parse called...\", flush=True)\n",
    "    debug_text = raw_response[:150].replace('\\n', ' ')\n",
    "    print(f\"   Input: {debug_text}...\", flush=True)\n",
    "\n",
    "    # Set num_contexts for validation BEFORE attempting to create QAOutput\n",
    "    # This needs to be done on the QAOutput class itself\n",
    "    if hasattr(QAOutput, '_num_contexts'):\n",
    "        original_num_contexts = QAOutput._num_contexts\n",
    "    else:\n",
    "        original_num_contexts = 0\n",
    "    QAOutput._num_contexts = len(contexts)\n",
    "\n",
    "    try:\n",
    "        # Attempt JSON parse first\n",
    "        parsed = json.loads(raw_response)\n",
    "        print(\"\u2705 fallback_parse: JSON OK\", flush=True)\n",
    "\n",
    "        # Normalize reasoning to string if it's a list or other type\n",
    "        reasoning = parsed.get('reasoning', '')\n",
    "        if isinstance(reasoning, list):\n",
    "            reasoning = ' '.join(str(step) for step in reasoning)\n",
    "        elif not isinstance(reasoning, str):\n",
    "             reasoning = str(reasoning)\n",
    "\n",
    "        # FIXED: Check both 'citations' and 'evidence' fields\n",
    "        citations = parsed.get('citations', parsed.get('evidence', []))\n",
    "        citations = [int(c) for c in citations if isinstance(c, (int, str)) and str(c).isdigit()]\n",
    "\n",
    "        # Create QAOutput - Validation will happen here\n",
    "        qa_output = QAOutput(\n",
    "            answer=parsed.get('answer', 'insufficient context'),\n",
    "            reasoning=reasoning,\n",
    "            citations=citations\n",
    "        )\n",
    "        print(\"\u2705 QAOutput created from JSON\", flush=True)\n",
    "        return qa_output\n",
    "\n",
    "    except (json.JSONDecodeError, ValidationError, ValueError, TypeError) as e:\n",
    "        print(f\"\u26a0\ufe0f fallback_parse: JSON failed, using regex\", flush=True)\n",
    "\n",
    "        # Regex fallback on the raw string\n",
    "        answer, citations = extract_answer_and_citations(raw_response) # Re-use the regex logic from extract_answer_and_citations\n",
    "        reasoning = \"\" # In this deep fallback, we don't try to reconstruct reasoning from unstructured text\n",
    "\n",
    "        # Ensure citations are within bounds using the set _num_contexts\n",
    "        valid_citations = [c for c in citations if 1 <= c <= len(contexts)]\n",
    "        if len(valid_citations) != len(citations):\n",
    "             print(f\"\u26a0\ufe0f Filtered {len(citations) - len(valid_citations)} invalid citations\", flush=True)\n",
    "\n",
    "        # Create QAOutput - Validation will happen again, but with filtered citations\n",
    "        try:\n",
    "            qa_output = QAOutput(\n",
    "                answer=answer,\n",
    "                reasoning=reasoning,\n",
    "                citations=valid_citations # Use filtered citations\n",
    "            )\n",
    "            print(\"\u2705 QAOutput created from regex\", flush=True)\n",
    "            return qa_output\n",
    "        except ValidationError as final_e:\n",
    "            print(f\"\u274c Final validation failed: {final_e}\", flush=True)\n",
    "            # Return a minimal QAOutput indicating failure\n",
    "            return QAOutput(answer=\"parsing error\", reasoning=\"\", citations=[])\n",
    "\n",
    "    finally:\n",
    "        # Restore original _num_contexts or clean up\n",
    "        if hasattr(QAOutput, '_num_contexts'):\n",
    "             if original_num_contexts > 0:\n",
    "                  QAOutput._num_contexts = original_num_contexts\n",
    "             else:\n",
    "                  delattr(QAOutput, '_num_contexts')\n",
    "\n",
    "\n",
    "# Test the fix\n",
    "print(\"\ud83d\udd27 Testing the REVISED extract_answer_and_citations()...\", flush=True)\n",
    "\n",
    "# Add test for \"evidence\" field\n",
    "test_response_with_evidence = \"\"\"{\n",
    "  \"answer\": \"Kurt Weill\",\n",
    "  \"reasoning\": [\"Step 1\", \"Step 2\"],\n",
    "  \"evidence\": [7, 1]\n",
    "}\"\"\"\n",
    "\n",
    "print(\"\\n--- Testing 'evidence' field (the actual issue!) ---\", flush=True)\n",
    "answer, citations = extract_answer_and_citations(test_response_with_evidence)\n",
    "print(f\"   Answer: '{answer}'\", flush=True)\n",
    "print(f\"   Citations: {citations}\", flush=True)\n",
    "assert answer == \"Kurt Weill\" and citations == [1, 7], f\"FAILED: got '{answer}' and {citations}\"\n",
    "print(\"\u2705 PASS\", flush=True)\n",
    "\n",
    "# Quick tests for key scenarios\n",
    "test_response_good = \"\"\"{\"answer\": \"Gimme Shelter\", \"citations\": [1, 7]}\"\"\"\n",
    "test_response_answer_list = \"\"\"{\"answer\": [\"Item 1\", \"Item 2\"], \"citations\": [6]}\"\"\"\n",
    "\n",
    "print(\"\\n--- Testing 'citations' field ---\", flush=True)\n",
    "answer, citations = extract_answer_and_citations(test_response_good)\n",
    "assert answer == \"Gimme Shelter\" and citations == [1, 7]\n",
    "print(\"\u2705 PASS\", flush=True)\n",
    "\n",
    "print(\"\\n--- Testing answer as list ---\", flush=True)\n",
    "answer, citations = extract_answer_and_citations(test_response_answer_list)\n",
    "assert answer == \"Item 1, Item 2\" and citations == [6]\n",
    "print(\"\u2705 PASS\", flush=True)\n",
    "\n",
    "print(\"\\n\u2705 REVISED parsing functions now handle both 'citations' and 'evidence' fields!\", flush=True)\n",
    "print(\"\ud83d\udcdd Reduced debug verbosity for cleaner evaluation output\", flush=True)\n",
    "print(\"\ud83d\udd27 Added flush=True to fix output buffering with tqdm progress bar\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7m7ZRa8tavjs",
   "metadata": {
    "id": "7m7ZRa8tavjs"
   },
   "source": [
    "### wandb integration for saving finetuned adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lsq5cc7qdr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1760019547850,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "lsq5cc7qdr",
    "outputId": "6a7e8f7d-f4c5-411a-d01b-0d9047266aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcbe W&B Checkpoint management ready!\n",
      "\ud83d\udccb Features:\n",
      "   - Adapter-only saves (never full base model)\n",
      "   - Compressed artifacts <500MB\n",
      "   - Aliases: 'latest' and 'best'\n",
      "   - Resume capability from artifacts\n"
     ]
    }
   ],
   "source": [
    "# W&B Checkpoint Management (Artifact-based, <500MB)\n",
    "def save_adapter_only(peft_model, output_dir: str, max_shard_size: str = \"400MB\") -> str:\n",
    "    \"\"\"Save only LoRA adapter weights, compress to zip\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save adapter weights only\n",
    "    peft_model.save_pretrained(\n",
    "        output_dir,\n",
    "        max_shard_size=max_shard_size,\n",
    "        safe_serialization=True\n",
    "    )\n",
    "\n",
    "    # Create zip file\n",
    "    zip_path = f\"{output_dir}.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, output_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "\n",
    "    # Get zip size\n",
    "    zip_size_mb = os.path.getsize(zip_path) / 1024 / 1024\n",
    "    print(f\"\ud83d\udce6 Adapter zip created: {zip_path} ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "    if zip_size_mb > 500:\n",
    "        print(f\"\u26a0\ufe0f Warning: Zip size {zip_size_mb:.1f} MB exceeds 500MB limit\")\n",
    "\n",
    "    return zip_path\n",
    "\n",
    "def upload_adapter_artifact(\n",
    "    wandb_run,\n",
    "    zip_path: str,\n",
    "    aliases: List[str],\n",
    "    metadata: Dict\n",
    ") -> str:\n",
    "    \"\"\"Upload adapter zip as W&B artifact\"\"\"\n",
    "\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"qlora-adapters\",\n",
    "        type=\"model\",\n",
    "        description=\"QLoRA adapter weights for Mistral-7B HotpotQA fine-tuning\",\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # Add the zip file\n",
    "    artifact.add_file(zip_path)\n",
    "\n",
    "    # Log artifact with aliases\n",
    "    wandb_run.log_artifact(artifact, aliases=aliases)\n",
    "\n",
    "    print(f\"\ud83d\udce4 Uploaded artifact with aliases: {aliases}\")\n",
    "    return artifact.id\n",
    "\n",
    "def download_and_restore_adapter(wandb_run, artifact_alias: str = \"latest\") -> Optional[str]:\n",
    "    \"\"\"Download adapter from W&B artifact and restore\"\"\"\n",
    "    try:\n",
    "        # Get artifact\n",
    "        artifact = wandb_run.use_artifact(f\"qlora-adapters:{artifact_alias}\")\n",
    "        artifact_dir = artifact.download()\n",
    "\n",
    "        # Find zip file\n",
    "        zip_files = [f for f in os.listdir(artifact_dir) if f.endswith('.zip')]\n",
    "        if not zip_files:\n",
    "            print(f\"\u274c No zip file found in artifact {artifact_alias}\")\n",
    "            return None\n",
    "\n",
    "        zip_path = os.path.join(artifact_dir, zip_files[0])\n",
    "\n",
    "        # Extract zip\n",
    "        extract_dir = zip_path.replace('.zip', '_extracted')\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "            zipf.extractall(extract_dir)\n",
    "\n",
    "        print(f\"\ud83d\udce5 Downloaded and extracted adapter from {artifact_alias}\")\n",
    "        return extract_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to download artifact {artifact_alias}: {e}\")\n",
    "        return None\n",
    "\n",
    "class WandBCheckpointCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for W&B artifact management\"\"\"\n",
    "\n",
    "    def __init__(self, wandb_run, output_dir: str = \"./checkpoints\"):\n",
    "        self.wandb_run = wandb_run\n",
    "        self.output_dir = output_dir\n",
    "        self.best_metric = 0.0\n",
    "\n",
    "    def on_save(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"Called when checkpoint is saved\"\"\"\n",
    "        print('saving checkpoint to wandb on_save')\n",
    "        if model is None:\n",
    "            return\n",
    "\n",
    "        # Create checkpoint directory\n",
    "        checkpoint_dir = os.path.join(self.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "\n",
    "        try:\n",
    "            # Save adapter and create zip\n",
    "            zip_path = save_adapter_only(model, checkpoint_dir)\n",
    "\n",
    "            # Upload with 'latest' alias\n",
    "            metadata = {\n",
    "                \"step\": state.global_step,\n",
    "                \"epoch\": state.epoch,\n",
    "                \"learning_rate\": state.log_history[-1].get(\"learning_rate\", 0) if state.log_history else 0,\n",
    "                \"train_loss\": state.log_history[-1].get(\"train_loss\", 0) if state.log_history else 0,\n",
    "                \"base_model\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "            }\n",
    "\n",
    "            upload_adapter_artifact(\n",
    "                self.wandb_run,\n",
    "                zip_path,\n",
    "                aliases=[\"latest\"],\n",
    "                metadata=metadata\n",
    "            )\n",
    "\n",
    "            # Cleanup local files to save space\n",
    "            shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "            os.remove(zip_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Failed to save/upload checkpoint: {e}\")\n",
    "\n",
    "    def on_evaluate(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        \"\"\"Called after evaluation\"\"\"\n",
    "        if model is None or logs is None:\n",
    "            return\n",
    "\n",
    "        # Check if this is the best model so far\n",
    "        current_metric = logs.get(\"eval_f1\", 0.0)\n",
    "\n",
    "        if current_metric > self.best_metric:\n",
    "            self.best_metric = current_metric\n",
    "            print(f\"\ud83c\udfc6 New best model! F1: {current_metric:.4f}\")\n",
    "\n",
    "            # Save and upload as 'best'\n",
    "            checkpoint_dir = os.path.join(self.output_dir, f\"best-checkpoint-{state.global_step}\")\n",
    "\n",
    "            try:\n",
    "                zip_path = save_adapter_only(model, checkpoint_dir)\n",
    "\n",
    "                metadata = {\n",
    "                    \"step\": state.global_step,\n",
    "                    \"epoch\": state.epoch,\n",
    "                    \"eval_f1\": current_metric,\n",
    "                    \"eval_em\": logs.get(\"eval_em\", 0.0),\n",
    "                    \"eval_citation_acc\": logs.get(\"eval_citation_acc\", 0.0),\n",
    "                    \"base_model\": \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "                }\n",
    "\n",
    "                upload_adapter_artifact(\n",
    "                    self.wandb_run,\n",
    "                    zip_path,\n",
    "                    aliases=[\"best\", \"latest\"],\n",
    "                    metadata=metadata\n",
    "                )\n",
    "\n",
    "                # Cleanup\n",
    "                shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "                os.remove(zip_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\u274c Failed to save/upload best checkpoint: {e}\")\n",
    "\n",
    "print(\"\ud83d\udcbe W&B Checkpoint management ready!\")\n",
    "print(\"\ud83d\udccb Features:\")\n",
    "print(\"   - Adapter-only saves (never full base model)\")\n",
    "print(\"   - Compressed artifacts <500MB\")\n",
    "print(\"   - Aliases: 'latest' and 'best'\")\n",
    "print(\"   - Resume capability from artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DIvd5Hbf0rYD",
   "metadata": {
    "id": "DIvd5Hbf0rYD"
   },
   "source": [
    "# Prompt Generation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4tcb9el69gk",
   "metadata": {
    "id": "4tcb9el69gk"
   },
   "source": [
    "##  Baseline Evaluation: RAG Prompting (Pre-training)\n",
    "\n",
    "This section evaluates the base Mistral-7B-Instruct model using RAG prompting strategy before fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fYdBRoxO4323",
   "metadata": {
    "id": "fYdBRoxO4323"
   },
   "source": [
    "### Loading Mistral-7B-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01lhet1eutdg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880,
     "referenced_widgets": [
      "393e1c4fc4444138baf45aa868857376",
      "3d2810c47bc547268c7ef0e9275cceeb",
      "bd4a2b63edeb4d938a86a18c08bfb319",
      "ca2cf828cf5b47779ddb270af704b9e5",
      "4cb63cdd54184cd5bfdf5019770bada4",
      "335079439dfa48898565592d96b5e3a0",
      "085305ec56e346ebbdf98a7f4110c941",
      "ce4d3ca7c29f4945b80b4777562e9342",
      "1612a18bc3e94b68865ddecc13bf857b",
      "b75d37e839ad46c1b028daeaf16ce0dc",
      "363ab3dc989b465da01f6dad251a9e35",
      "c31306979cf5433f9969b5f7a700924c",
      "1fa47185cda4487bbcbd1bd0ade7aa08",
      "ae2330341f024f25893268efb1351bc7",
      "842982ed82d24b54b66030814605b85b",
      "7a846fc69808402499ae77acec99ceff",
      "5f444056f2d34e2e83c87aaa8bd08c74",
      "97c351c9a07a415db893fc5e4f37b353",
      "87cd0c83a63f4a8982b9afbb77da4131",
      "01bf9cefce43421d800b14b223487194",
      "ccd3a3229be94a75950748c57a40431d",
      "c76d3754ceee483da3b4ad1f8118f3e9",
      "65c424f15a0c41d6b12dcbc4760b0a0c",
      "707f577cb8ab4bfa91a1fbf7136b62b7",
      "d58ee84b514848838fed2e73df07485e",
      "a2e4ef4eb36d4d63b5b94c5b342af164",
      "dd93f787b0e747e8a8ef199d26a07213",
      "4fe26d02caad41528d5e64a45004a070",
      "2431ccdffec249b5b6b5a7c361b0e270",
      "8cbc4fa9adcb438696b0035c476136d2",
      "d02038fea5cc41fbb3ed8ffef21b48fb",
      "1cf16a2faed1473ba9ae4f4645e1c06b",
      "509cd0b9862b4c318dd9682615bc945d",
      "7a682bcdcf6f4fe09d64bc54a972f7df",
      "3e84f05ae0274c69ab6f322e6ba722d3",
      "ed17f280177e46abb93219ed4792f05c",
      "cfbb8c4c46d842c7905d41b1cf5e9cf5",
      "c8b46470b21b45958ad4ecb2c5e4e419",
      "e0a127e339cb4174bf00654cf3f32f78",
      "d2c8af1cb1df4c52af892a5225dcc1c6",
      "2ec4e474c8114be7978b0b2867567604",
      "ba851777dfe845a9ab888330073d0e8e",
      "2d86d10d6c8b438a95a3215fc9089bdd",
      "e435f20e3fb94e1cbce2c1f14fa27d5f",
      "0ff0bfa4510b45458f48dc8f185283d1",
      "e6f5bed45ea4420a863e61242518e0eb",
      "41ce69f8fc6a4112a4edf7072de860b5",
      "06feb49afef6433498442b813539716e",
      "cc128d431dd346aaa8cde7999808bc75",
      "814299fbdb7f4192b17b5700bce29d76",
      "b94b35bfeab348449a9c5de97195849a",
      "9f4288faa20d45b68b8aaa1434d196c8",
      "829ced0967144b4b8d759d36191c8a3c",
      "0910ae59a4074b55af9f7413642000dd",
      "f995fecfbb6042698d8d9155895a8c5f",
      "3d27c4fe526644f6bbb710a6e1d04c65",
      "b4fff7bee8584db98b91a19708319453",
      "86bfc101972d4e108269f33340d1e7e3",
      "978554eb4607431b91cdaf138ebb84b2",
      "4aba96986c95416782a068d1022877ab",
      "c1fe713e86044d6b8ba3203474d48187",
      "458e9b1854684513b7ae8d66c0eab48c",
      "d2278ecf3d03484aa04367bffbb8ed89",
      "a2d1fbb3adf94ecda033e3ccc4a6d186",
      "d890aec42924427f8a5d3cf7448a2a15",
      "32eaf86422d842068b067a47e8172e84",
      "35ccbbe7d2ca4289a33055e128f2e0d3",
      "24c29762c9ef4e7e80d9c9294e116af3",
      "bb5eccb2a97242e7810e7cf13ce713fb",
      "4a7197cfd8544606a0f6bb35c2afd956",
      "b65e89a8d36c4701a8ea30070b2cc6d7",
      "1e7d3b05ea3245da9c5bd72092096471",
      "383facbcd33641088d8ce1c0a9679fff",
      "4fa21d794f734d078c62f1c96ca342ca",
      "daaad329400341d7a6f82259f2fccb3d",
      "633708e71daf4140a8161ec3405cda70",
      "804c36176e5c4d1ab7964e24584614a6",
      "8b3e7f8e1e4341c9b2b0f52f31181089",
      "9065c28a88b5488cb5413415bfeae51a",
      "be101df85dfd4e3d8396e0d815da49d8",
      "bf69f8e28e64435192aaf73e65ea2854",
      "90133904a807401c83391a6eb9acda8d",
      "9f06d62d0716456887fceda88251e689",
      "fd3a4eef23c64d4e87c365d9b9e2b95f",
      "4ea46134a60d4522b8076987fb045033",
      "fc55ce38fd9448a0b6855a819a815f9f",
      "7783ec693b064f1daac952ad899df9de",
      "8e046df5dcaa443c83b7b0842b4b4538"
     ]
    },
    "executionInfo": {
     "elapsed": 134740,
     "status": "ok",
     "timestamp": 1760019682591,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "01lhet1eutdg",
    "outputId": "14713d21-b4a9-4e91-cc99-0b4689cdc2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\uddf9 Attempting to clear GPU memory...\n",
      "   GPU Memory BEFORE cleanup:\n",
      "     Allocated: 0.00 GB\n",
      "     Cached: 0.00 GB\n",
      "   Attempting to clear CUDA cache...\n",
      "   Cleared CUDA cache.\n",
      "\u2705 Memory clear attempt complete.\n",
      "   GPU Memory AFTER cleanup (before loading new model):\n",
      "     Allocated: 0.00 GB\n",
      "     Cached: 0.00 GB\n",
      "   Memory reduction (Allocated): 0.00 GB\n",
      "   Memory reduction (Cached): 0.00 GB\n",
      "\ud83d\udd27 Loading model: mistralai/Mistral-7B-Instruct-v0.2\n",
      "\ud83d\udcd0 LoRA Config: rank=16, alpha=32, dropout=0.1\n",
      "\ud83d\udcbe Cache directory: ./models\n",
      "\u2705 HuggingFace authenticated as: jeffgong11235\n",
      "tokenizer is already here\n",
      "\ud83d\udd04 Loading quantized model with use_cache=False...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393e1c4fc4444138baf45aa868857376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31306979cf5433f9969b5f7a700924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c424f15a0c41d6b12dcbc4760b0a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a682bcdcf6f4fe09d64bc54a972f7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff0bfa4510b45458f48dc8f185283d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d27c4fe526644f6bbb710a6e1d04c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ccbbe7d2ca4289a33055e128f2e0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3e7f8e1e4341c9b2b0f52f31181089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Adding LoRA adapters...\n",
      "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n",
      "\n",
      "\ud83d\udcca Model Statistics:\n",
      "   Total parameters: 7,283,675,136\n",
      "   Trainable parameters: 41,943,040\n",
      "   Trainable %: 0.58%\n",
      "   Memory footprint: ~6.8 GB (8-bit)\n",
      "\u2705 Mistral-7B model loaded with persistent cache!\n",
      "\ud83d\udcbe Model cached at: ./models\n",
      "\ud83d\udd04 Ready for QLoRA training on RTX A5000\n",
      "\n",
      "   GPU Memory AFTER loading new model:\n",
      "     Allocated: 7.64 GB\n",
      "     Cached: 8.42 GB\n"
     ]
    }
   ],
   "source": [
    "# Release memory from previously loaded model if it exists\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "print(\"\ud83e\uddf9 Attempting to clear GPU memory...\")\n",
    "\n",
    "# --- Check memory BEFORE cleanup ---\n",
    "if torch.cuda.is_available():\n",
    "    allocated_before = torch.cuda.memory_allocated() / 1024**3\n",
    "    cached_before = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"   GPU Memory BEFORE cleanup:\")\n",
    "    print(f\"     Allocated: {allocated_before:.2f} GB\")\n",
    "    print(f\"     Cached: {cached_before:.2f} GB\")\n",
    "else:\n",
    "    print(\"   CUDA not available, skipping memory checks.\")\n",
    "\n",
    "\n",
    "if 'model' in globals() and model is not None:\n",
    "    try:\n",
    "        print(\"   Deleting 'model' variable...\")\n",
    "        del model\n",
    "        print(\"   Deleted 'model' variable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error deleting model: {e}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    print(\"   Attempting to clear CUDA cache...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"   Cleared CUDA cache.\")\n",
    "else:\n",
    "    print(\"   CUDA not available, skipping cache clear.\")\n",
    "print(\"\u2705 Memory clear attempt complete.\")\n",
    "\n",
    "# --- Check memory AFTER cleanup (before loading new model) ---\n",
    "if torch.cuda.is_available():\n",
    "    allocated_after_cleanup = torch.cuda.memory_allocated() / 1024**3\n",
    "    cached_after_cleanup = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"   GPU Memory AFTER cleanup (before loading new model):\")\n",
    "    print(f\"     Allocated: {allocated_after_cleanup:.2f} GB\")\n",
    "    print(f\"     Cached: {cached_after_cleanup:.2f} GB\")\n",
    "    print(f\"   Memory reduction (Allocated): {allocated_before - allocated_after_cleanup:.2f} GB\")\n",
    "    print(f\"   Memory reduction (Cached): {cached_before - cached_after_cleanup:.2f} GB\")\n",
    "\n",
    "\n",
    "# Model configuration - Mistral-7B-Instruct-v0.2 with persistent cache\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "\n",
    "# Cache directory for RunPod persistence (will be preserved across sessions)\n",
    "CACHE_DIR = \"/workspace/models\" if os.path.exists(\"/workspace\") else \"./models\"\n",
    "\n",
    "print(f\"\ud83d\udd27 Loading model: {MODEL_NAME}\")\n",
    "print(f\"\ud83d\udcd0 LoRA Config: rank={LORA_RANK}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\")\n",
    "print(f\"\ud83d\udcbe Cache directory: {CACHE_DIR}\")\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Check if we're authenticated with HuggingFace (required for Mistral)\n",
    "try:\n",
    "    from huggingface_hub import whoami\n",
    "    user_info = whoami()\n",
    "    print(f\"\u2705 HuggingFace authenticated as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f HuggingFace authentication required for Mistral model\")\n",
    "    print(f\"   Run: huggingface-cli login\")\n",
    "    print(f\"   Or set HF_TOKEN environment variable\")\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# 8-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "if \"tokenizer\" not in globals():\n",
    "  print(\"\ud83d\udd04 Loading tokenizer...\")\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\n",
    "      MODEL_NAME,\n",
    "      cache_dir=CACHE_DIR,\n",
    "      trust_remote_code=True\n",
    "  )\n",
    "  if tokenizer.pad_token is None:\n",
    "      tokenizer.pad_token = tokenizer.eos_token\n",
    "  tokenizer.padding_side = \"right\"\n",
    "else:\n",
    "  print('tokenizer is already here')\n",
    "\n",
    "print(\"\ud83d\udd04 Loading quantized model with use_cache=False...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, # Keep bfloat16 for compute\n",
    "    cache_dir=CACHE_DIR,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False # Explicitly set use_cache to False\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration for Mistral architecture\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # attention modules\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP modules\n",
    "    ],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "print(\"\ud83d\udd04 Adding LoRA adapters...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print model info\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Calculate model size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"   Memory footprint: ~{total_params * 1 / 1024**3:.1f} GB (8-bit)\") # Estimate for 8-bit\n",
    "\n",
    "\n",
    "print(\"\u2705 Mistral-7B model loaded with persistent cache!\")\n",
    "print(f\"\ud83d\udcbe Model cached at: {CACHE_DIR}\")\n",
    "print(\"\ud83d\udd04 Ready for QLoRA training on RTX A5000\")\n",
    "\n",
    "# --- Check memory AFTER loading new model ---\n",
    "if torch.cuda.is_available():\n",
    "    allocated_after_load = torch.cuda.memory_allocated() / 1024**3\n",
    "    cached_after_load = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\n   GPU Memory AFTER loading new model:\")\n",
    "    print(f\"     Allocated: {allocated_after_load:.2f} GB\")\n",
    "    print(f\"     Cached: {cached_after_load:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679k9oabAQqG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1760019682601,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "679k9oabAQqG",
    "outputId": "6eb11474-04e9-4957-d08f-63c39d602861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of instruction: 1363\n",
      "Length of CoT exemplars string: 0\n",
      "Total length of prompt template (instruction + exemplars): 1363 characters.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of the combined instruction and CoT exemplars\n",
    "if 'building_prompts_rag' in globals():\n",
    "    instruction_length = len(building_prompts_rag.get('instruction', ''))\n",
    "    cot_exemplar_length = len(building_prompts_rag.get('cot_exemplar', ''))\n",
    "    total_prompt_template_length = instruction_length + cot_exemplar_length\n",
    "    print(f\"Length of instruction: {instruction_length}\")\n",
    "    print(f\"Length of CoT exemplars string: {cot_exemplar_length}\")\n",
    "    print(f\"Total length of prompt template (instruction + exemplars): {total_prompt_template_length} characters.\")\n",
    "else:\n",
    "    print(\"Variable 'building_prompts_rag' is not defined in the current environment. Please run the relevant cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sww-fAUU5O94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1760019682618,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "sww-fAUU5O94",
    "outputId": "f93ca93c-a061-4c64-9cc8-45ca5f8cfddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot_exemplar: \n",
      "instruction: Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned through the given sources,\n",
      "say insufficient context.Please give an answer that could only be deduced from the evidences presented to you. If you could not deduce the result from the evidences presented to you, please say insufficient contexts.\n",
      "Additionally, please keep your output strictly following the JSON format.  \"output\": {\n",
      "    \"answer\": \"Failsworth\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell\",\n",
      "      \"From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England\",\n",
      "      \"Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth\",\n",
      "      \"Therefore, the company Peter Hobbs founded is based in Failsworth\"\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "      7,\n",
      "      8\n",
      "    ]\n",
      "  }\n",
      "    Please give the direct answer for this case, for answer you dont need to show reasoning, reasoning goes to field \"reasoning\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"cot_exemplar: {building_prompts_rag.get('cot_exemplar', '')}\")\n",
    "print(f\"instruction: {building_prompts_rag.get('instruction', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vvEmNRaQ50OZ",
   "metadata": {
    "id": "vvEmNRaQ50OZ"
   },
   "source": [
    "## Demo testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lCDfjoMiywe4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133236,
     "status": "ok",
     "timestamp": 1760019815869,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "lCDfjoMiywe4",
    "outputId": "be17dd3e-43d2-4198-ada0-338c1a5f6bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Debugging Low Scores: Inspecting Model Outputs\n",
      "======================================================================\n",
      "model config:  MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "\ud83d\udcdd Displaying 5 examples from the evaluation set:\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcdd EXAMPLE 1\n",
      "================================================================================\n",
      "\u2753 Question: What nationality was Oliver Reed's character in the film Royal Flash?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airport is a county owned, public use airport in Knox County, Maine, United States Since the question asks which airport is in Maine, and Sacramento International Airport is in California while Knox County Regional Airport is in Maine Therefore, Knox County Regional Airport is the airport located in Maine\",\n",
      "  \"answer\": \"Prussian\",\n",
      "  \"citations\": [\n",
      "    1,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Provided Passages:\n",
      "   [1] Royal Flash (film): Royal Flash is a 1975 film based on George MacDonald Fraser's second Flashman novel, \"Royal Flash\".  It stars Malcolm McDowell as Flashman.  Additionally, Oliver Reed appeared in the role of Otto von Bismarck, Alan Bates as Rudi von Sternberg, and Florinda Bolkan played Lola Montez.  Fraser wrote the screenplay and the film was directed by Richard Lester. have \n",
      "   [2] Lion of the Desert: Lion of the Desert is a 1981 Libyan historical action film starring Anthony Quinn as Libyan tribal leader Omar Mukhtar, a Bedouin leader fighting the \"Regio Esercito\" (Italian Royal Army) in the years leading up to World War II, and Oliver Reed as Italian General Rodolfo Graziani, who attempted to defeat Mukhtar.  It was directed by Moustapha Akkad and funded by the government under Colonel Muammar Gaddafi.  Released in May 1981, the film was liked by critics and audiences but performed poorly financially, bringing in just $1 million net worldwide.  .  The film was banned in Italy in 1982 and was only shown on pay TV in 2009. have \n",
      "   [3] Ivan Dragomiloff: Ivan Dragomiloff is a fictional character, the chairman of \"The Assassination Bureau, Ltd\" in the book of that name by Jack London.  The character was played by actor Oliver Reed in the film of the same name. have \n",
      "   [4] The Duke of Hamilton: The Duke of Hamilton was one of the oldest pubs in London, situated in Hampstead.  It was a popular meeting place for actors Peter O'Toole, Oliver Reed and Richard Burton.  Reed would be seen for long periods at the pub on a daily basis. have \n",
      "   [5] Otto von Bismarck: Otto Eduard Leopold, Prince of Bismarck, Duke of Lauenburg (1 April 1815 \u2013 30 July 1898), known as Otto von Bismarck (] ), was a conservative Prussian statesman who dominated German and European affairs from the 1860s until 1890.  In the 1860s, he engineered a series of wars that unified the German states, deliberately excluding Austria, into a powerful German Empire under Prussian leadership.  With that accomplished by 1871, he skillfully used balance of power diplomacy to maintain Germany's position in a Europe which, despite many disputes and war scares, remained at peace.  For historian Eric Hobsbawm, it was Bismarck who \"remained undisputed world champion at the game of multilateral diplomatic chess for almost twenty years after 1871, [and] devoted himself exclusively, and successfully, to maintaining peace between the powers\".  However, his annexation of Alsace-Lorraine gave new fuel to French nationalism and promoted Germanophobia in France.  This helped set the stage for the First World War. have \n",
      "   [6] Harry Flashman: Sir Harry Paget Flashman is a fictional character created by Thomas Hughes (1822\u20131896) in a semi-autobiographical \"Tom Brown's School Days\" (1857) and later developed by George MacDonald Fraser (1925\u20132008).  Harry Flashman appears in a series of 12 of Fraser's books, collectively known as \"The Flashman Papers\", with covers illustrated by Arthur Barbosa.  Flashman was played by Malcolm McDowell in the Richard Lester 1975 film \"Royal Flash\". have \n",
      "   [7] Funny Bones: Funny Bones is a 1995 British-American comedy-drama film from Hollywood Pictures.  It was written, directed and produced by Peter Chelsom, co produced by Simon Fields, and co written by Peter Flannery.  The music score was by John Altman, and the cinematography by Eduardo Serra.  Set in Las Vegas and Blackpool, England, the film stars Oliver Platt, Jerry Lewis, Lee Evans, Leslie Caron, Richard Griffiths, Sadie Corre, Oliver Reed, George Carl, Freddie Davies and Ian McNeice. have \n",
      "   [8] Robin Barton: Robin Barton (born 5 November 1958) is a British art dealer dealing primarily with Banksy's.  Barton studied photography and graphic design at the Exeter College of Art and Design and this was his first encounter with Russell Young.  Moving to London in 1980 he began working as a freelance photographer for music and fashion publications \"Sounds\", \"NME\", \"Blitz\", \"The Face\" moving on to working regularly for pioneering \"Independent Magazine\" photographing amongst others Sir Alec Guinness, Oliver Reed, Johnny Depp, Lou Reed, Hugh Grant and Sir Peter Hall.  Laterly he worked for other publications \"Sunday Times\", \"Sunday Telegraph\", \"Elle\", \"Vogue\", \"Tatler\" and \"Blueprint\". have \n",
      "Prompt length: 5937\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'evidence'])\n",
      "\u274c Parsing error: 1 validation error for QAOutput\n",
      "citations\n",
      "  Field required [type=missing, input_value={'answer': 'Otto von Bism...sman'], 'evidence': [1]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\ud83d\udd04 fallback_parse called...\n",
      "   Input: {   \"answer\": \"Otto von Bismarck (German)\",   \"reasoning\": [     \"From evidence [1]: Oliver Reed acted as Otto von Bismarck in the film Royal Flash\", ...\n",
      "\u2705 fallback_parse: JSON OK\n",
      "\u2705 QAOutput created from JSON\n",
      "Generated answer length: 248\n",
      "\n",
      "\ud83e\udd16 Non finetuned Model Prediction:\n",
      "   {\n",
      "  \"answer\": \"Otto von Bismarck (German)\",\n",
      "  \"reasoning\": [\n",
      "    \"From evidence [1]: Oliver Reed acted as Otto von Bismarck in the film Royal Flash\",\n",
      "    \"From evidence [5]: Otto von Bismarck was a German statesman\"\n",
      "  ],\n",
      "  \"evidence\": [\n",
      "    1\n",
      "  ]\n",
      "}\n",
      "Non finetuned Model Prediction finished\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcdd EXAMPLE 2\n",
      "================================================================================\n",
      "\u2753 Question: Pacific Mozart Ensemble performed which German composer's Der Lindberghflug in 2002?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell From evidence [8]: Russell Hobbs is a manufacturer of household appliances based in Failsworth, Greater Manchester, England Since Peter Hobbs founded Russell Hobbs, and Russell Hobbs is based in Failsworth Therefore, the company Peter Hobbs founded is based in Failsworth\",\n",
      "  \"answer\": \"Kurt Julian Weill\",\n",
      "  \"citations\": [\n",
      "    1,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Provided Passages:\n",
      "   [1] Pacific Mozart Ensemble: Pacific Edge Voices (formerly The Pacific Mozart Ensemble (PME)) is a volunteer choral organization based in Berkeley, CA.  The group was formed to provide a chorus of professional quality for highly skilled and experienced singers who did not wish to make singing a full-time profession.  It was to be large enough to perform the major concert literature, but small enough to remain highly selective.  PEV presents a wide range of choral musical styles, including, but not limited to, traditional choral literature, new works by contemporary composers and a cappella jazz and pop.  PEV performs at least three self-produced concerts sets each year, along with various collaborations, often with prominent artists including Dave Brubeck, Meredith Monk, Kent Nagano & Sufjan Stevens.  The first and second concerts of the year (typically Nov and March) are classically oriented programs.  Over the years these programs have tended toward 20th-century composers.  The chorus has become known around the San Francisco Bay Area for its innovative programming.  A particular highlight came in 2002 when the chorus performed Kurt Weill\u2019s Der Lindberghflug alongside works by Philip Glass, Meredith Monk and David Lang.  The concert was presented in the East Bay on the aircraft carrier Hornet and in San Francisco in the newly constructed Aviation Museum at SFO.  The 3rd concert set each year is an all a cappella \u2018pops\u2019 concert featuring the group in various formations from 2 up to 50, performing arrangements of jazz, pop, rock, & folk tunes. have \n",
      "   [2] Der Widersp\u00e4nstigen Z\u00e4hmung: Der Widersp\u00e4nstigen Z\u00e4hmung (also: Der Widerspenstigen Z\u00e4hmung ) (English: \"The Taming of the Shrew\") is a German-language comic opera in four acts by the German composer Hermann Goetz.  It was written between 1868 and 1872 and first performed at the National Theatre Mannheim on 11 October 1874 under the conductor Ernst Frank.  The libretto, by and the composer, is based on Shakespeare's \"The Taming of the Shrew\".  The style of the opera shows Goetz turning away from the musical ideas of Richard Wagner towards the classicism of Mozart.  \"Der Widersp\u00e4nstigen Z\u00e4hmung \" was a huge success, not only in Germany but in the United States and in Great Britain, where it received high praise from George Bernard Shaw. have \n",
      "   [3] The Flight Across the Ocean: The Flight across the Ocean (\"Der Ozeanflug\") is a \"Lehrst\u00fcck\" by the German dramatist Bertolt Brecht, inspired by \"We\", Charles Lindbergh's 1927 account of his transatlantic flight.  Written for the Baden-Baden Music Festival, it was originally entitled \"Lindbergh's Flight\" (\"Der Lindberghflug\") and premiered in 1929 with music by Kurt Weill and Paul Hindemith in a broadcast by the Southwest German Radio Orchestra under the direction of Hermann Scherchen. have \n",
      "   [4] Martin Boykan: Boykan was born in New York City.  He studied composition first with Walter Piston at Harvard, where he received a BA in 1951.  He then went to Z\u00fcrich to study with Paul Hindemith, with whom he continued his studies at Yale University, earning an MM in 1953.  Subsequently, he went to Vienna on a Fulbright scholarship .  He also studied composition with Aaron Copland at Tanglewood (1949, 1950), and piano with Eduard Steuermann.  Upon his return to the United States in 1955 he founded the Brandeis Chamber Ensemble, whose other members included Robert Koff (Juilliard String Quartet), Nancy Cirillo (Wellesley), Eugene Lehner (Kolisch Quartet), and Madeline Foley (Marlboro Festival).  This ensemble performed widely with a repertory divided equally between contemporary music and the tradition.  At the same time Boykan appeared regularly as a pianist with soloists such as Joseph Silverstein and Jan DeGaetani.  In 1964\u201365, he was the pianist with the Boston Symphony Orchestra under Erich Leinsdorf. have \n",
      "   [5] Portland Youth Philharmonic: The Portland Youth Philharmonic (PYP) is the oldest youth orchestra in the United States, established in 1924 as the Portland Junior Symphony (PJS).  Now based in Portland, Oregon, the orchestra's origin dates back to 1910 when music teacher Mary V. Dodge began playing music for local children in Burns.  Dodge purchased instruments for the children and organized the orchestra which would become known as the Sagebrush Symphony Orchestra.  After touring throughout the U.S. state of Oregon, including a performance at the Oregon State Fair in Salem, the orchestra disbanded in 1918 when Dodge moved to Portland.  There, Dodge opened a violin school and became music director of the Irvington School Orchestra.  Hoping to create a permanent youth symphony, Dodge approached Jacques Gershkovitch in 1924 to lead the orchestra as music director of the Portland Junior Symphony.  The ensemble performed for the first time in 1925, and by the 1930s PJS concerts were being broadcast nationally.  Following Gershkovitch's death in 1953, alumnus Jacob Avshalomov became the orchestra's music director.  The ensemble's name was changed to the Portland Youth Philharmonic in 1978. have \n",
      "   [6] University of Utah Singers: The University of Utah Singers (UU Singers) was the premier choral ensemble at the University of Utah until 2010.  The ensemble was organized in 2003 by Dr. Brady R. Allred.  Composed of approximately 45 voices, the ensemble performed repertoire from a wide range of musical styles and eras.  In their short history, UU Singers achieved both national and international acclaim, winning the Grand Prize at the 2005 Floril\u00e8ge Vocal de Tours International Choir Competition in Tours, France, winning the European Grand Prix Choral Competition in Tolosa, Spain in 2006, winning first prize at the 11th International Chamber Choir Competition Marktoberdorf in 2009, participating in the 19th Festival \u201cChoralies de Vaison-la-Romaine\u201d in France and the 37th Abu Gosh international vocal music festival near Jerusalem.  The UU Singers performed in concerts throughout England, France, Spain, the Netherlands, Italy, the Czech Republic, Hungary, Croatia, Slovenia, Austria, Germany and Israel on five international concert tours, and has appeared on French national television at the Nancy International Choir Festival. have \n",
      "   [7] The Tutor (Brecht): The Tutor is the 1950 adaptation, by 20th century German dramatist Bertolt Brecht, of an 18th-century play by Lenz.  The original Lenz play was produced in 1774 and is also known by the title \"The Advantages of a Private Education\".  Brecht contributed few additions to the plot of the original work, but made many cuts and alterations.  Brecht's work is two thirds the length of the original play and over half the material is new.  The play was Brecht's first production which featured work from the German Classical Era for the Berliner Ensemble.  Overall, it was the third production the Berliner Ensemble performed.  Brecht himself directed this production.  'The Tutor' was translated by Ralph Manheim and Wolfgang Sauerlander. have \n",
      "   [8] Kurt Weill: Kurt Julian Weill (March 2, 1900April 3, 1950) was a German composer, active from the 1920s in his native country, and in his later years in the United States.  He was a leading composer for the stage who was best known for his fruitful collaborations with Bertolt Brecht.  With Brecht, he developed productions such as his best-known work \"The Threepenny Opera\", which included the ballad \"Mack the Knife\".  Weill held the ideal of writing music that served a socially useful purpose.  He also wrote several works for the concert hall.  He became a United States citizen on August 27, 1943. have \n",
      "Prompt length: 9276\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'evidence'])\n",
      "\u274c Parsing error: 1 validation error for QAOutput\n",
      "citations\n",
      "  Field required [type=missing, input_value={'answer': 'Kurt Weill', ...t'], 'evidence': [3, 8]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\ud83d\udd04 fallback_parse called...\n",
      "   Input: {   \"answer\": \"Kurt Weill\",   \"reasoning\": [     \"From evidence [3]: Der Lindberghflug is a Lehrst\u00fcck by Bertolt Brecht, premiered with music by Kurt ...\n",
      "\u2705 fallback_parse: JSON OK\n",
      "\u2705 QAOutput created from JSON\n",
      "Generated answer length: 303\n",
      "\n",
      "\ud83e\udd16 Non finetuned Model Prediction:\n",
      "   {\n",
      "  \"answer\": \"Kurt Weill\",\n",
      "  \"reasoning\": [\n",
      "    \"From evidence [3]: Der Lindberghflug is a Lehrst\u00fcck by Bertolt Brecht, premiered with music by Kurt Weill\",\n",
      "    \"From evidence [8]: Kurt Weill is a German composer known for his collaborations with Bertolt Brecht\"\n",
      "  ],\n",
      "  \"evidence\": [\n",
      "    3,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "Non finetuned Model Prediction finished\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcdd EXAMPLE 3\n",
      "================================================================================\n",
      "\u2753 Question: Who released the song \"With or Without You\" first, Jai McDowall or U2?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Provided Passages:\n",
      "   [1] U218 Singles: U218 Singles is a greatest hits album by the Irish rock band U2, released in November 2006.  In most markets, the album contains 18 songs.  The first 16 tracks are 16 of their most successful and popular singles.  The seventeenth track is a cover version, in collaboration with Green Day, of The Skids' \"The Saints Are Coming\" to benefit Hurricane Katrina charities.  The eighteenth and closing track is a new song called, \"Window in the Skies\".  However, in some markets, such as the United Kingdom, an extra song, \"I Will Follow\" is added to the track list as the opening track. \" U218 Videos\", a DVD featuring music videos from throughout U2's career, was released concurrently. have \n",
      "   [2] Heartbeat (The Fray song): \"Heartbeat\" is the first single from The Fray's third album \"Scars & Stories\".  The band premiered the song while opening for U2 on their U2 360\u00b0 Tour in May 2011.  The song was released for airplay on October 8, 2011, and was released for download in the United States on iTunes on October 11, 2011. have \n",
      "   [3] Bill Grainer: Bill Grainer is a Grammy certified American songwriter and producer.  He has written for such artists as Jai McDowall, Linda Eder, and Jennifer Hudson, with whom he co-wrote the song \"Stand Up\" for her Grammy Award-winning self-titled debut album. have \n",
      "   [4] Jai Ho! (You Are My Destiny): \"Jai Ho!  (You Are My Destiny)\" is a song performed by American girl group The Pussycat Dolls for their second studio album \"Doll Domination\" (2008).  It was released on February 23, 2009, by Interscope Records as the fourth single from the record.  After watching \"Slumdog Millionaire\" record executives Jimmy Iovine and Ron Fair wanted to turn \"Jai Ho\" into a \"pop record without deviating from the original melody\" and asked Scherzinger to do her own interpretation of the song.  The song was credited as \"\"A. R. Rahman and The Pussycat Dolls featuring Nicole Scherzinger\"\". have \n",
      "   [5] The Fly (song): \"The Fly\" is a song by Irish rock band U2.  It is the seventh track from their 1991 album, \"Achtung Baby\", and it was released as the album's first single on 21 October 1991.  \"The Fly\" introduced a more abrasive sounding U2, as the song featured danceable hip-hop beats, industrial textures, distorted vocals, and an elaborate guitar solo.  Lead vocalist Bono described the song as \"the sound of four men chopping down \"The Joshua Tree\",\" due to its departure from the traditional sound that had characterised the band in the 1980s. have \n",
      "   [6] Win (film): Win is a romance thriller trilingual film directed in three languages Hindi, Telugu & Tamil and written by Vinod Kumar assisisted by Sudarshanan.  Director Vinod Kumar is making his first directorial debut.  The film will be released under the banner of Rahmath Productions in Telugu & Jai Balaji Movie Makers in Tamil.  The film will feature Jai Akash alongside Angel Jitendra, Kavya, Nikita, Kousalya, Dinesh Nair, S. Ve.  Sheker, Ganja Karuppu, and various others.  Background score and soundtrack are composed by U. K. Murali audio is released in Telugu on 28 March 2013.  For the first time ever we have three music directors Shankar Ganesh, Deva, A. R. Reihana singing a song together for another music composer for this film.  Shooting for the film will be finished October 2013, and post-production works are also currently going on at Chennai & Hyderabad. have \n",
      "   [7] Brothers of the Road: Brothers of the Road is the eighth studio album, and tenth album overall, by the rock group the Allman Brothers Band.  Released in 1981, it is the band's only album without drummer Jai Johanny Johanson and the last album to feature bassist David Goldflies and guitarist Dan Toler and the only album to feature drummer David Toler.  The song \"Straight from the Heart\" was the group's third, and to date last, Top 40 hit. have \n",
      "   [8] With or Without You: \"With or Without You\" is a song by Irish rock band U2.  It is the third track from their fifth studio album, \"The Joshua Tree\" (1987), and was released as the album's lead single on 16 March 1987.  The song was the group's most successful single at the time, becoming their first number-one hit in both the United States and Canada by topping the \"Billboard\" Hot 100 for three weeks and the \"RPM\" national singles chart for one week, with a further three weeks at number two. have \n",
      "Prompt length: 5998\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'evidence'])\n",
      "\u274c Parsing error: 1 validation error for QAOutput\n",
      "citations\n",
      "  Field required [type=missing, input_value={'answer': 'U2', 'reasoni...usic\"], 'evidence': [8]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\ud83d\udd04 fallback_parse called...\n",
      "   Input: {     \"answer\": \"U2\",     \"reasoning\": [       \"From evidence [8]: 'With or Without You' is a song by Irish rock band U2\",       \"From evidence [5]: '...\n",
      "\u2705 fallback_parse: JSON OK\n",
      "\u2705 QAOutput created from JSON\n",
      "Generated answer length: 376\n",
      "\n",
      "\ud83e\udd16 Non finetuned Model Prediction:\n",
      "   {\n",
      "    \"answer\": \"U2\",\n",
      "    \"reasoning\": [\n",
      "      \"From evidence [8]: 'With or Without You' is a song by Irish rock band U2\",\n",
      "      \"From evidence [5]: 'The Fly' is a song by Irish rock band U2, but it was released after 'With or Without You'\",\n",
      "      \"Therefore, U2 released 'With or Without You' before Jai McDowall released any music\"\n",
      "    ],\n",
      "    \"evidence\": [\n",
      "      8\n",
      "    ]\n",
      "  }\n",
      "Non finetuned Model Prediction finished\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcdd EXAMPLE 4\n",
      "================================================================================\n",
      "\u2753 Question: What Kentucky county has a population of 60,316 and features the Lake Louisvilla neighborhood?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Provided Passages:\n",
      "   [1] Tara Conner: Tara Elizabeth Conner (born December 18, 1985) is an American actress, model, advocate and beauty queen who was crowned Miss USA 2006 and has also competed in the Miss Teen USA and Miss Universe pageants.  Apart from her role as Miss USA, Conner has been employed as a model.  She was a featured model on the HDNet series \"Bikini Destinations\" in 2004, posing in Lake Tahoe.  She has also held the titles Miss Kentucky Teen USA 2002, Miss Kentucky County Fair 2004, and Miss Kentucky USA 2006. have \n",
      "   [2] Benjamin Logan: Benjamin Logan (c.1742 \u2013 December 11, 1802) was an American pioneer, soldier, and politician from Shelby County, Kentucky.  As colonel of the Kentucky County, Virginia militia during the American Revolutionary War, he was second-in-command of all the militia in Kentucky.  He was also a leader in Kentucky's efforts to become a state.  His brother, John Logan, was the first state treasurer of Kentucky. have \n",
      "   [3] Casey County, Kentucky: Casey County is a county located in the U.S. Commonwealth of Kentucky.  As of the 2010 census, the population was 15,955.  Its county seat is Liberty.  The county was formed in 1806 from the western part of Lincoln County and named for Colonel William Casey, a pioneer settler who moved his family to Kentucky in 1779.  It is the only Kentucky county entirely in the Knobs region.  Casey County is home to annual Casey County Apple Festival, and is a prohibition or dry county.  It is considered part of the Appalachian region of Kentucky. have \n",
      "   [4] Lake Louisvilla, Louisville: Lake Louisvilla is a neighborhood partially located in Louisville, Kentucky.  It is located between Westport Road in Louisville and KY 22 in Oldham County.  Lake Louisvilla was developed in the 1920s as a summer resort for people living in the city of Louisville.  The state of Kentucky drained the lake in the late 1980s due to safety concerns regarding the stability of a dam. have \n",
      "   [5] Westervelt massacre: The Westervelt massacre, also known as the Westerfield massacre, was an indigenous attack on a caravan of Dutch frontier settlers that occurred during the American Revolutionary War around 3:00 am on June 27, 1780 in Kentucky County, Virginia, the present day state of Kentucky.  It remains one of the largest massacres in Kentucky state history.  The settlers were traveling southeast from Low Dutch Station to Harrod's Town.  The settler relocation was in part a reaction to British Captain Henry Bird's invasion of Kentucky.  The area immediately east of Low Dutch Station had been overrun with British allied Indians.  Harrod's Town was fortified and a move south would lead the settlers away from Captain Bird's invading army from the north.  The caravan was ambushed in a surprise attack, during the night, after a day's travel of twelve miles.  The exact location of the massacre is not definitively known.  However, it is likely to have occurred at Floyd's Fork and Broad Run.  The caravan was formed by Jacobus Westervelt and consisted of forty-one settlers from ten different families.  Ten of the seventeen settlers killed were members of the Westervelt family.  The victims included men, women, and children.  The Indians responsible for the massacre were allied to the British and received \u20a45 for each victim's scalp cut off and returned to the British authorities.  The Indians were thus awarded \u20a485 by the British for massacring the Dutch settlers.  The Westervelt Massacre had a chilling effect on the region.  A number of settlers from Low Dutch Station joined Colonel George Rogers Clark's militia after the massacre. have \n",
      "   [6] John Logan (pioneer): John Logan (1747July 1807) was a pioneer and politician from the U.S. state of Virginia and later, Kentucky.  He participated in Lord Dunmore's War in 1774, serving under his brother, Benjamin.  After moving to Kentucky County, Virginia, he took part in several expeditions against the Shawnee, including some led by Daniel Boone, John Bowman, and George Rogers Clark.  After Kentucky County was split into three counties, Logan represented his home county, Lincoln in the Virginia House of Delegates and at several of the conventions that effected the separation of Virginia from Kentucky. have \n",
      "   [7] Battle of Blue Licks: The Battle of Blue Licks, fought on August 19, 1782, was one of the last battles of the American Revolutionary War.  The battle occurred ten months after Lord Cornwallis's famous surrender at Yorktown, which had effectively ended the war in the east.  On a hill next to the Licking River in what is now Robertson County, Kentucky (but was then in Kentucky County, Virginia), a force of about 50 American and Canadian Loyalists along with 300 American Indians ambushed and routed 182 Kentucky militiamen.  It was the last victory for the Loyalists and Natives during the frontier war. have \n",
      "   [8] Kentucky County, Virginia: Kentucky County (then alternately spelled Kentucke County) was formed by the Commonwealth of Virginia by dividing Fincastle County into three new counties: Kentucky, Washington, and Montgomery, effective December 31, 1776.  During the three and one-half years of Kentucky County's existence, its seat of government was Harrodstown (then also known as Oldtown, later renamed Harrodsburg). have \n",
      "Prompt length: 6952\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning'])\n",
      "\u274c Parsing error: 1 validation error for QAOutput\n",
      "citations\n",
      "  Field required [type=missing, input_value={'answer': 'Insufficient ...ntext', 'reasoning': []}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\ud83d\udd04 fallback_parse called...\n",
      "   Input: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The given evidences do not provide enough context to determine which Kentucky county has a ...\n",
      "\u26a0\ufe0f fallback_parse: JSON failed, using regex\n",
      "\ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The given evidences do not provide enough context to determine which Kentucky county has a population of 60,316 and features the Lake Louisvi...\n",
      "\u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "\ud83d\udd0d Found JSON substring, parsing...\n",
      "\u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "\u2705 QAOutput created from regex\n",
      "Generated answer length: 417\n",
      "\n",
      "\ud83e\udd16 Non finetuned Model Prediction:\n",
      "   {\n",
      "  \"answer\": \"Insufficient context\",\n",
      "  \"reasoning\": []\n",
      "}\n",
      "\n",
      "The given evidences do not provide enough context to determine which Kentucky county has a population of 60,316 and features the Lake Louisvilla neighborhood. The evidences mainly focus on historical events and individuals related to Kentucky and Virginia, but they do not mention any specific county with the given population and the mentioned neighborhood.\n",
      "Non finetuned Model Prediction finished\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udcdd EXAMPLE 5\n",
      "================================================================================\n",
      "\u2753 Question: Para Hills West, South Australia lies within a city with what estimated population?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Provided Passages:\n",
      "   [1] Gulfview Heights, South Australia: Gulfview Heights is a small suburb of Adelaide, South Australia and is within the City of Salisbury and City of Tea Tree Gully local government area.  It is adjacent to Wynn Vale, Salisbury East and Para Hills. have \n",
      "   [2] Wentworth, New South Wales: Wentworth is a small border town in the far south west of the state of New South Wales, Australia.  It lies at the confluence of Australia's two most important rivers, the Darling and the Murray, the latter forming the border with the state of Victoria to the south.  The border with the state of South Australia lies approximately 100 km to the west.  The town of Wentworth is in the local government area of the same name. have \n",
      "   [3] Para Hills West, South Australia: Para Hills West is a suburb of Adelaide, South Australia, and is within the City of Salisbury.  It is on the eastern side of Main North Road, opposite Parafield Airport.  The other boundaries are McIntyre Road, Bridge Road and Maxwell Road. have \n",
      "   [4] Para Hills Knights SC: Para Hills Knights SC are a soccer club based in Para Hills, South Australia.  The club competes in the FFSA National Premier League.  The Para Hills Knights home ground is at The Paddocks in Para Hills, north of Adelaide.  They currently have some of the strongest sides in the Junior Premier League and senior league.  2012 was a very successful year for the club gaining promotion to the top league in South Australia by winning the premier league competition.  The club also gained a place in the cup after beating some strong super league sides.  The under 19's team also finished top of their league and the reserves had a successful season, finishing the 2006 Premier League season on top earning promotion to the 2007 South Australian Super League.  The Knights recently recently regained Premier League status following promotion from the State League in 2016. have \n",
      "   [5] Division of Makin: The Division of Makin is an electoral division for the Australian House of Representatives located in the northeastern suburbs of Adelaide.  The 130\u00a0km\u00b2 seat covers an area from Little Para River and Gould Creek in the north-east to Grand Junction Road in the south and Port Wakefield Road in the west, including the suburbs of Banksia Park, Fairview Park, Golden Grove, Greenwith, Gulfview Heights, Ingle Farm, Mawson Lakes, Modbury, Para Hills, Para Vista, Pooraka, Redwood Park, Ridgehaven, Salisbury East, Salisbury Heights, St Agnes, Surrey Downs, Tea Tree Gully, Valley View, Vista, Walkley Heights, Wynn Vale, Yatala Vale, and parts of Gepps Cross and Hope Valley. have \n",
      "   [6] Wynn Vale, South Australia: Wynn Vale is an outer north-eastern suburb of Adelaide, South Australia and is located within the City of Tea Tree Gully local government area.  It is adjacent to Golden Grove, Modbury Heights, Surrey Downs, Salisbury East and Para Hills.  It is located approximately 20\u00a0km north-east of the city of Adelaide. have \n",
      "   [7] Electoral district of Playford: Playford is an electoral district of the House of Assembly in the Australian state of South Australia.  Named after the long serving South Australian premier Tom Playford, it is a 22.7\u00a0km\u00b2 urban electorate in Adelaide's north, taking in the suburbs of Ingle Farm, Para Hills West and Walkley Heights as well as parts of Gepps Cross, Gulfview Heights, Northfield, Para Hills, Para Vista, Pooraka and Valley View. have \n",
      "   [8] Para Hills, South Australia: Para Hills is a residential suburb of Adelaide, South Australia.  There is a light aircraft airport close to its boundary, and numerous sporting facilities, abundant parks and schools and two medium-sized shopping centres.  Most of the suburb is in the City of Salisbury while some is in the City of Tea Tree Gully. have \n",
      "Prompt length: 5436\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['answer', 'reasoning', 'evidence'])\n",
      "\u274c Parsing error: 1 validation error for QAOutput\n",
      "citations\n",
      "  Field required [type=missing, input_value={'answer': 'The populatio...1, 2, 3, 4, 5, 6, 7, 8]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\ud83d\udd04 fallback_parse called...\n",
      "   Input: {   \"answer\": \"The population of the city where Para Hills West is located is not directly mentioned in the given evidences.\",   \"reasoning\": [],   \"e...\n",
      "\u26a0\ufe0f fallback_parse: JSON failed, using regex\n",
      "\ud83d\udd0d Extracting from: {   \"answer\": \"The population of the city where Para Hills West is located is not directly mentioned in the given evidences.\",   \"reasoning\": [],   \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8] }  Insufficient...\n",
      "\u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 188). Trying fallback...\n",
      "\ud83d\udd0d Found JSON substring, parsing...\n",
      "\u2705 Substring parse OK: answer='The population of the city where Para Hills West i...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "\u2705 QAOutput created from regex\n",
      "Generated answer length: 209\n",
      "\n",
      "\ud83e\udd16 Non finetuned Model Prediction:\n",
      "   {\n",
      "  \"answer\": \"The population of the city where Para Hills West is located is not directly mentioned in the given evidences.\",\n",
      "  \"reasoning\": [],\n",
      "  \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "}\n",
      "\n",
      "Insufficient context.\n",
      "Non finetuned Model Prediction finished\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d Debugging examples displayed. Analyze the outputs above to identify patterns in errors.\n"
     ]
    }
   ],
   "source": [
    "# Debugging Low Scores: Display Examples\n",
    "print(\"\ud83d\udd0d Debugging Low Scores: Inspecting Model Outputs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# print the model used for inference\n",
    "print(\"model config: \", model.config)\n",
    "\n",
    "\n",
    "# Select a few examples from the evaluation dataset\n",
    "num_debug_examples = 5  # You can adjust this number\n",
    "debug_examples = eval_dataset.select(range(min(num_debug_examples, len(eval_dataset))))\n",
    "\n",
    "print(f\"\ud83d\udcdd Displaying {len(debug_examples)} examples from the evaluation set:\")\n",
    "\n",
    "for i, example in enumerate(debug_examples):\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"\ud83d\udcdd EXAMPLE {i+1}\")\n",
    "    print(f\"=\"*80)\n",
    "\n",
    "    print(f\"\u2753 Question: {example['question']}\")\n",
    "    print(f\"\u2705 Gold Answer: {example['answer']}\")\n",
    "\n",
    "    print(f\"\\n\ud83d\udcda Provided Passages:\")\n",
    "    for j, passage in enumerate(example['passages'], 1):\n",
    "        print(f\"   [{j}] {passage['title']}: {passage['text']} have \")\n",
    "\n",
    "    # Get the fine-tuned model's prediction for this example\n",
    "    chain_of_thought_prediction = generate_answer(example['question'], example['passages'], building_prompts_rag, model)\n",
    "\n",
    "    print(f\"\\n\ud83e\udd16 Non finetuned Model Prediction:\")\n",
    "    print(f\"   {chain_of_thought_prediction}\")\n",
    "    print(f\"Non finetuned Model Prediction finished\")\n",
    "\n",
    "    # You can manually compare the \"Gold Answer\" and \"Fine-tuned Model Prediction\"\n",
    "    # to understand discrepancies and potential issues.\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"\ud83d\udd0d Debugging examples displayed. Analyze the outputs above to identify patterns in errors.\")\n",
    "#   [2] Oliver Reed: Robert Oliver Reed (13 February 1938 \u2013 2 May 1999) was an English actor known for his upper-middle class, macho image, hellraiser lifestyle,\n",
    "#and \"tough guy\" roles.  Notable films include \"The Trap\" (1966), \"Oliver! \" (1968), \"Women in Love\" (1969), \"Hannibal Brooks\" (1969), \"The Devils\" (1971),\n",
    "#\"The Three Musketeers\" (1973), \"Tommy\" (1975), \"Lion of the Desert\" (1981), \"Castaway\" (1986), \"The Adventures of Baron Munchausen\" (1988) and \"Funny Bones\" (1995).\n",
    "# For \"Gladiator\" (2000), his final film, Reed was posthumously nominated for the BAFTA Award for Best Actor in a Supporting Role. have\n",
    "\n",
    "\n",
    "#We need to ensure the validation process is correct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-kb9MJTx_epa",
   "metadata": {
    "id": "-kb9MJTx_epa"
   },
   "source": [
    "Turn this into markdown: As we could see that prompting the model even with clear instructions and exemplars for in-context learning, the model still struggles to follow the pattern to answer the question. For instance, we want direct answer without explanation, but the model struggles on this. Morever, it seems the model finds it difficult to know when the context given to it is in-sufficient for answering the question. The citation are also none complete. Before considering domain-specific instruction tuning or supervised finetuning, lets try to fully evaluate the prompt approach given its simplicity.\n",
    "\n",
    "\n",
    "\n",
    "This example demonstrate the difficulty of controlling model output style:\n",
    "\n",
    "\n",
    "Question: Who released the song \"With or Without You\" first, Jai McDowall or U2?\n",
    " Gold Answer: U2 [5, 8]\n",
    "Model Prediction:\n",
    "   Answer: U2 released the song \"With or Without You\" first.\n",
    "Reasoning:\n",
    "From evidence [5], U2 released the song \"With or Without You\" as the lead single from their fifth studio album \"The Joshua Tree\" in 1987.\n",
    "From evidence [8], Jai McDowall released a promotional single of the same name, \"With or Without You,\" from his debut album \"Believe\" in 2011.\n",
    "Therefore, U2's release of the song predates Jai McDowall's by over 14 years.\n",
    "Evidence: [5], [8]\n",
    "\n",
    "Comments: As you can see the answer of ground truth is U2 but yours is not direct, i want direct anwer like U2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xuf3e1duy3o",
   "metadata": {
    "id": "xuf3e1duy3o"
   },
   "source": [
    "### Baseline Performance Metrics\n",
    "\n",
    "Comprehensive evaluation of baseline model on evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3x2jvhzfkt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221728,
     "status": "ok",
     "timestamp": 1760026919885,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "a3x2jvhzfkt",
    "outputId": "3f798998-e96f-4fdd-e3aa-642f6a118f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Starting baseline evaluation with RAG prompting approach...\n",
      "   Model: Mistral-7B-Instruct (base, no fine-tuning)\n",
      "   Strategy: RAG with few-shot exemplars\n",
      "   Dataset: First 100 examples from eval_dataset\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d Evaluating Baseline RAG Prompting on 200 examples...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 1 ---\n",
      "Question: What nationality was Oliver Reed's character in the film Royal Flash?...\n",
      "[Ex 0] \ud83d\udd0d Extracting from: {   \"answer\": \"Otto von Bismarck (German)\",   \"reasoning\": [     \"From evidence [1]: Oliver Reed acted as Otto von Bismarck in the film 'Royal Flash'\",     \"From historical evidence [5]: Otto von Bism...\n",
      "[Ex 0] \u2705 JSON parse OK: answer='Otto von Bismarck (German)...', citations=[1, 5]\n",
      "[Ex 0] \ud83d\udd0d Extracting from: {   \"reasoning\": \"From evidence [23]: Sacramento International Airport is located 10 mi northwest of downtown Sacramento, in Sacramento County, California From evidence [26]: Knox County Regional Airp...\n",
      "[Ex 0] \u2705 JSON parse OK: answer='Prussian...', citations=[1, 5]\n",
      "Predicted: Otto von Bismarck (German)\n",
      "Gold: Prussian\n",
      "Pred Citations: [1, 5]\n",
      "Gold Citations: [1, 5]\n",
      "F1: 0.000, EM: 0.000, Citation F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   0%|          | 1/200 [00:25<1:23:12, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 2 ---\n",
      "Question: Pacific Mozart Ensemble performed which German composer's Der Lindberghflug in 2002?...\n",
      "[Ex 1] \ud83d\udd0d Extracting from: 18th century English novel by Henry Mackenzie, \"The Man of Feeling\".  The play was written for the Berliner Ensemble and premiered on 3 April 1950.  The play's themes include the nature of morality an...\n",
      "[Ex 1] \u26a0\ufe0f  JSON parse failed: Extra data: line 1 column 3 (char 2). Trying fallback...\n",
      "[Ex 1] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 1] \ud83d\udcdd Regex result: answer='18th century English novel by Henry Mackenzie, \"Th...', citations=[]\n",
      "[Ex 1] \ud83d\udd0d Extracting from: {   \"reasoning\": \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell From evidence [8]: Russell Hobbs is a manufacturer of household applian...\n",
      "[Ex 1] \u2705 JSON parse OK: answer='Kurt Julian Weill...', citations=[1, 8]\n",
      "Predicted: 18th century English novel by Henry Mackenzie, \"The Man of Feeling\".  The play was written for the B...\n",
      "Gold: Kurt Julian Weill\n",
      "Pred Citations: []\n",
      "Gold Citations: [1, 8]\n",
      "F1: 0.000, EM: 0.000, Citation F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   1%|          | 2/200 [01:43<3:05:25, 56.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 3 ---\n",
      "Question: Who released the song \"With or Without You\" first, Jai McDowall or U2?...\n",
      "[Ex 2] \ud83d\udd0d Extracting from: {     \"answer\": \"U2\",     \"reasoning\": [       \"From evidence [8]: With or Without You is a song by Irish rock band U2\",       \"From evidence [5]: The Fly is a song by Irish rock band U2, but it was r...\n",
      "[Ex 2] \u2705 JSON parse OK: answer='U2...', citations=[1, 8]\n",
      "[Ex 2] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 2] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "Predicted: U2\n",
      "Gold: insufficient context\n",
      "Pred Citations: [1, 8]\n",
      "Gold Citations: []\n",
      "F1: 0.000, EM: 0.000, Citation F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   2%|\u258f         | 3/200 [02:23<2:40:46, 48.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 4 ---\n",
      "Question: What Kentucky county has a population of 60,316 and features the Lake Louisvilla neighborhood?...\n",
      "[Ex 3] \ud83d\udd0d Extracting from: {   \"answer\": \"Kentucky County (now Kentucky, USA)\",   \"reasoning\": [     \"From evidence [5]: The Westervelt massacre occurred in Kentucky County, Virginia\",     \"From evidence [8]: Kentucky County, V...\n",
      "[Ex 3] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 451). Trying fallback...\n",
      "[Ex 3] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 3] \u2705 Substring parse OK: answer='Kentucky County (now Kentucky, USA)...', citations=[5, 8]\n",
      "[Ex 3] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 3] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "Predicted: Kentucky County (now Kentucky, USA)\n",
      "Gold: insufficient context\n",
      "Pred Citations: [5, 8]\n",
      "Gold Citations: []\n",
      "F1: 0.000, EM: 0.000, Citation F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   2%|\u258f         | 4/200 [02:59<2:23:32, 43.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 5 ---\n",
      "Question: Para Hills West, South Australia lies within a city with what estimated population?...\n",
      "[Ex 4] \ud83d\udd0d Extracting from: {   \"answer\": \"The population of the city where Para Hills West is located is not directly stated in the given evidences.\",   \"reasoning\": [],   \"evidence\": [1, 3, 5, 6, 7] }  Insufficient context. Th...\n",
      "[Ex 4] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 176). Trying fallback...\n",
      "[Ex 4] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 4] \u2705 Substring parse OK: answer='The population of the city where Para Hills West i...', citations=[1, 3, 5, 6, 7]\n",
      "[Ex 4] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 4] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "Predicted: The population of the city where Para Hills West is located is not directly stated in the given evidences.\n",
      "Gold: insufficient context\n",
      "Pred Citations: [1, 3, 5, 6, 7]\n",
      "Gold Citations: []\n",
      "F1: 0.000, EM: 0.000, Citation F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   2%|\u258e         | 5/200 [03:22<1:57:29, 36.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 5] \ud83d\udd0d Extracting from: {     \"answer\": \"Hugh Laurie was born on 11 June 1959\",     \"reasoning\": [],     \"evidence\": [5] }...\n",
      "[Ex 5] \u2705 JSON parse OK: answer='Hugh Laurie was born on 11 June 1959...', citations=[5]\n",
      "[Ex 5] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"1959\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 5] \u2705 JSON parse OK: answer='1959...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   3%|\u258e         | 6/200 [03:34<1:30:48, 28.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 6] \ud83d\udd0d Extracting from: {     \"answer\": \"October 26, 1881\",     \"reasoning\": [       \"From evidence [2] and [8]: The Gunfight at the O.K. Corral took place on October 26, 1881\",       \"Therefore, the date of the gunfight is ...\n",
      "[Ex 6] \u2705 JSON parse OK: answer='October 26, 1881...', citations=[2, 8]\n",
      "[Ex 6] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 6] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   4%|\u258e         | 7/200 [04:04<1:32:09, 28.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 7] \ud83d\udd0d Extracting from: {   \"answer\": [\"Gippy Grewal\", \"Smeep Kang\"],   \"reasoning\": [     \"From evidence [3] and [6]: Smeep Kang directed the Punjabi films 'Lucky Di Unlucky Story' in 2013 and 'Lock' in 2016, both featuring...\n",
      "[Ex 7] \u2705 JSON parse OK: answer='Gippy Grewal, Smeep Kang...', citations=[3, 6, 7]\n",
      "[Ex 7] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 7] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   4%|\u258d         | 8/200 [04:45<1:44:18, 32.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 8] \ud83d\udd0d Extracting from: {     \"answer\": \"Charlie Murphy\",     \"reasoning\": [       \"From evidence [1]: Twisted Fortune is a comedy about a character played by Charlie Murphy\",       \"Therefore, Charlie Murphy was in Twisted ...\n",
      "[Ex 8] \u2705 JSON parse OK: answer='Charlie Murphy...', citations=[1]\n",
      "[Ex 8] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Charlie Murphy\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 8] \u2705 JSON parse OK: answer='Charlie Murphy...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   4%|\u258d         | 9/200 [05:05<1:31:41, 28.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 9] \ud83d\udd0d Extracting from: {     \"answer\": \"Bill Dudman\",     \"reasoning\": [       \"From evidence [1]: Dudman was Industrial Chaplain to the Bishop of Lincoln from 1957 to 1971\"     ],     \"evidence\": [       1     ]   }...\n",
      "[Ex 9] \u2705 JSON parse OK: answer='Bill Dudman...', citations=[1]\n",
      "[Ex 9] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Dudman\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 9] \u2705 JSON parse OK: answer='Dudman...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   5%|\u258c         | 10/200 [05:26<1:23:22, 26.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 10] \ud83d\udd0d Extracting from: {   \"answer\": \"Sail On\",   \"reasoning\": [     \"From evidence [3]: Sail On: The 30th Anniversary Collection is the fifth compilation album from the band Kansas, and the first two words of its title are...\n",
      "[Ex 10] \u2705 JSON parse OK: answer='Sail On...', citations=[3]\n",
      "[Ex 10] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 10] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   6%|\u258c         | 11/200 [05:48<1:18:45, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 11] \ud83d\udd0d Extracting from: {     \"answer\": \"The Twelfth United States Army Group, not specified which chairman\",     \"reasoning\": [],     \"evidence\": [4] }  The given evidence does not provide information on who was the first c...\n",
      "[Ex 11] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 130). Trying fallback...\n",
      "[Ex 11] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 11] \u2705 Substring parse OK: answer='The Twelfth United States Army Group, not specifie...', citations=[4]\n",
      "[Ex 11] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Joint Chiefs of Staff\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 11] \u2705 JSON parse OK: answer='Joint Chiefs of Staff...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   6%|\u258c         | 12/200 [06:10<1:15:27, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 12] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  There is no evidence provided about Lyman Sherwood's birthplace....\n",
      "[Ex 12] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 12] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 12] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 12] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"Rensselaer County\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 12] \u2705 JSON parse OK: answer='Rensselaer County...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   6%|\u258b         | 13/200 [06:21<1:02:24, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 13] \ud83d\udd0d Extracting from: {   \"answer\": \"William Grove Skelly, Chesley Coleman Herndon, and Frederick A. Pielsticker\",   \"reasoning\": [     \"From evidence [1]: Skelly Oil was founded by William Grove Skelly, Chesley Coleman He...\n",
      "[Ex 13] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 271). Trying fallback...\n",
      "[Ex 13] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 13] \u2705 Substring parse OK: answer='William Grove Skelly, Chesley Coleman Herndon, and...', citations=[1]\n",
      "[Ex 13] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 13] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   7%|\u258b         | 14/200 [07:05<1:24:56, 27.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 14] \ud83d\udd0d Extracting from: {     \"answer\": \"Axl Rose\",     \"reasoning\": [       \"From evidence [4]: 'November Rain' is a power ballad by the American hard rock band Guns N' Roses.\",       \"From evidence [4]: Axl Rose is the lea...\n",
      "[Ex 14] \u2705 JSON parse OK: answer='Axl Rose...', citations=[4]\n",
      "[Ex 14] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 14] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   8%|\u258a         | 15/200 [07:31<1:22:48, 26.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 15] \ud83d\udd0d Extracting from: { \"answer\": \"Tenerife airport disaster occurred first\", \"reasoning\": [ \"From evidence [3] and [4]: Tenerife airport disaster is the deadliest aviation accident in history and Jacob Veldhuyzen van Zant...\n",
      "[Ex 15] \u2705 JSON parse OK: answer='Tenerife airport disaster occurred first...', citations=[3, 4]\n",
      "[Ex 15] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"On March 27, 1977, two Boeing 747 passenger jets, KLM Flight 4805 and Pan Am Flight 1736, collided on the runway at Los R...\n",
      "[Ex 15] \u2705 JSON parse OK: answer='On March 27, 1977, two Boeing 747 passenger jets, ...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   8%|\u258a         | 16/200 [08:08<1:31:31, 29.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 16] \ud83d\udd0d Extracting from: {     \"answer\": \"No\",     \"reasoning\": [       \"From evidence [1] and [3]: Michael Bubl\u00e9's sixth studio album 'Crazy Love' was released on October 9, 2009\",       \"From evidence [6]: Welcome to Nollyw...\n",
      "[Ex 16] \u2705 JSON parse OK: answer='No...', citations=[1, 3, 6]\n",
      "[Ex 16] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 16] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   8%|\u258a         | 17/200 [08:41<1:34:44, 31.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 17] \ud83d\udd0d Extracting from: {   \"answer\": \"None of the evidences provide information about the birthplace of Duffy Jackson.\",   \"reasoning\": [],   \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8] }  Insufficient context....\n",
      "[Ex 17] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 159). Trying fallback...\n",
      "[Ex 17] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 17] \u2705 Substring parse OK: answer='None of the evidences provide information about th...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 17] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 17] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:   9%|\u2589         | 18/200 [09:00<1:22:59, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 18] \ud83d\udd0d Extracting from: {     \"answer\": \"Ole Einar Bj\u00f8rndalen\",     \"reasoning\": [       \"From evidence [5]: Defending titlist for 2008-09 Biathlon World Cup \u2013 Pursuit Men is Ole Einar Bj\u00f8rndalen\",       \"The question asks f...\n",
      "[Ex 18] \u26a0\ufe0f  JSON parse failed: Extra data: line 13 column 1 (char 403). Trying fallback...\n",
      "[Ex 18] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 18] \u2705 Substring parse OK: answer='Ole Einar Bj\u00f8rndalen...', citations=[5]\n",
      "[Ex 18] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 18] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  10%|\u2589         | 19/200 [09:57<1:49:17, 36.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 19] \ud83d\udd0d Extracting from: {     \"answer\": \"Insufficient context\",     \"reasoning\": [] }  The given evidences do not provide enough context to deduce which mountain is taller between Gasherbrum II and Langtang Ri. Both mountain...\n",
      "[Ex 19] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 63). Trying fallback...\n",
      "[Ex 19] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 19] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 19] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Gasherbrum II\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 19] \u2705 JSON parse OK: answer='Gasherbrum II...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  10%|\u2588         | 20/200 [10:20<1:36:49, 32.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 20] \ud83d\udd0d Extracting from: {     \"answer\": \"John de Mol\",     \"reasoning\": [       \"From evidence [7] and [8]: John de Mol is a Dutch media tycoon and the founder of Talpa Holding\",       \"Therefore, John de Mol is the Dutch me...\n",
      "[Ex 20] \u2705 JSON parse OK: answer='John de Mol...', citations=[7, 8]\n",
      "[Ex 20] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"John de Mol Jr.\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 20] \u2705 JSON parse OK: answer='John de Mol Jr....', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  10%|\u2588         | 21/200 [10:46<1:30:29, 30.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 21] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context.\",   \"reasoning\": [] }  The given evidences do not provide any context regarding the car depicted on the cover of Pentastar: In the Style of Demons or when it cease...\n",
      "[Ex 21] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 60). Trying fallback...\n",
      "[Ex 21] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 21] \u2705 Substring parse OK: answer='Insufficient context....', citations=[]\n",
      "[Ex 21] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"1974\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 21] \u2705 JSON parse OK: answer='1974...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  11%|\u2588         | 22/200 [11:02<1:17:06, 25.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 22] \ud83d\udd0d Extracting from: {     \"answer\": \"Rachel Maddow and Michael Pollan\",     \"reasoning\": [       \"From evidence [6]: Experts on non-profit law have questioned the validity of CORE's non-profit status\",       \"From eviden...\n",
      "[Ex 22] \u2705 JSON parse OK: answer='Rachel Maddow and Michael Pollan...', citations=[6]\n",
      "[Ex 22] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Rachel Anne Maddow\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 22] \u2705 JSON parse OK: answer='Rachel Anne Maddow...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  12%|\u2588\u258f        | 23/200 [11:31<1:19:55, 27.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 23] \ud83d\udd0d Extracting from: {   \"answer\": \"Robert Wise\",   \"reasoning\": [     \"From evidence [2]: Robert Wise won two Academy Awards for Best Director and Best Picture\",     \"No evidence was provided about the number of award no...\n",
      "[Ex 23] \u26a0\ufe0f  JSON parse failed: Extra data: line 12 column 1 (char 262). Trying fallback...\n",
      "[Ex 23] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 23] \u2705 Substring parse OK: answer='Robert Wise...', citations=[2]\n",
      "[Ex 23] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 23] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  12%|\u2588\u258f        | 24/200 [12:04<1:24:36, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 24] \ud83d\udd0d Extracting from: {   \"answer\": \"1985\",   \"reasoning\": [     \"From evidence [8]: Studio Ghibli was founded in 1985\"   ],   \"evidence\": [     8   ] }...\n",
      "[Ex 24] \u2705 JSON parse OK: answer='1985...', citations=[8]\n",
      "[Ex 24] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 24] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  12%|\u2588\u258e        | 25/200 [12:21<1:13:38, 25.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 25] \ud83d\udd0d Extracting from: { \"answer\": \"Insufficient context\", \"reasoning\": [] }  None of the given evidences provide any information about an English local newspaper changing names or even being mentioned in relation to the Fo...\n",
      "[Ex 25] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 55). Trying fallback...\n",
      "[Ex 25] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 25] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 25] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 25] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  13%|\u2588\u258e        | 26/200 [12:37<1:05:19, 22.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 26] \ud83d\udd0d Extracting from: . [9] Title: Pablo Escobar - Pablo Emilio Escobar Gaviria (December 1, 1949 \u2013 December 2, 1993), also known as \"El Patr\u00f3n\" and \"El Padrino\", was a Colombian drug lord and the founder of the Medell\u00edn C...\n",
      "[Ex 26] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 26] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 26] \ud83d\udcdd Regex result: answer='....', citations=[]\n",
      "[Ex 26] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"1970s and 1980s\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 26] \u2705 JSON parse OK: answer='1970s and 1980s...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  14%|\u2588\u258e        | 27/200 [13:56<1:53:38, 39.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 27] \ud83d\udd0d Extracting from: {     \"answer\": \"Agatha Christie\",     \"reasoning\": [],     \"evidence\": [       3,       5     ]   }  The evidence provides the birth and death years of Clement Greenberg and Agatha Christie respectiv...\n",
      "[Ex 27] \u26a0\ufe0f  JSON parse failed: Extra data: line 10 column 1 (char 102). Trying fallback...\n",
      "[Ex 27] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 27] \u2705 Substring parse OK: answer='Agatha Christie...', citations=[3, 5]\n",
      "[Ex 27] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Clement Greenberg\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 27] \u2705 JSON parse OK: answer='Clement Greenberg...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  14%|\u2588\u258d        | 28/200 [14:28<1:46:18, 37.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 28] \ud83d\udd0d Extracting from: { \"answer\": \"Arthur Noss was a British World War I flying ace and served as Keith Park's gunner during two battles, but the sources do not provide information on which two European theatre battles in ...\n",
      "[Ex 28] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 293). Trying fallback...\n",
      "[Ex 28] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 28] \u2705 Substring parse OK: answer='Arthur Noss was a British World War I flying ace a...', citations=[1, 7]\n",
      "[Ex 28] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"Battle of Britain and the Battle of Malta\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 28] \u2705 JSON parse OK: answer='Battle of Britain and the Battle of Malta...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  14%|\u2588\u258d        | 29/200 [14:52<1:34:38, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 29] \ud83d\udd0d Extracting from: {     \"answer\": \"Alopecurus\",     \"reasoning\": [       \"From evidence [2]: Alopecurus is a common and widespread genus of plants in the grass family, common across temperate and subtropical parts of E...\n",
      "[Ex 29] \u2705 JSON parse OK: answer='Alopecurus...', citations=[2]\n",
      "[Ex 29] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [3].\",   \"answer\": \"Alopecurus\",   \"citations\": [     2,     3   ] }...\n",
      "[Ex 29] \u2705 JSON parse OK: answer='Alopecurus...', citations=[2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  15%|\u2588\u258c        | 30/200 [15:16<1:25:49, 30.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 30] \ud83d\udd0d Extracting from: {     \"answer\": \"Peter M\u00e6rsk M\u00f8ller\",     \"reasoning\": [       \"From evidence [1]: A.P. M\u00f8ller is the father of M\u00e6rsk Mc-Kinney M\u00f8ller\",       \"From evidence [1]: A.P. M\u00f8ller's father is Peter M\u00e6rsk M...\n",
      "[Ex 30] \u2705 JSON parse OK: answer='Peter M\u00e6rsk M\u00f8ller...', citations=[1]\n",
      "[Ex 30] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"A.P. M\\u00f8ller\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 30] \u2705 JSON parse OK: answer='A.P. M\u00f8ller...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  16%|\u2588\u258c        | 31/200 [15:43<1:23:02, 29.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 31] \ud83d\udd0d Extracting from: {     \"answer\": \"Samantha Cristoforetti\",     \"reasoning\": [       \"From evidence [1] and [2]: Samantha Cristoforetti was the first person to drink espresso coffee in space on 3 May 2015\",       \"Ther...\n",
      "[Ex 31] \u2705 JSON parse OK: answer='Samantha Cristoforetti...', citations=[1, 2]\n",
      "[Ex 31] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [8].\",   \"answer\": \"Samantha Cristoforetti\",   \"citations\": [     2,     8   ] }...\n",
      "[Ex 31] \u2705 JSON parse OK: answer='Samantha Cristoforetti...', citations=[2, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  16%|\u2588\u258c        | 32/200 [16:12<1:22:20, 29.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 32] \ud83d\udd0d Extracting from: {     \"answer\": \"Park Ye-jin\",     \"reasoning\": [       \"From evidence [2]: I Love Lee Tae-ri is a 2012 South Korean romantic comedy series that starred Kim Ki-bum and Park Ye-jin\"     ],     \"evidenc...\n",
      "[Ex 32] \u2705 JSON parse OK: answer='Park Ye-jin...', citations=[2]\n",
      "[Ex 32] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"Park Ye-jin\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 32] \u2705 JSON parse OK: answer='Park Ye-jin...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  16%|\u2588\u258b        | 33/200 [16:34<1:14:59, 26.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 33] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [],   \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8] }  The question asks for a port city approximately 25 km north of the Lingnan Fine Arts Museum. However,...\n",
      "[Ex 33] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 99). Trying fallback...\n",
      "[Ex 33] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 33] \u2705 Substring parse OK: answer='Insufficient context...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 33] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Keelung\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 33] \u2705 JSON parse OK: answer='Keelung...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  17%|\u2588\u258b        | 34/200 [17:02<1:15:58, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 34] \ud83d\udd0d Extracting from: {     \"answer\": \"Olathe East High School\",     \"reasoning\": [       \"From evidence [4]: Tyler Kalinoski is a basketball player who attended Olathe East High School\"     ],     \"evidence\": [       4   ...\n",
      "[Ex 34] \u2705 JSON parse OK: answer='Olathe East High School...', citations=[4]\n",
      "[Ex 34] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"located in Olathe, Kansas\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 34] \u2705 JSON parse OK: answer='located in Olathe, Kansas...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  18%|\u2588\u258a        | 35/200 [17:20<1:07:28, 24.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 35] \ud83d\udd0d Extracting from: {   \"answer\": \"D1NZ is a production car drifting series in New Zealand\",   \"reasoning\": [],   \"evidence\": [     6   ] }...\n",
      "[Ex 35] \u2705 JSON parse OK: answer='D1NZ is a production car drifting series in New Ze...', citations=[6]\n",
      "[Ex 35] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"Drifting\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 35] \u2705 JSON parse OK: answer='Drifting...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  18%|\u2588\u258a        | 36/200 [17:33<57:52, 21.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 36] \ud83d\udd0d Extracting from: {   \"answer\": \"There is no direct answer to the question from the provided evidences.\",   \"reasoning\": [     \"The evidences do not contain any information about the most famous song of the last monarc...\n",
      "[Ex 36] \u2705 JSON parse OK: answer='There is no direct answer to the question from the...', citations=[]\n",
      "[Ex 36] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"Aloha \\u02bbOe\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 36] \u2705 JSON parse OK: answer='Aloha \u02bbOe...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  18%|\u2588\u258a        | 37/200 [17:53<56:35, 20.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 37] \ud83d\udd0d Extracting from: {     \"answer\": \"Approximately 3 miles\",     \"reasoning\": [       \"From evidence [8]: Gamston is approximately 3 mi south-east of Nottingham\",       \"The NG postcode area, which includes Gamston, is g...\n",
      "[Ex 37] \u2705 JSON parse OK: answer='Approximately 3 miles...', citations=[5, 8]\n",
      "[Ex 37] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"approximately 3 mi\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 37] \u2705 JSON parse OK: answer='approximately 3 mi...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  19%|\u2588\u2589        | 38/200 [18:17<58:51, 21.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 38] \ud83d\udd0d Extracting from: {     \"answer\": \"Tom Waits\",     \"reasoning\": [       \"From evidence [4]: The Boarding House was a music nightclub where several artists recorded their albums, including Tom Waits\"     ],     \"evidenc...\n",
      "[Ex 38] \u2705 JSON parse OK: answer='Tom Waits...', citations=[4]\n",
      "[Ex 38] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Steve Martin\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 38] \u2705 JSON parse OK: answer='Steve Martin...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  20%|\u2588\u2589        | 39/200 [18:36<55:40, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 39] \ud83d\udd0d Extracting from: { \"answer\": \"10 October 2010\", \"reasoning\": [ \"From evidence [1]: The Netherlands Antilles was dissolved on 10 October 2010.\", \"Therefore, the Cura\u00e7ao Centre for Correction and Detention changed its n...\n",
      "[Ex 39] \u26a0\ufe0f  JSON parse failed: Extra data: line 13 column 1 (char 335). Trying fallback...\n",
      "[Ex 39] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 39] \u2705 Substring parse OK: answer='10 October 2010...', citations=[1]\n",
      "[Ex 39] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 39] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  20%|\u2588\u2588        | 40/200 [19:50<1:38:20, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 40] \ud83d\udd0d Extracting from: in North American professional sports history.  Since 1980, the Lakers have won 16 NBA titles, more than any other team in the league. [7] Title: Russell Hobbs - Russell Hobbs is a British manufacture...\n",
      "[Ex 40] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 40] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 40] \ud83d\udcdd Regex result: answer='Orlando Magic, Houston Rockets...', citations=[]\n",
      "[Ex 40] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Eastern Conference champion Orlando Magic against the Western Conference champion Houston Rockets.\",   \"citations\": [    ...\n",
      "[Ex 40] \u2705 JSON parse OK: answer='Eastern Conference champion Orlando Magic against ...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  20%|\u2588\u2588        | 41/200 [21:08<2:10:11, 49.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 41] \ud83d\udd0d Extracting from: {   \"answer\": \"Ballarat Bitter\",   \"reasoning\": [     \"From evidence [3]: Ballarat Bitter is an Australian beer\",     \"From evidence [3]: Ballarat Bitter is a British style pale ale\",     \"From eviden...\n",
      "[Ex 41] \u2705 JSON parse OK: answer='Ballarat Bitter...', citations=[3]\n",
      "[Ex 41] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 41] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  21%|\u2588\u2588        | 42/200 [21:37<1:53:38, 43.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 42] \ud83d\udd0d Extracting from: {     \"answer\": \"Mikael Stanne\",     \"reasoning\": [       \"From evidence [2]: Mikael Stanne first appeared as the lead vocalist of Dark Tranquillity in their second full-length studio album\",       \"T...\n",
      "[Ex 42] \u26a0\ufe0f  JSON parse failed: Extra data: line 12 column 1 (char 304). Trying fallback...\n",
      "[Ex 42] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 42] \u2705 Substring parse OK: answer='Mikael Stanne...', citations=[2]\n",
      "[Ex 42] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 42] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  22%|\u2588\u2588\u258f       | 43/200 [22:31<2:01:30, 46.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 43] \ud83d\udd0d Extracting from: {     \"answer\": \"France\",     \"reasoning\": [       \"From evidence [1] and [8]: Robert brothers (Anne-Jean and Nicolas-Louis) and Jacques Charles are all identified as French individuals\"     ],     \"e...\n",
      "[Ex 43] \u2705 JSON parse OK: answer='France...', citations=[1, 8]\n",
      "[Ex 43] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"French\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 43] \u2705 JSON parse OK: answer='French...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  22%|\u2588\u2588\u258f       | 44/200 [22:51<1:39:53, 38.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 44] \ud83d\udd0d Extracting from: {   \"answer\": \"Selina Giles played the character Evey in the 2005 dystopian political thriller named 'V for Vendetta'\",   \"reasoning\": [],   \"evidence\": [     7   ] }...\n",
      "[Ex 44] \u2705 JSON parse OK: answer='Selina Giles played the character Evey in the 2005...', citations=[7]\n",
      "[Ex 44] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Evey's mother\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 44] \u2705 JSON parse OK: answer='Evey's mother...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  22%|\u2588\u2588\u258e       | 45/200 [23:07<1:22:01, 31.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 45] \ud83d\udd0d Extracting from: {   \"answer\": \"Carus Publishing\",   \"reasoning\": [     \"From evidence [1]: Carus Publishing is the publisher of 'Muse' children's magazine\"   ],   \"evidence\": [     1   ] }  Dionne Bunsha most recentl...\n",
      "[Ex 45] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 174). Trying fallback...\n",
      "[Ex 45] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 45] \u2705 Substring parse OK: answer='Carus Publishing...', citations=[1]\n",
      "[Ex 45] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 45] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  23%|\u2588\u2588\u258e       | 46/200 [23:29<1:13:41, 28.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 46] \ud83d\udd0d Extracting from: {     \"answer\": \"Turkey\",     \"reasoning\": [],     \"evidence\": [       1,       4,       7     ]   }  Explanation: The question asks for the country where the Atik Valide Mosque and Valens Aqueduct ar...\n",
      "[Ex 46] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 102). Trying fallback...\n",
      "[Ex 46] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 46] \u2705 Substring parse OK: answer='Turkey...', citations=[1, 4, 7]\n",
      "[Ex 46] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"Turkey\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 46] \u2705 JSON parse OK: answer='Turkey...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  24%|\u2588\u2588\u258e       | 47/200 [24:09<1:22:11, 32.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 47] \ud83d\udd0d Extracting from: {     \"answer\": \"Helen Dunmore is of British descent, insufficient context to determine M. P. Shiel's ethnicity related to this question\",     \"reasoning\": [],     \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8]...\n",
      "[Ex 47] \u2705 JSON parse OK: answer='Helen Dunmore is of British descent, insufficient ...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 47] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 47] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  24%|\u2588\u2588\u258d       | 48/200 [24:28<1:11:40, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 48] \ud83d\udd0d Extracting from: {     \"answer\": \"Fleetwood Mac\",     \"reasoning\": [       \"From evidence [5]: 'Blue Letter' is a song written by Michael Curtis and Richard Curtis that was first released by Fleetwood Mac\",       \"The...\n",
      "[Ex 48] \u2705 JSON parse OK: answer='Fleetwood Mac...', citations=[5]\n",
      "[Ex 48] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Fleetwood Mac\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 48] \u2705 JSON parse OK: answer='Fleetwood Mac...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  24%|\u2588\u2588\u258d       | 49/200 [24:53<1:08:35, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 49] \ud83d\udd0d Extracting from: {   \"answer\": 3000000,   \"reasoning\": [     \"From evidence [6]: Albania had a total population of almost 3 million people as of 2016\"   ],   \"evidence\": [     6   ] }...\n",
      "[Ex 49] \u2705 JSON parse OK: answer='3000000...', citations=[6]\n",
      "[Ex 49] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"almost 3 million\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 49] \u2705 JSON parse OK: answer='almost 3 million...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  25%|\u2588\u2588\u258c       | 50/200 [25:12<1:02:09, 24.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 50] \ud83d\udd0d Extracting from: {   \"answer\": \"Gimme Shelter (the Rolling Stones documentary) was Oscar nominated\",   \"reasoning\": [     \"From evidence [8]: LaLee's Kin: The Legacy of Cotton was nominated for Best Documentary Featur...\n",
      "[Ex 50] \u2705 JSON parse OK: answer='Gimme Shelter (the Rolling Stones documentary) was...', citations=[8]\n",
      "[Ex 50] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 50] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  26%|\u2588\u2588\u258c       | 51/200 [25:49<1:10:51, 28.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 51] \ud83d\udd0d Extracting from: {     \"answer\": \"Insufficient context\",     \"reasoning\": [] }  The question asks about the Isles led by Aonghus M\u00f3r with a total land area of over 8300 km2, but none of the provided evidence refers to...\n",
      "[Ex 51] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 63). Trying fallback...\n",
      "[Ex 51] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 51] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 51] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 51] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  26%|\u2588\u2588\u258c       | 52/200 [26:16<1:08:41, 27.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 52] \ud83d\udd0d Extracting from: the \"flight into Egypt,\" is an event described in the New Testament.  According to the Gospels of Matthew and Luke, Joseph, Mary, and Jesus went to Egypt to escape persecution from King Herod, then re...\n",
      "[Ex 52] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 52] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 52] \u2705 Substring parse OK: answer='Bethlehem...', citations=[1, 6, 7]\n",
      "[Ex 52] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 52] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  26%|\u2588\u2588\u258b       | 53/200 [27:33<1:44:30, 42.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 53] \ud83d\udd0d Extracting from: {     \"answer\": \"In a Better World\",     \"reasoning\": [       \"From evidence [3]: In a Better World is a Danish drama thriller film\",       \"From evidence [8]: Sisse Graum J\u00f8rgensen is a Danish film p...\n",
      "[Ex 53] \u2705 JSON parse OK: answer='In a Better World...', citations=[3, 8]\n",
      "[Ex 53] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [8].\",   \"answer\": \"In a Better World\",   \"citations\": [     3,     8   ] }...\n",
      "[Ex 53] \u2705 JSON parse OK: answer='In a Better World...', citations=[3, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  27%|\u2588\u2588\u258b       | 54/200 [28:10<1:39:44, 40.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 54] \ud83d\udd0d Extracting from: {   \"answer\": \"child actor\",   \"reasoning\": [     \"From evidence [7]: Katie Sagona is identified as a child actor\"   ],   \"evidence\": [     7   ] }...\n",
      "[Ex 54] \u2705 JSON parse OK: answer='child actor...', citations=[7]\n",
      "[Ex 54] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"child actor\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 54] \u2705 JSON parse OK: answer='child actor...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  28%|\u2588\u2588\u258a       | 55/200 [28:26<1:20:56, 33.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 55] \ud83d\udd0d Extracting from: {   \"answer\": \"Jean Bart\",   \"reasoning\": [     \"From evidence [8]: Jean Bart led the French forces that recaptured a French convoy and captured 3 Dutch ships\",     \"Therefore, Jean Bart is the comman...\n",
      "[Ex 55] \u2705 JSON parse OK: answer='Jean Bart...', citations=[8]\n",
      "[Ex 55] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 55] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  28%|\u2588\u2588\u258a       | 56/200 [28:50<1:13:47, 30.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 56] \ud83d\udd0d Extracting from: {     \"answer\": \"New York City\",     \"reasoning\": [       \"From evidence [1]: The documentary 'Mathematically Alive' mentions Columbia University as one of the universities where a psychology professo...\n",
      "[Ex 56] \u2705 JSON parse OK: answer='New York City...', citations=[1, 3]\n",
      "[Ex 56] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"New York City\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 56] \u2705 JSON parse OK: answer='New York City...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  28%|\u2588\u2588\u258a       | 57/200 [29:16<1:09:45, 29.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 57] \ud83d\udd0d Extracting from: {     \"answer\": \"Rock and Roll\",     \"reasoning\": [       \"From evidence [1] and [6]: Bo Diddley is a rock and roll pioneer and his songs 'Bo Diddley' and 'Bo Diddley Is a Gunslinger' were released as...\n",
      "[Ex 57] \u2705 JSON parse OK: answer='Rock and Roll...', citations=[1, 2, 5, 6, 7]\n",
      "[Ex 57] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 57] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  29%|\u2588\u2588\u2589       | 58/200 [30:11<1:27:04, 36.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 58] \ud83d\udd0d Extracting from: {   \"answer\": \"Jilin province, China\",   \"reasoning\": [     \"From evidence [4] and [5]: Nanping and Jiutai are both located in Jilin province\",     \"Therefore, Sanming and Jiutai are both located in J...\n",
      "[Ex 58] \u2705 JSON parse OK: answer='Jilin province, China...', citations=[4, 5]\n",
      "[Ex 58] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"China\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 58] \u2705 JSON parse OK: answer='China...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  30%|\u2588\u2588\u2589       | 59/200 [30:35<1:17:53, 33.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 59] \ud83d\udd0d Extracting from: Berlin.  Based on the given evidences, there is no direct answer to the question of which production company George Balanchine founded to create a live stage version of the 1942 film Casablanca. The e...\n",
      "[Ex 59] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 59] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 59] \ud83d\udcdd Regex result: answer='Berlin....', citations=[]\n",
      "[Ex 59] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"The Los Angeles Dance Theater\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 59] \u2705 JSON parse OK: answer='The Los Angeles Dance Theater...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  30%|\u2588\u2588\u2588       | 60/200 [31:03<1:13:29, 31.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 60] \ud83d\udd0d Extracting from: {     \"answer\": \"Ian Watkins\",     \"reasoning\": [       \"From evidence [5]: 'We Bring an Arsenal' was the second single from 'Weapons', the fifth studio album by Lostprophets, planned to be released i...\n",
      "[Ex 60] \u2705 JSON parse OK: answer='Ian Watkins...', citations=[5]\n",
      "[Ex 60] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 60] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  30%|\u2588\u2588\u2588       | 61/200 [31:33<1:12:09, 31.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 61] \ud83d\udd0d Extracting from: {   \"answer\": \"Jeff Meldrum and Grover Krantz\",   \"reasoning\": [     \"From evidence [4]: Paul Freeman's plaster casts were considered critical pieces of evidence by anthropologists Jeff Meldrum and Gr...\n",
      "[Ex 61] \u2705 JSON parse OK: answer='Jeff Meldrum and Grover Krantz...', citations=[4]\n",
      "[Ex 61] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 61] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  31%|\u2588\u2588\u2588       | 62/200 [31:55<1:05:00, 28.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 62] \ud83d\udd0d Extracting from: {   \"answer\": [\"Elizabeth Banks\", \"Sebasti\u00e1n Silva\"],   \"reasoning\": [     \"From evidence [6]: The Uninvited is a 2009 psychological horror film starring Emily Browning and Elizabeth Banks\",     \"From...\n",
      "[Ex 62] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 521). Trying fallback...\n",
      "[Ex 62] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 62] \u2705 Substring parse OK: answer='Elizabeth Banks, Sebasti\u00e1n Silva...', citations=[6, 7]\n",
      "[Ex 62] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 62] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  32%|\u2588\u2588\u2588\u258f      | 63/200 [32:38<1:14:45, 32.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 63] \ud83d\udd0d Extracting from: {     \"answer\": \"The first major improved highway in the United States is the National Road (U.S. Route 40)\",     \"reasoning\": [       \"From evidence [6]: The National Road (U.S. Route 40) is the firs...\n",
      "[Ex 63] \u2705 JSON parse OK: answer='The first major improved highway in the United Sta...', citations=[6]\n",
      "[Ex 63] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [6].\",   \"answer\": \"Wheeling, West Virginia\",   \"citations\": [     1,     6   ] }...\n",
      "[Ex 63] \u2705 JSON parse OK: answer='Wheeling, West Virginia...', citations=[1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  32%|\u2588\u2588\u2588\u258f      | 64/200 [33:03<1:08:47, 30.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 64] \ud83d\udd0d Extracting from: {     \"answer\": \"Godiva (from evidence [4])\",     \"reasoning\": [],     \"evidence\": [4]   }  This question asks which store in Gurney Paragon was founded in Belgium in 1926. The only evidence that ment...\n",
      "[Ex 64] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 92). Trying fallback...\n",
      "[Ex 64] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 64] \u2705 Substring parse OK: answer='Godiva (from evidence [4])...', citations=[4]\n",
      "[Ex 64] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"Godiva\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 64] \u2705 JSON parse OK: answer='Godiva...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  32%|\u2588\u2588\u2588\u258e      | 65/200 [33:29<1:05:39, 29.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 65] \ud83d\udd0d Extracting from: {   \"answer\": \"Shaun Micallef\",   \"reasoning\": [     \"From evidence [4]: Shaun Micallef is an Australian actor, comedian and writer. He first gained recognition as a cast member of the sketch comedy s...\n",
      "[Ex 65] \u26a0\ufe0f  JSON parse failed: Extra data: line 13 column 1 (char 444). Trying fallback...\n",
      "[Ex 65] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 65] \u2705 Substring parse OK: answer='Shaun Micallef...', citations=[4, 5]\n",
      "[Ex 65] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [7].\",   \"answer\": \"Eric Banadinovi\\u0107\",   \"citations\": [     6,     7   ] }...\n",
      "[Ex 65] \u2705 JSON parse OK: answer='Eric Banadinovi\u0107...', citations=[6, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  33%|\u2588\u2588\u2588\u258e      | 66/200 [34:21<1:20:06, 35.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 66] \ud83d\udd0d Extracting from: {   \"answer\": \"Munich, Germany\",   \"reasoning\": [     \"From evidence [1]: B\u00fcrgerbr\u00e4ukeller is located in Munich, Germany\",     \"From evidence [1]: After B\u00fcrgerliches merged with L\u00f6wenbr\u00e4u in 1921, the...\n",
      "[Ex 66] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 336). Trying fallback...\n",
      "[Ex 66] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 66] \u2705 Substring parse OK: answer='Munich, Germany...', citations=[1, 2]\n",
      "[Ex 66] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"Germany\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 66] \u2705 JSON parse OK: answer='Germany...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  34%|\u2588\u2588\u2588\u258e      | 67/200 [35:07<1:26:27, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 67] \ud83d\udd0d Extracting from: {     \"answer\": \"The satellite that first launched into orbit in 1993 and was resupplyed by Progress MS-09 is Mir space station\",     \"reasoning\": [       \"From evidence [1], Progress M-17 was launche...\n",
      "[Ex 67] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 637). Trying fallback...\n",
      "[Ex 67] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 67] \u2705 Substring parse OK: answer='The satellite that first launched into orbit in 19...', citations=[1, 4]\n",
      "[Ex 67] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"1998\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 67] \u2705 JSON parse OK: answer='1998...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  34%|\u2588\u2588\u2588\u258d      | 68/200 [35:59<1:34:23, 42.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 68] \ud83d\udd0d Extracting from: te Park is located along Marginal Tiet\u00ea.  Output: {     \"answer\": \"Manistee County\",     \"reasoning\": [       \"From evidence [1]: Manistee County is a county located in Michigan\",       \"From evidence...\n",
      "[Ex 68] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 68] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 68] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 68] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 68] \ud83d\udcdd Regex result: answer='Manistee County...', citations=[1, 5]\n",
      "[Ex 68] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"Chippewa County\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 68] \u2705 JSON parse OK: answer='Chippewa County...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  34%|\u2588\u2588\u2588\u258d      | 69/200 [37:17<1:56:26, 53.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 69] \ud83d\udd0d Extracting from: {   \"answer\": \"None of the given sources provide information about who used a Barrack buster to shoot down a British Army Lynx helicopter\",   \"reasoning\": [],   \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8] }...\n",
      "[Ex 69] \u2705 JSON parse OK: answer='None of the given sources provide information abou...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 69] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 69] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  35%|\u2588\u2588\u2588\u258c      | 70/200 [37:36<1:33:45, 43.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 70] \ud83d\udd0d Extracting from: {     \"answer\": \"Insufficient context\",     \"reasoning\": [] }  The given evidences do not provide enough context to deduce which liberal arts college has their athletic teams named the Lincoln Memoria...\n",
      "[Ex 70] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 63). Trying fallback...\n",
      "[Ex 70] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 70] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 70] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 70] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  36%|\u2588\u2588\u2588\u258c      | 71/200 [38:08<1:25:25, 39.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 71] \ud83d\udd0d Extracting from: {   \"answer\": \"Eri Muraoka\",   \"reasoning\": [     \"From evidence [3]: Hanako Muraoka is the first person to translate 'Anne of Green Gables' into Japanese\",     \"Therefore, Eri Muraoka is the woman wh...\n",
      "[Ex 71] \u2705 JSON parse OK: answer='Eri Muraoka...', citations=[3]\n",
      "[Ex 71] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 71] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  36%|\u2588\u2588\u2588\u258c      | 72/200 [38:33<1:15:21, 35.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 72] \ud83d\udd0d Extracting from: {     \"answer\": \"Yasuzo Masumura and Yasuzo Masumura are both Japanese film directors.\",     \"reasoning\": [       \"From evidence [1], [5], [7], and [8]: Yasuzo Masumura is identified as the director o...\n",
      "[Ex 72] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 568). Trying fallback...\n",
      "[Ex 72] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 72] \u2705 Substring parse OK: answer='Yasuzo Masumura and Yasuzo Masumura are both Japan...', citations=[1, 2, 5, 7, 8]\n",
      "[Ex 72] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"no\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 72] \u2705 JSON parse OK: answer='no...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  36%|\u2588\u2588\u2588\u258b      | 73/200 [39:45<1:38:03, 46.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 73] \ud83d\udd0d Extracting from: {     \"answer\": \"Eric Radomski produced Avengers Assemble (TV series) [5]\",     \"reasoning\": [       \"From evidence [5]: Avengers Assemble premiered on Disney XD on May 26, 2013, and is an American an...\n",
      "[Ex 73] \u2705 JSON parse OK: answer='Eric Radomski produced Avengers Assemble (TV serie...', citations=[5, 8]\n",
      "[Ex 73] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 73] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  37%|\u2588\u2588\u2588\u258b      | 74/200 [40:24<1:33:00, 44.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 74] \ud83d\udd0d Extracting from: {   \"answer\": \"Pieter van Musschenbroek\",   \"reasoning\": [     \"From evidence [5]: A tribometer is an instrument invented by the 18th century Dutch scientist Musschenbroek\",     \"From evidence [6]: Pi...\n",
      "[Ex 74] \u2705 JSON parse OK: answer='Pieter van Musschenbroek...', citations=[5, 6]\n",
      "[Ex 74] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Pieter van Musschenbroek\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 74] \u2705 JSON parse OK: answer='Pieter van Musschenbroek...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  38%|\u2588\u2588\u2588\u258a      | 75/200 [41:10<1:32:48, 44.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 75] \ud83d\udd0d Extracting from: {   \"answer\": \"Dirt track racing\",   \"reasoning\": [     \"From evidence [1] and [4]: Dirt track racing is a type of auto racing performed on clay or dirt surfaced oval tracks in Australia. The Australi...\n",
      "[Ex 75] \u2705 JSON parse OK: answer='Dirt track racing...', citations=[1, 4]\n",
      "[Ex 75] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Dirt track racing\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 75] \u2705 JSON parse OK: answer='Dirt track racing...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  38%|\u2588\u2588\u2588\u258a      | 76/200 [41:35<1:20:26, 38.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 76] \ud83d\udd0d Extracting from: {     \"answer\": \"Fort Worth\",     \"reasoning\": [       \"From evidence [7]: Forest Hill is a suburb of Fort Worth, Texas\",       \"From evidence [8]: Fort Worth is the fifth-largest city in the state of...\n",
      "[Ex 76] \u2705 JSON parse OK: answer='Fort Worth...', citations=[7, 8]\n",
      "[Ex 76] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"Fort Worth\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 76] \u2705 JSON parse OK: answer='Fort Worth...', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  38%|\u2588\u2588\u2588\u258a      | 77/200 [41:58<1:09:55, 34.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 77] \ud83d\udd0d Extracting from: {     \"answer\": \"The slogan 'Blood and Soil' is associated with the Nazi Party and the ideology of Lebensraum.\",     \"reasoning\": [       \"From evidence [3]: 'Blood and soil' is a slogan expressing th...\n",
      "[Ex 77] \u2705 JSON parse OK: answer='The slogan 'Blood and Soil' is associated with the...', citations=[3, 4]\n",
      "[Ex 77] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Blood and soil\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 77] \u2705 JSON parse OK: answer='Blood and soil...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  39%|\u2588\u2588\u2588\u2589      | 78/200 [42:46<1:17:50, 38.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 78] \ud83d\udd0d Extracting from: {     \"answer\": \"The 2016 Oklahoma Sooners football team\",     \"reasoning\": [],     \"evidence\": [       3     ]   }  The question asks which team coached by Bob Stoops beat the 2016 Auburn Tigers foot...\n",
      "[Ex 78] \u26a0\ufe0f  JSON parse failed: Extra data: line 9 column 1 (char 117). Trying fallback...\n",
      "[Ex 78] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 78] \u2705 Substring parse OK: answer='The 2016 Oklahoma Sooners football team...', citations=[3]\n",
      "[Ex 78] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [3].\",   \"answer\": \"2016 Oklahoma Sooners football team\",   \"citations\": [     2,     3   ] }...\n",
      "[Ex 78] \u2705 JSON parse OK: answer='2016 Oklahoma Sooners football team...', citations=[2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  40%|\u2588\u2588\u2588\u2589      | 79/200 [43:22<1:15:54, 37.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 79] \ud83d\udd0d Extracting from: {     \"answer\": \"Insufficient context\",     \"reasoning\": [] }  There is no evidence provided regarding the city after which Vice President Elbridge Gerry was named and its population according to the ...\n",
      "[Ex 79] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 63). Trying fallback...\n",
      "[Ex 79] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 79] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 79] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"5,922\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 79] \u2705 JSON parse OK: answer='5,922...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  40%|\u2588\u2588\u2588\u2588      | 80/200 [43:37<1:01:38, 30.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 80] \ud83d\udd0d Extracting from: { \"answer\": \"Insufficient context\", \"reasoning\": [] }  The given evidence does not provide enough context to determine which was published first, \"Take It Easy\" or \"Personal Preference\". The first evi...\n",
      "[Ex 80] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 55). Trying fallback...\n",
      "[Ex 80] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 80] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 80] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 80] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  40%|\u2588\u2588\u2588\u2588      | 81/200 [44:11<1:03:05, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 81] \ud83d\udd0d Extracting from: {   \"answer\": \"The Prospect of Whitby, in Wapping, London, has the oldest riverside tavern, dating from around 1520.\",   \"reasoning\": [],   \"evidence\": [     2   ] }...\n",
      "[Ex 81] \u2705 JSON parse OK: answer='The Prospect of Whitby, in Wapping, London, has th...', citations=[2]\n",
      "[Ex 81] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"from around 1520\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 81] \u2705 JSON parse OK: answer='from around 1520...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  41%|\u2588\u2588\u2588\u2588      | 82/200 [44:29<54:27, 27.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 82] \ud83d\udd0d Extracting from: {     \"answer\": \"Paola Su\u00e1rez and Virginia Ruano Pascual\",     \"reasoning\": [       \"From evidence [4]: Paola Su\u00e1rez was a prominent women's doubles player throughout the early and mid-2000s, winning ...\n",
      "[Ex 82] \u26a0\ufe0f  JSON parse failed: Extra data: line 13 column 1 (char 442). Trying fallback...\n",
      "[Ex 82] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 82] \u2705 Substring parse OK: answer='Paola Su\u00e1rez and Virginia Ruano Pascual...', citations=[4, 5]\n",
      "[Ex 82] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [4].\",   \"answer\": \"Paola Su\\u00e1rez\",   \"citations\": [     1,     4   ] }...\n",
      "[Ex 82] \u2705 JSON parse OK: answer='Paola Su\u00e1rez...', citations=[1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  42%|\u2588\u2588\u2588\u2588\u258f     | 83/200 [45:43<1:20:40, 41.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 83] \ud83d\udd0d Extracting from: {     \"answer\": \"Yes\",     \"reasoning\": [       \"From evidence [7] and [8]: Joe Orton is identified as an English playwright\",       \"From evidence [1]: Bernard-Marie Kolt\u00e8s is identified as a French ...\n",
      "[Ex 83] \u2705 JSON parse OK: answer='Yes...', citations=[1, 7, 8]\n",
      "[Ex 83] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"yes\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 83] \u2705 JSON parse OK: answer='yes...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  42%|\u2588\u2588\u2588\u2588\u258f     | 84/200 [46:14<1:13:52, 38.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 84] \ud83d\udd0d Extracting from: {   \"answer\": \"Carl Perkins\",   \"reasoning\": [     \"From evidence [5]: The song 'Restless' was recorded by Mark O'Connor's band project New Nashville Cats\",     \"From evidence [5]: Carl Perkins perfor...\n",
      "[Ex 84] \u2705 JSON parse OK: answer='Carl Perkins...', citations=[5]\n",
      "[Ex 84] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 84] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  42%|\u2588\u2588\u2588\u2588\u258e     | 85/200 [46:53<1:13:52, 38.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 85] \ud83d\udd0d Extracting from: {   \"answer\": \"Philip Jos\u00e9 Farmer lived longer\",   \"reasoning\": [     \"From evidence [2] and [4]: Philip Jos\u00e9 Farmer was born on January 26, 1918\",     \"From evidence [1] and [6]: Philip Jos\u00e9 Farmer w...\n",
      "[Ex 85] \u2705 JSON parse OK: answer='Philip Jos\u00e9 Farmer lived longer...', citations=[2, 4]\n",
      "[Ex 85] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"Philip Jos\\u00e9 Farmer\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 85] \u2705 JSON parse OK: answer='Philip Jos\u00e9 Farmer...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  43%|\u2588\u2588\u2588\u2588\u258e     | 86/200 [47:30<1:12:24, 38.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 86] \ud83d\udd0d Extracting from: {   \"answer\": \"Rockstar San Diego\",   \"reasoning\": [     \"From evidence [1] and [4]: Red Dead Redemption and Red Dead Revolver are video games developed by Rockstar San Diego\",     \"From evidence [2]:...\n",
      "[Ex 86] \u2705 JSON parse OK: answer='Rockstar San Diego...', citations=[1, 4]\n",
      "[Ex 86] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Rockstar San Diego\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 86] \u2705 JSON parse OK: answer='Rockstar San Diego...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  44%|\u2588\u2588\u2588\u2588\u258e     | 87/200 [47:56<1:05:08, 34.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 87] \ud83d\udd0d Extracting from: { \"answer\": \"Neither The Pogues nor Pillar are electronic dance music groups.\", \"reasoning\": [ ], \"evidence\": [ 2, 4 ] }  Explanation: The evidence provided does not contain any information about The ...\n",
      "[Ex 87] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 122). Trying fallback...\n",
      "[Ex 87] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 87] \u2705 Substring parse OK: answer='Neither The Pogues nor Pillar are electronic dance...', citations=[2, 4]\n",
      "[Ex 87] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"no\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 87] \u2705 JSON parse OK: answer='no...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  44%|\u2588\u2588\u2588\u2588\u258d     | 88/200 [48:23<59:55, 32.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 88] \ud83d\udd0d Extracting from: {   \"answer\": \"Cornell University\",   \"reasoning\": [     \"From evidence [7]: Gary Bettman is a graduate of Cornell University\"   ],   \"evidence\": [     7   ] }...\n",
      "[Ex 88] \u2705 JSON parse OK: answer='Cornell University...', citations=[7]\n",
      "[Ex 88] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"New York University School of Law\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 88] \u2705 JSON parse OK: answer='New York University School of Law...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  44%|\u2588\u2588\u2588\u2588\u258d     | 89/200 [48:39<50:24, 27.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 89] \ud83d\udd0d Extracting from: {     \"answer\": \"Nils Gaup\",     \"reasoning\": [       \"From evidence [6] and [8]: Nils Gaup is a film director born on April 12, 1955\",       \"From evidence [2], [4], [5], and [7]: Nils Gaup has direc...\n",
      "[Ex 89] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 375). Trying fallback...\n",
      "[Ex 89] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 89] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 89] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 89] \ud83d\udcdd Regex result: answer='Nils Gaup...', citations=[6, 8]\n",
      "[Ex 89] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [6].\",   \"answer\": \"Bruce Conner\",   \"citations\": [     1,     6   ] }...\n",
      "[Ex 89] \u2705 JSON parse OK: answer='Bruce Conner...', citations=[1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  45%|\u2588\u2588\u2588\u2588\u258c     | 90/200 [49:43<1:10:32, 38.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 90] \ud83d\udd0d Extracting from: {   \"answer\": \"OSRIC (Old School Reference and Index Compilation)\",   \"reasoning\": [     \"From evidence [5]: OSRIC is a retro-clone of the first edition of Advanced Dungeons & Dragons and one of the m...\n",
      "[Ex 90] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 522). Trying fallback...\n",
      "[Ex 90] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 90] \u2705 Substring parse OK: answer='OSRIC (Old School Reference and Index Compilation)...', citations=[5]\n",
      "[Ex 90] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 90] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  46%|\u2588\u2588\u2588\u2588\u258c     | 91/200 [50:33<1:16:15, 41.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 91] \ud83d\udd0d Extracting from: {     \"answer\": \"Duff Goldman\",     \"reasoning\": [       \"From evidence [6]: Duff Goldman was born on December 17, 1974\",       \"From evidence [1], [2], [3], [4], and [8]: Duff Goldman has been a judg...\n",
      "[Ex 91] \u2705 JSON parse OK: answer='Duff Goldman...', citations=[6]\n",
      "[Ex 91] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"Jeffrey Adam \\\"Duff\\\" Goldman\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 91] \u2705 JSON parse OK: answer='Jeffrey Adam \"Duff\" Goldman...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  46%|\u2588\u2588\u2588\u2588\u258c     | 92/200 [50:59<1:06:29, 36.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 92] \ud83d\udd0d Extracting from: { \"answer\": \"Brittany Murphy\", \"reasoning\": [ \"From evidence [4]: Brittany Murphy was a native of Atlanta\", \"From evidence [7]: Brittany Murphy starred in the breakthrough role of Daisy Randone in 'Gi...\n",
      "[Ex 92] \u26a0\ufe0f  JSON parse failed: Extra data: line 13 column 1 (char 245). Trying fallback...\n",
      "[Ex 92] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 92] \u2705 Substring parse OK: answer='Brittany Murphy...', citations=[4, 7]\n",
      "[Ex 92] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"Tai Frasier in \\\"Clueless\\\"\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 92] \u2705 JSON parse OK: answer='Tai Frasier in \"Clueless\"...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  46%|\u2588\u2588\u2588\u2588\u258b     | 93/200 [51:33<1:04:42, 36.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 93] \ud83d\udd0d Extracting from: {     \"answer\": \"Bing Crosby\",     \"reasoning\": [       \"From evidence [3]: College Humor is a 1933 American pre-Code musical comedy film that stars Bing Crosby\"     ],     \"evidence\": [       3     ]...\n",
      "[Ex 93] \u2705 JSON parse OK: answer='Bing Crosby...', citations=[3]\n",
      "[Ex 93] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Harry Lillis \\\"Bing\\\" Crosby Jr.\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 93] \u2705 JSON parse OK: answer='Harry Lillis \"Bing\" Crosby Jr....', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  47%|\u2588\u2588\u2588\u2588\u258b     | 94/200 [51:52<54:49, 31.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 94] \ud83d\udd0d Extracting from: the Caliban, a character in Shakespeare's \"The Tempest\".  The two species are \"Calibanus microcarpus\" and \"Calibanus giganteus\".  \"Calibanus microcarpus\" has small flowers and is the more common speci...\n",
      "[Ex 94] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 94] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 94] \ud83d\udcdd Regex result: answer='the Caliban, a character in Shakespeare's \"The Tem...', citations=[]\n",
      "[Ex 94] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 94] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  48%|\u2588\u2588\u2588\u2588\u258a     | 95/200 [53:10<1:18:56, 45.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 95] \ud83d\udd0d Extracting from: {     \"answer\": \"Jaguar Land Rover\",     \"reasoning\": [       \"From evidence [4]: Jaguar Land Rover has its headquarters in Whitley, Coventry, United Kingdom\"     ],     \"evidence\": [       4     ]   ...\n",
      "[Ex 95] \u2705 JSON parse OK: answer='Jaguar Land Rover...', citations=[4]\n",
      "[Ex 95] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"Jaguar Land Rover\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 95] \u2705 JSON parse OK: answer='Jaguar Land Rover...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  48%|\u2588\u2588\u2588\u2588\u258a     | 96/200 [53:28<1:04:13, 37.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 96] \ud83d\udd0d Extracting from: {   \"answer\": \"Jeanne Tripplehorn\",   \"reasoning\": [],   \"evidence\": [1] }  Jeanne Tripplehorn is an American film and television actress who starred as Jacqueline Kennedy Onassis in the 2009 HBO movi...\n",
      "[Ex 96] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 76). Trying fallback...\n",
      "[Ex 96] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 96] \u2705 Substring parse OK: answer='Jeanne Tripplehorn...', citations=[1]\n",
      "[Ex 96] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 96] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  48%|\u2588\u2588\u2588\u2588\u258a     | 97/200 [53:54<57:43, 33.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 97] \ud83d\udd0d Extracting from: {     \"answer\": \"Princess Charlotte of Cambridge was born before the repealing of the Royal Marriages Act 1772\",     \"reasoning\": [       \"From evidence [2]: Prince Henry, Duke of Cumberland and Strat...\n",
      "[Ex 97] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 777). Trying fallback...\n",
      "[Ex 97] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 97] \u2705 Substring parse OK: answer='Princess Charlotte of Cambridge was born before th...', citations=[2, 4]\n",
      "[Ex 97] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 97] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  49%|\u2588\u2588\u2588\u2588\u2589     | 98/200 [55:00<1:13:42, 43.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 98] \ud83d\udd0d Extracting from: {   \"answer\": \"World War I\",   \"reasoning\": [     \"From evidence [4]: World War I was a global war that lasted from 28 July 1914 to 11 November 1918.\",     \"From evidence [7]: Hermann G\u00f6ring was a vet...\n",
      "[Ex 98] \u2705 JSON parse OK: answer='World War I...', citations=[4, 7]\n",
      "[Ex 98] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"1918\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 98] \u2705 JSON parse OK: answer='1918...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  50%|\u2588\u2588\u2588\u2588\u2589     | 99/200 [55:37<1:09:40, 41.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 99] \ud83d\udd0d Extracting from: {   \"answer\": \"None of the given evidences provide information about Ethel Winter working with any avant-garde theater director.\",   \"reasoning\": [],   \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8] }...\n",
      "[Ex 99] \u2705 JSON parse OK: answer='None of the given evidences provide information ab...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 99] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 99] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  50%|\u2588\u2588\u2588\u2588\u2588     | 100/200 [55:55<57:34, 34.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 100] \ud83d\udd0d Extracting from: {   \"answer\": \"Haleiwa, Hawaii\",   \"reasoning\": [     \"From evidence [6]: Haleiwa is a North Shore community and census-designated place in Hawaii\",     \"From evidence [2]: The women's Vans Triple Cro...\n",
      "[Ex 100] \u2705 JSON parse OK: answer='Haleiwa, Hawaii...', citations=[2, 6]\n",
      "[Ex 100] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"City and County of Honolulu\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 100] \u2705 JSON parse OK: answer='City and County of Honolulu...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  50%|\u2588\u2588\u2588\u2588\u2588     | 101/200 [56:25<54:29, 33.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 101] \ud83d\udd0d Extracting from: {     \"answer\": \"White Horse\",     \"reasoning\": [       \"From evidence [1]: White Horse is a song co-written by Liz Rose and Taylor Swift\",       \"Therefore, Liz Rose co-wrote a song released on Decem...\n",
      "[Ex 101] \u2705 JSON parse OK: answer='White Horse...', citations=[1]\n",
      "[Ex 101] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"White Horse\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 101] \u2705 JSON parse OK: answer='White Horse...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  51%|\u2588\u2588\u2588\u2588\u2588     | 102/200 [56:49<49:34, 30.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 102] \ud83d\udd0d Extracting from: {     \"answer\": \"The man was born in the United Kingdom\",     \"reasoning\": [       \"From evidence [2]: British people are the citizens of the United Kingdom\",       \"The man is a British citizen [1, 2...\n",
      "[Ex 102] \u2705 JSON parse OK: answer='The man was born in the United Kingdom...', citations=[1, 2]\n",
      "[Ex 102] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 102] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 103/200 [57:13<45:46, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 103] \ud83d\udd0d Extracting from: {   \"answer\": \"Al Jazira Club (Abu Dhabi, United Arab Emirates)\",   \"reasoning\": [],   \"evidence\": [1] }  The question asks for the football club where Aly Mohamed plays. The only evidence provided th...\n",
      "[Ex 103] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 106). Trying fallback...\n",
      "[Ex 103] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 103] \u2705 Substring parse OK: answer='Al Jazira Club (Abu Dhabi, United Arab Emirates)...', citations=[1]\n",
      "[Ex 103] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 103] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 104/200 [57:45<47:09, 29.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 104] \ud83d\udd0d Extracting from: {     \"answer\": \"Atat\u00fcrk Museum\",     \"reasoning\": [       \"From evidence [2]: Atat\u00fcrk Museum Mansion was Mustafa Kemal Atat\u00fcrk's residence between 1921 and 1932\",       \"From evidence [8]: Atat\u00fcrk Mu...\n",
      "[Ex 104] \u2705 JSON parse OK: answer='Atat\u00fcrk Museum...', citations=[2, 8]\n",
      "[Ex 104] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"museum\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 104] \u2705 JSON parse OK: answer='museum...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 105/200 [58:28<53:23, 33.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 105] \ud83d\udd0d Extracting from: {     \"answer\": \"1985\",     \"evidence\": [       5,       6,       8     ] }  Reasoning: The evidence indicates that \"The Captain of Her Heart\" is a single by the Swiss duo Double, released in 1985. Th...\n",
      "[Ex 105] \u26a0\ufe0f  JSON parse failed: Extra data: line 10 column 1 (char 77). Trying fallback...\n",
      "[Ex 105] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 105] \u2705 Substring parse OK: answer='1985...', citations=[5, 6, 8]\n",
      "[Ex 105] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 105] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 106/200 [58:51<47:41, 30.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 106] \ud83d\udd0d Extracting from: {   \"answer\": \"Peter Schmeichel\",   \"reasoning\": [     \"From evidence [2]: Peter Schmeichel was voted the IFFHS World's Best Goalkeeper in 1992 and 1993\",     \"Therefore, Peter Schmeichel was the fath...\n",
      "[Ex 106] \u2705 JSON parse OK: answer='Peter Schmeichel...', citations=[2]\n",
      "[Ex 106] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"World's Best Goalkeeper\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 106] \u2705 JSON parse OK: answer='World's Best Goalkeeper...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 107/200 [59:22<47:12, 30.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 107] \ud83d\udd0d Extracting from: {   \"answer\": \"Philip K. Dick\",   \"reasoning\": [     \"From evidence [2]: Eric Overmyer is an American writer\",     \"From evidence [3]: The Ganymede Takeover is a science fiction novel written by Ameri...\n",
      "[Ex 107] \u2705 JSON parse OK: answer='Philip K. Dick...', citations=[2, 3]\n",
      "[Ex 107] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Philip K. Dick\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 107] \u2705 JSON parse OK: answer='Philip K. Dick...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 108/200 [1:00:07<53:34, 34.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 108] \ud83d\udd0d Extracting from:  The Corpus Evangelicorum was dissolved on 14 May 1663, when Sweden, in the aftermath of the Battle of Warsaw, ceded all its rights and interests in the Empire to France. [6] Title: Battle of Gavinana...\n",
      "[Ex 108] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 2 (char 1). Trying fallback...\n",
      "[Ex 108] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 108] \ud83d\udcdd Regex result: answer='The Corpus Evangelicorum was dissolved on 14 May 1...', citations=[]\n",
      "[Ex 108] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Army of the Holy Roman\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 108] \u2705 JSON parse OK: answer='Army of the Holy Roman...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 109/200 [1:01:25<1:12:41, 47.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 109] \ud83d\udd0d Extracting from: to the United Nations, and Assistant Secretary of State.  He is the co-founder of the private equity firm Lazard Fr\u00e8res & Co. LLC and served as its chairman and chief executive officer until 1991.  He...\n",
      "[Ex 109] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 109] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 109] \u2705 Substring parse OK: answer='Caddo National Grassland, Lyndon B. Johnson Nation...', citations=[5]\n",
      "[Ex 109] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 109] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 110/200 [1:02:36<1:22:03, 54.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 110] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The given evidences do not provide enough context to determine which street was the adult entertainment district of the Liberty Tree District...\n",
      "[Ex 110] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 110] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 110] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 110] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Washington Street\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 110] \u2705 JSON parse OK: answer='Washington Street...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 111/200 [1:03:06<1:10:05, 47.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 111] \ud83d\udd0d Extracting from: play for the Argonauts for two seasons before being traded to the Winnipeg Blue Bombers on July 12, 2014.  He played for the Blue Bombers until 2017.  Output: {     \"answer\": \"Chuck Noll\",     \"reason...\n",
      "[Ex 111] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 111] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 111] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 111] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 111] \ud83d\udcdd Regex result: answer='Chuck Noll...', citations=[2, 7]\n",
      "[Ex 111] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Chuck Noll\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 111] \u2705 JSON parse OK: answer='Chuck Noll...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 112/200 [1:03:58<1:11:26, 48.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 112] \ud83d\udd0d Extracting from: {   \"answer\": \"Huck Hartman\",   \"reasoning\": [     \"From evidence [1]: Huck Hartman was a player for the Chicago American Gears\",     \"From evidence [5]: George Yardley broke the record of scoring 2,0...\n",
      "[Ex 112] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 687). Trying fallback...\n",
      "[Ex 112] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 112] \u2705 Substring parse OK: answer='Huck Hartman...', citations=[1, 5]\n",
      "[Ex 112] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [5].\",   \"answer\": \"George Lawrence Mikan, Jr.\",   \"citations\": [     3,     5   ] }...\n",
      "[Ex 112] \u2705 JSON parse OK: answer='George Lawrence Mikan, Jr....', citations=[3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 113/200 [1:05:13<1:21:56, 56.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 113] \ud83d\udd0d Extracting from: {     \"answer\": \"Lilys\",     \"reasoning\": [       \"From evidence [6]: Lilys is an American indie rock band with only one constant member, Kurt Heasley\"     ],     \"evidence\": [       6     ]   }...\n",
      "[Ex 113] \u2705 JSON parse OK: answer='Lilys...', citations=[6]\n",
      "[Ex 113] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"Ratatat\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 113] \u2705 JSON parse OK: answer='Ratatat...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 114/200 [1:05:31<1:04:26, 44.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 114] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The provided evidences do not contain any information about a hotel named Kaye Stevens or any connection of her to any of the mentioned hotel...\n",
      "[Ex 114] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 114] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 114] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 114] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 114] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 115/200 [1:05:46<51:10, 36.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 115] \ud83d\udd0d Extracting from: {   \"answer\": \"Jessica Mauboy\",   \"reasoning\": [     \"From evidence [2]: Jessica Mauboy recorded a cover of 'Bridge over Troubled Water' for her debut live album 'The Journey' (2007)\",     \"From evide...\n",
      "[Ex 115] \u2705 JSON parse OK: answer='Jessica Mauboy...', citations=[2, 5]\n",
      "[Ex 115] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Anthony Callea\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 115] \u2705 JSON parse OK: answer='Anthony Callea...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 116/200 [1:06:29<53:14, 38.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 116] \ud83d\udd0d Extracting from: { \"answer\": \"Zadar (or Zara)\", \"reasoning\": [ \"From evidence [7]: Zadar (or Zara) is a city whose king, Emeric, pledged himself to join the Crusade and was attacked by the crusaders despite a papal de...\n",
      "[Ex 116] \u26a0\ufe0f  JSON parse failed: Expecting ',' delimiter: line 7 column 1 (char 342. Trying fallback...\n",
      "[Ex 116] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 116] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 116] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 116] \ud83d\udcdd Regex result: answer='Zadar (or Zara)...', citations=[7]\n",
      "[Ex 116] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 116] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 117/200 [1:06:59<49:39, 35.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 117] \ud83d\udd0d Extracting from: Minnesota Twins (1966\u20131971). [7] Title: Peter Wallace Hobbs - Peter Hobbs (born July 23, 1917) is a British electrical engineer and entrepreneur.  He formed the electrical appliance company Russell Ho...\n",
      "[Ex 117] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 117] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 117] \ud83d\udcdd Regex result: answer='Seven years before the opening of the Brewer Field...', citations=[]\n",
      "[Ex 117] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 117] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 118/200 [1:08:17<1:06:16, 48.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 118] \ud83d\udd0d Extracting from: {   \"answer\": \"Ashanti\",   \"reasoning\": [     \"From evidence [3]: 'Rock wit U (Awww Baby)' is a song by American R&B singer Ashanti\",     \"From evidence [4]: Ashanti is known for her second studio alb...\n",
      "[Ex 118] \u2705 JSON parse OK: answer='Ashanti...', citations=[3, 4]\n",
      "[Ex 118] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"October 13, 1980\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 118] \u2705 JSON parse OK: answer='October 13, 1980...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 119/200 [1:08:51<59:16, 43.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 119] \ud83d\udd0d Extracting from: {     \"answer\": \"Motion City Soundtrack\",     \"reasoning\": [       \"From evidence [1] and [4-6]: Motion City Soundtrack has released six studio albums (mentioned in evidence [1] and confirmed by evide...\n",
      "[Ex 119] \u2705 JSON parse OK: answer='Motion City Soundtrack...', citations=[1, 4, 5, 6]\n",
      "[Ex 119] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"Motion City Soundtrack\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 119] \u2705 JSON parse OK: answer='Motion City Soundtrack...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 120/200 [1:09:16<51:02, 38.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 120] \ud83d\udd0d Extracting from: {     \"answer\": \"Pago Pago\",     \"reasoning\": [       \"From evidence [3]: Veterans Memorial Stadium is located in Pago Pago, American Samoa and is the home venue of the American Samoa national footbal...\n",
      "[Ex 120] \u2705 JSON parse OK: answer='Pago Pago...', citations=[3, 5, 7]\n",
      "[Ex 120] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"He is from Pago Pago, American Samoa and played college football at Oregon.\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 120] \u2705 JSON parse OK: answer='He is from Pago Pago, American Samoa and played co...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 121/200 [1:10:01<53:15, 40.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 121] \ud83d\udd0d Extracting from: {   \"answer\": \"Michael LeSieur, Greg Mottola (for Keanu, as per evidence [1])\",   \"reasoning\": [],   \"evidence\": [1] }  This question asks for the writer(s) of the 2016 American action comedy film \"Ke...\n",
      "[Ex 121] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 120). Trying fallback...\n",
      "[Ex 121] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 121] \u2705 Substring parse OK: answer='Michael LeSieur, Greg Mottola (for Keanu, as per e...', citations=[1]\n",
      "[Ex 121] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 121] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 122/200 [1:10:44<53:34, 41.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 122] \ud83d\udd0d Extracting from: {     \"answer\": \"Both Yameen and Activision are associated with each other through the music industry.\",     \"reasoning\": [       \"From evidence [7]: Yameen is a hiphop producer and his music can be h...\n",
      "[Ex 122] \u2705 JSON parse OK: answer='Both Yameen and Activision are associated with eac...', citations=[3, 7]\n",
      "[Ex 122] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 122] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 123/200 [1:11:19<50:29, 39.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 123] \ud83d\udd0d Extracting from: {     \"answer\": \"Johnny McDaid\",     \"reasoning\": [       \"From evidence [6]: Paul van Dyk's sixth studio album, Evolution, features collaborations with various artists including Johnny McDaid's Field...\n",
      "[Ex 123] \u2705 JSON parse OK: answer='Johnny McDaid...', citations=[5, 6]\n",
      "[Ex 123] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Johnny McDaid\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 123] \u2705 JSON parse OK: answer='Johnny McDaid...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 124/200 [1:11:54<48:10, 38.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 124] \ud83d\udd0d Extracting from: {     \"answer\": \"Them\",     \"reasoning\": [       \"From evidence [6]: Them was a Northern Irish band formed in Belfast in April 1964\",       \"From evidence [5]: The Story of Them Featuring Van Morrison...\n",
      "[Ex 124] \u2705 JSON parse OK: answer='Them...', citations=[5, 6]\n",
      "[Ex 124] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Them\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 124] \u2705 JSON parse OK: answer='Them...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 125/200 [1:12:27<45:42, 36.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 125] \ud83d\udd0d Extracting from: {     \"answer\": \"Model\",     \"reasoning\": [       \"From evidence [5]: Sunny Leone is a Canadian-born Indian-American actress and model\",       \"From evidence [1], [6], [7]: Karishma Tanna is an Indian...\n",
      "[Ex 125] \u26a0\ufe0f  JSON parse failed: Extra data: line 16 column 1 (char 363). Trying fallback...\n",
      "[Ex 125] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 125] \u2705 Substring parse OK: answer='Model...', citations=[1, 5, 6, 7]\n",
      "[Ex 125] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [5].\",   \"answer\": \"pornstar\",   \"citations\": [     3,     5   ] }...\n",
      "[Ex 125] \u2705 JSON parse OK: answer='pornstar...', citations=[3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 126/200 [1:13:10<47:27, 38.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 126] \ud83d\udd0d Extracting from: {   \"answer\": \"Don Johnson\",   \"reasoning\": [     \"From evidence [7]: Don Johnson starred in the television series 'Miami Vice'\",     \"From evidence [8]: Don Johnson also starred in the 2003 film 'Wor...\n",
      "[Ex 126] \u2705 JSON parse OK: answer='Don Johnson...', citations=[7, 8]\n",
      "[Ex 126] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"Donald Wayne Johnson\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 126] \u2705 JSON parse OK: answer='Donald Wayne Johnson...', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 127/200 [1:13:34<41:16, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 127] \ud83d\udd0d Extracting from: {     \"answer\": \"St. George\",     \"reasoning\": [],     \"evidence\": [       7     ]   }...\n",
      "[Ex 127] \u2705 JSON parse OK: answer='St. George...', citations=[7]\n",
      "[Ex 127] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"St.George\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 127] \u2705 JSON parse OK: answer='St.George...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 128/200 [1:13:45<32:27, 27.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 128] \ud83d\udd0d Extracting from: {     \"answer\": \"Thirukkalacherry\",     \"reasoning\": [],     \"evidence\": [       7     ]   }...\n",
      "[Ex 128] \u2705 JSON parse OK: answer='Thirukkalacherry...', citations=[7]\n",
      "[Ex 128] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 128] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 129/200 [1:13:56<26:28, 22.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 129] \ud83d\udd0d Extracting from: {   \"answer\": \"2011 (for the novel)\",   \"reasoning\": [     \"From evidence [2]: Fifty Shades of Grey is a 2011 erotic romance novel\"   ],   \"evidence\": [     2   ] }  Output: {   \"answer\": \"2017 (for t...\n",
      "[Ex 129] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 166). Trying fallback...\n",
      "[Ex 129] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 129] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 129] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 129] \ud83d\udcdd Regex result: answer='2011 (for the novel)...', citations=[2]\n",
      "[Ex 129] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"2011\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 129] \u2705 JSON parse OK: answer='2011...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 130/200 [1:14:36<32:13, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 130] \ud83d\udd0d Extracting from: {     \"answer\": \"Barton Mine and Hermiston-McCauley Mine are not located in The United States of America\",     \"reasoning\": [       \"From evidence [6] and [6]: Barton Mine and Hermiston-McCauley Mine ...\n",
      "[Ex 130] \u2705 JSON parse OK: answer='Barton Mine and Hermiston-McCauley Mine are not lo...', citations=[6]\n",
      "[Ex 130] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"no\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 130] \u2705 JSON parse OK: answer='no...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 131/200 [1:15:02<31:10, 27.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 131] \ud83d\udd0d Extracting from: {   \"answer\": \"Daniel Vacek reached a career-high singles ranking of World No. 26, [2]\",   \"evidence\": [2] }  The provided evidence does not contain any information regarding Fabio Fognini's career-hi...\n",
      "[Ex 131] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 110). Trying fallback...\n",
      "[Ex 131] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 131] \u2705 Substring parse OK: answer='Daniel Vacek reached a career-high singles ranking...', citations=[2]\n",
      "[Ex 131] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 131] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 132/200 [1:15:27<30:06, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 132] \ud83d\udd0d Extracting from: {   \"answer\": \"Gymnocladus (Kentucky coffeetree)\",   \"reasoning\": [     \"From evidence [6]: Caesalpinioideae is a subfamily of trees that includes the genus Dryopteris and the species Gymnocladus dioi...\n",
      "[Ex 132] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 612). Trying fallback...\n",
      "[Ex 132] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 132] \u2705 Substring parse OK: answer='Gymnocladus (Kentucky coffeetree)...', citations=[6, 8]\n",
      "[Ex 132] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 132] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 133/200 [1:16:39<44:44, 40.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 133] \ud83d\udd0d Extracting from: {   \"answer\": \"J\u00fcrgen Vollmer is a German photographer\",   \"reasoning\": [     \"From evidence [1] and [8]: Astrid Kirchherr is a German photographer and friend of the Beatles, and J\u00fcrgen Vollmer is als...\n",
      "[Ex 133] \u2705 JSON parse OK: answer='J\u00fcrgen Vollmer is a German photographer...', citations=[1, 8]\n",
      "[Ex 133] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"German\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 133] \u2705 JSON parse OK: answer='German...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 134/200 [1:17:04<39:12, 35.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 134] \ud83d\udd0d Extracting from: { \"answer\": \"Insufficient context\", \"reasoning\": [] }  The given evidences do not provide any information about a book that the president and CEO of Tracinda Corporation bought....\n",
      "[Ex 134] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 55). Trying fallback...\n",
      "[Ex 134] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 134] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 134] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 134] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 135/200 [1:17:18<31:34, 29.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 135] \ud83d\udd0d Extracting from: {     \"answer\": \"The University of Texas at Austin Longhorns\",     \"reasoning\": [       \"From evidence [4]: The University of Texas at Austin Longhorns have a mascot named Bevo\",       \"From evidence ...\n",
      "[Ex 135] \u2705 JSON parse OK: answer='The University of Texas at Austin Longhorns...', citations=[4, 5]\n",
      "[Ex 135] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"Texas Longhorns\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 135] \u2705 JSON parse OK: answer='Texas Longhorns...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 136/200 [1:17:50<32:07, 30.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 136] \ud83d\udd0d Extracting from: {     \"answer\": \"Both Marge Piercy and Richard Aldington are writers.\"     \"reasoning\": [],     \"evidence\": [6, 1] }  Explanation: Marge Piercy is identified as a poet, novelist, and social activist i...\n",
      "[Ex 136] \u26a0\ufe0f  JSON parse failed: Expecting ',' delimiter: line 3 column 5 (char 75). Trying fallback...\n",
      "[Ex 136] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 136] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 136] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 136] \ud83d\udcdd Regex result: answer='Both Marge Piercy and Richard Aldington are writer...', citations=[1, 6]\n",
      "[Ex 136] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [6].\",   \"answer\": \"poet\",   \"citations\": [     1,     6   ] }...\n",
      "[Ex 136] \u2705 JSON parse OK: answer='poet...', citations=[1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 137/200 [1:18:20<31:30, 30.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 137] \ud83d\udd0d Extracting from: {     \"answer\": \"CMLL or Arena M\u00e9xico\",     \"reasoning\": [       \"From evidence [1], [3], [5], [6], and [7]: EMLL and CMLL are professional wrestling promotions that celebrate their anniversaries with...\n",
      "[Ex 137] \u26a0\ufe0f  JSON parse failed: Extra data: line 17 column 1 (char 559). Trying fallback...\n",
      "[Ex 137] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 137] \u2705 Substring parse OK: answer='CMLL or Arena M\u00e9xico...', citations=[1, 3, 5, 6, 7]\n",
      "[Ex 137] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 137] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 138/200 [1:19:22<40:49, 39.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 138] \ud83d\udd0d Extracting from: {     \"answer\": \"Derry\",     \"reasoning\": [       \"From evidence [7]: Johnny Campbell was a footballer from Derry, Northern Ireland\"     ],     \"evidence\": [       7     ]   }...\n",
      "[Ex 138] \u2705 JSON parse OK: answer='Derry...', citations=[7]\n",
      "[Ex 138] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"Londonderry\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 138] \u2705 JSON parse OK: answer='Londonderry...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 139/200 [1:19:39<33:16, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 139] \ud83d\udd0d Extracting from: { \"answer\": \"Insufficient context\", \"reasoning\": [] }  The given question asks which is farther west, Sheridan County, Montana or Chandra Taal. However, none of the provided evidences mention the west...\n",
      "[Ex 139] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 55). Trying fallback...\n",
      "[Ex 139] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 139] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 139] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Sheridan County\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 139] \u2705 JSON parse OK: answer='Sheridan County...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 140/200 [1:20:00<29:12, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 140] \ud83d\udd0d Extracting from: {   \"answer\": \"The Football League Championship\",   \"reasoning\": [     \"From evidence [1], [2], [5], and [4]: Ipswich Town F.C. has been a member of The Football League Championship for the 2010-11, 2...\n",
      "[Ex 140] \u2705 JSON parse OK: answer='The Football League Championship...', citations=[1, 2, 5, 7]\n",
      "[Ex 140] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 140] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 141/200 [1:20:46<33:38, 34.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 141] \ud83d\udd0d Extracting from: {     \"answer\": \"Martin McCann\",     \"reasoning\": [],     \"evidence\": [       2,       8     ]   }...\n",
      "[Ex 141] \u2705 JSON parse OK: answer='Martin McCann...', citations=[2, 8]\n",
      "[Ex 141] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [8].\",   \"answer\": \"Martin \\\"Marty\\\" McCann\",   \"citations\": [     2,     8   ] }...\n",
      "[Ex 141] \u2705 JSON parse OK: answer='Martin \"Marty\" McCann...', citations=[2, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 142/200 [1:20:58<26:42, 27.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 142] \ud83d\udd0d Extracting from: { \"answer\": \"Bud Wilkinson died in 1963\", \"reasoning\": [ \"From evidence [2] and [6]: Bud Wilkinson coached the 1963 Oklahoma Sooners football team\", \"From evidence [6]: Bud Wilkinson retired from coac...\n",
      "[Ex 142] \u2705 JSON parse OK: answer='Bud Wilkinson died in 1963...', citations=[]\n",
      "[Ex 142] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"February 9, 1994\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 142] \u2705 JSON parse OK: answer='February 9, 1994...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 143/200 [1:21:21<25:00, 26.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 143] \ud83d\udd0d Extracting from: leaves Port Charles.  She later returns to Port Charles and starts a relationship with Dante, who divorces Lulu and marries Valerie.  output\": {     \"answer\": \"None of the given evidences provide info...\n",
      "[Ex 143] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 143] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 143] \u2705 Substring parse OK: answer='None of the given evidences provide information ab...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 143] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 143] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 144/200 [1:21:53<26:01, 27.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 144] \ud83d\udd0d Extracting from: {     \"answer\": \"Japan\",     \"reasoning\": [       \"From evidence [6] and [7]: Huis Ten Bosch Station is a railway station in Japan, and Huis Ten Bosch is a limited express train service in Japan\",    ...\n",
      "[Ex 144] \u2705 JSON parse OK: answer='Japan...', citations=[2, 6, 7]\n",
      "[Ex 144] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Netherlands\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 144] \u2705 JSON parse OK: answer='Netherlands...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 145/200 [1:22:28<27:36, 30.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 145] \ud83d\udd0d Extracting from: {     \"answer\": \"J. G. Quintel\",     \"reasoning\": [       \"From evidence [6] and [7]: Thea Ruth White is best known for her voice over work as Muriel Bagge on 'Courage the Cowardly Dog'\",       \"From ...\n",
      "[Ex 145] \u2705 JSON parse OK: answer='J. G. Quintel...', citations=[6, 7]\n",
      "[Ex 145] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 145] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 146/200 [1:23:07<29:26, 32.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 146] \ud83d\udd0d Extracting from: {   \"answer\": \"The Predelta National Park is located in the provinces of Misiones, Corrientes and Entre R\u00edos in Argentina.\",   \"reasoning\": [     \"From evidence [4]: National Route 12 runs through the...\n",
      "[Ex 146] \u2705 JSON parse OK: answer='The Predelta National Park is located in the provi...', citations=[4, 5]\n",
      "[Ex 146] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 146] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 147/200 [1:24:07<36:04, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 147] \ud83d\udd0d Extracting from: {   \"answer\": \"Brian Gottfried\",   \"reasoning\": [     \"From evidence [1] and [3]: Brian Gottfried won the singles title at the 1974 Paris Open and the 1980 Paris Open\",     \"From evidence [4]: Brian G...\n",
      "[Ex 147] \u2705 JSON parse OK: answer='Brian Gottfried...', citations=[1, 3, 4]\n",
      "[Ex 147] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"21\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 147] \u2705 JSON parse OK: answer='21...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 148/200 [1:24:32<31:27, 36.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 148] \ud83d\udd0d Extracting from: {   \"answer\": \"Tom Vaughan (director)\",   \"reasoning\": [     \"From evidence [1], [3], [4] and [5]: Tom Vaughan is a Scottish television and film director\",     \"From evidence [1], [3], [4] and [5]: To...\n",
      "[Ex 148] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 306). Trying fallback...\n",
      "[Ex 148] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 148] \u2705 Substring parse OK: answer='Tom Vaughan (director)...', citations=[1, 3, 4, 5]\n",
      "[Ex 148] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 148] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 149/200 [1:25:06<30:17, 35.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 149] \ud83d\udd0d Extracting from: {   \"answer\": \"International Ultravtraviolet Explorer (TAUVEX)\",   \"reasoning\": [     \"From evidence [1]: TAUVEX is a space telescope array for ultraviolet (UV) sky exploration and was operational fro...\n",
      "[Ex 149] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 464). Trying fallback...\n",
      "[Ex 149] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 149] \u2705 Substring parse OK: answer='International Ultravtraviolet Explorer (TAUVEX)...', citations=[1, 4]\n",
      "[Ex 149] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 149] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 150/200 [1:26:00<34:18, 41.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 150] \ud83d\udd0d Extracting from: {     \"answer\": \"Li Yitong made her acting debut on a Chinese web series or television adaptation\",     \"reasoning\": [       \"From evidence [5]: Li Yitong made her acting debut in 2016 with a leading ...\n",
      "[Ex 150] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 653). Trying fallback...\n",
      "[Ex 150] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 150] \u2705 Substring parse OK: answer='Li Yitong made her acting debut on a Chinese web s...', citations=[5, 7]\n",
      "[Ex 150] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 150] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 151/200 [1:27:05<39:27, 48.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 151] \ud83d\udd0d Extracting from: {   \"answer\": \"The W. R. Grace Building was completed before 10 Hudson Yards\",   \"reasoning\": [     \"From evidence [5]: The W. R. Grace Building was completed in 1974\"   ],   \"evidence\": [     5   ] }...\n",
      "[Ex 151] \u2705 JSON parse OK: answer='The W. R. Grace Building was completed before 10 H...', citations=[5]\n",
      "[Ex 151] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"W. R. Grace Building\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 151] \u2705 JSON parse OK: answer='W. R. Grace Building...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 152/200 [1:27:25<31:40, 39.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 152] \ud83d\udd0d Extracting from: {   \"answer\": [\"Arya\", \"Bobby Simha\", \"Sri Divya\", \"Rana Daggubati\", \"Raai Laxmi\", \"Parvathy\", \"Samantha\"],   \"reasoning\": [],   \"evidence\": [1] }  The evidence provided states that the Indian Tamil c...\n",
      "[Ex 152] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 148). Trying fallback...\n",
      "[Ex 152] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 152] \u2705 Substring parse OK: answer='Arya, Bobby Simha, Sri Divya, Rana Daggubati, Raai...', citations=[1]\n",
      "[Ex 152] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Ramanaidu Daggubati\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 152] \u2705 JSON parse OK: answer='Ramanaidu Daggubati...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 153/200 [1:28:18<34:13, 43.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 153] \ud83d\udd0d Extracting from: {     \"answer\": \"Craig Nicholls is older than Norman Blake\",     \"reasoning\": [       \"From evidence [5]: Craig Nicholls was born on 31 August 1977\",       \"From evidence [1]: Norman Blake was born on...\n",
      "[Ex 153] \u2705 JSON parse OK: answer='Craig Nicholls is older than Norman Blake...', citations=[1, 5]\n",
      "[Ex 153] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Norman Blake\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 153] \u2705 JSON parse OK: answer='Norman Blake...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 154/200 [1:28:50<30:51, 40.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 154] \ud83d\udd0d Extracting from: {     \"answer\": \"2012 Premier League Darts or 2015 Premier League Darts or 2017 Premier League Darts\",     \"reasoning\": [       \"From evidences [1], [3], [7]: The 2012 Premier League Darts, 2015 Premi...\n",
      "[Ex 154] \u2705 JSON parse OK: answer='2012 Premier League Darts or 2015 Premier League D...', citations=[1, 3, 7]\n",
      "[Ex 154] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"2018 Unibet Premier League Darts\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 154] \u2705 JSON parse OK: answer='2018 Unibet Premier League Darts...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 155/200 [1:29:53<35:15, 47.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 155] \ud83d\udd0d Extracting from: people\" or \"Bai Aku\" in Liberia, are a subgroup of the Manding people of West Africa.  They are mainly found in the Eastern Province of Sierra Leone and in the Grand Gedeh County of Liberia.  They are...\n",
      "[Ex 155] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 155] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 155] \ud83d\udcdd Regex result: answer='people\" or \"Bai Aku\" in Liberia, are a subgroup of...', citations=[]\n",
      "[Ex 155] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 155] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 156/200 [1:31:11<41:18, 56.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 156] \ud83d\udd0d Extracting from: {     \"answer\": \"No\",     \"reasoning\": [],     \"evidence\": [1, 2, 3, 4, 5, 6, 7, 8] }  The evidence provided does not contain any information about Fabi\u00e1n Bielinsky being a director or having any conn...\n",
      "[Ex 156] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 87). Trying fallback...\n",
      "[Ex 156] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 156] \u2705 Substring parse OK: answer='No...', citations=[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[Ex 156] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 156] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 157/200 [1:31:37<33:49, 47.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 157] \ud83d\udd0d Extracting from: { \"answer\": \"ESAF \u2013 Esp\u00edrito Santo Fundos de Investimento Mobili\u00e1rio S.A.\", \"reasoning\": [ \"From evidence [1]: Sporting Portugal Fund is managed by ESAF \u2013 Esp\u00edrito Santo Fundos de Investimento Mobili\u00e1...\n",
      "[Ex 157] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 215). Trying fallback...\n",
      "[Ex 157] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 157] \u2705 Substring parse OK: answer='ESAF \u2013 Esp\u00edrito Santo Fundos de Investimento Mobil...', citations=[]\n",
      "[Ex 157] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Esp\\u00edrito Santo Financial Group\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 157] \u2705 JSON parse OK: answer='Esp\u00edrito Santo Financial Group...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 158/200 [1:32:13<30:42, 43.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 158] \ud83d\udd0d Extracting from: {   \"answer\": \"Pakistan\",   \"reasoning\": [     \"From evidence [8]: Pakistan Super League (PSL) is a men's professional Twenty20 cricket league representing the sport's highest level in Pakistan\"   ], ...\n",
      "[Ex 158] \u2705 JSON parse OK: answer='Pakistan...', citations=[8]\n",
      "[Ex 158] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"Canada\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 158] \u2705 JSON parse OK: answer='Canada...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 159/200 [1:32:33<25:04, 36.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 159] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The given evidences do not provide enough information to determine which battle, Battle of H\u00fcrtgen Forest or Battle of Pusan Perimeter, laste...\n",
      "[Ex 159] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 159] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 159] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 159] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 159] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 160/200 [1:33:05<23:34, 35.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 160] \ud83d\udd0d Extracting from: { \"answer\": \"The Saturn I army is not mentioned in the evidences provided as having fielded a weapon launcher widely known as Blindicide.\", \"reasoning\": [] }  The question asks for the army that field...\n",
      "[Ex 160] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 159). Trying fallback...\n",
      "[Ex 160] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 160] \u2705 Substring parse OK: answer='The Saturn I army is not mentioned in the evidence...', citations=[]\n",
      "[Ex 160] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [8].\",   \"answer\": \"the United States Army\",   \"citations\": [     3,     8   ] }...\n",
      "[Ex 160] \u2705 JSON parse OK: answer='the United States Army...', citations=[3, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 161/200 [1:33:35<21:50, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 161] \ud83d\udd0d Extracting from: {   \"answer\": \"Max Hoffmann masterminded the devastating defeat of the Russian armies in a battle during World War I\",   \"reasoning\": [],   \"evidence\": [1, 8] }  Reasoning: The evidence provides infor...\n",
      "[Ex 161] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 162). Trying fallback...\n",
      "[Ex 161] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 161] \u2705 Substring parse OK: answer='Max Hoffmann masterminded the devastating defeat o...', citations=[1, 8]\n",
      "[Ex 161] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 161] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 162/200 [1:34:17<22:50, 36.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 162] \ud83d\udd0d Extracting from: {     \"answer\": \"Dan Barker, Heater Henderson, Hemant Mehta, Michael Nugent, Goparaju Ramachandra Rao, and Eddie Tabash are atheist activists.\",     \"reasoning\": [],     \"evidence\": [1, 2, 4, 5, 6, 7,...\n",
      "[Ex 162] \u2705 JSON parse OK: answer='Dan Barker, Heater Henderson, Hemant Mehta, Michae...', citations=[1, 2, 4, 5, 6, 7, 8]\n",
      "[Ex 162] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 162] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 163/200 [1:34:38<19:36, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 163] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The given evidences do not provide information about the global rank by population of the city that Daewoong Pharmaceutical Co., Ltd is based...\n",
      "[Ex 163] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 163] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 163] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 163] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 163] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 164/200 [1:34:54<16:09, 26.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 164] \ud83d\udd0d Extracting from: {     \"answer\": \"Russell Hobbs\",     \"reasoning\": [],     \"evidence\": [7, 8] }  Insufficient context to answer the question as it requires knowledge of English rock bands formed in a specific year, wh...\n",
      "[Ex 164] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 80). Trying fallback...\n",
      "[Ex 164] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 164] \u2705 Substring parse OK: answer='Russell Hobbs...', citations=[7, 8]\n",
      "[Ex 164] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 164] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 165/200 [1:35:13<14:17, 24.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 165] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The question asks for the birthdate of a specific Australian dramatic coloratura soprano who taught Simon Gilbert. However, none of the given...\n",
      "[Ex 165] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 165] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 165] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 165] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 165] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 166/200 [1:35:35<13:32, 23.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 166] \ud83d\udd0d Extracting from: {   \"answer\": \"Anna Camp\",   \"reasoning\": [],   \"evidence\": [     7   ] }  Explanation: The evidence provided states that Anna Camp played Jill Mason in the 2008 Broadway revival of \"Equus\" [7]. There...\n",
      "[Ex 166] \u26a0\ufe0f  JSON parse failed: Extra data: line 9 column 1 (char 75). Trying fallback...\n",
      "[Ex 166] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 166] \u2705 Substring parse OK: answer='Anna Camp...', citations=[7]\n",
      "[Ex 166] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 166] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 167/200 [1:36:19<16:28, 29.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 167] \ud83d\udd0d Extracting from: of \"I'm Not the One\" by the Replacements, a song from their 1984 album, \"Let It Be\".  The cover art for the 12\" single and the CD single features a photograph of a woman named Mrs. Washington. [8] Tit...\n",
      "[Ex 167] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 167] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 167] \u2705 Substring parse OK: answer='None of the given evidences represent the cover ar...', citations=[1, 2, 3, 4, 5, 6, 7]\n",
      "[Ex 167] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 167] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 168/200 [1:37:37<23:38, 44.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 168] \ud83d\udd0d Extracting from: {   \"answer\": \"Sellem\",   \"reasoning\": [     \"From evidence [8]: Nico van der Meel is a Dutch tenor who performed the operatic role of Sellem in Igor Stravinsky's 'The Rake's Progress'\",     \"Therefor...\n",
      "[Ex 168] \u2705 JSON parse OK: answer='Sellem...', citations=[8]\n",
      "[Ex 168] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"The Rake's Progress\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 168] \u2705 JSON parse OK: answer='The Rake's Progress...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 169/200 [1:38:08<20:47, 40.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 169] \ud83d\udd0d Extracting from: {     \"answer\": \"Dan Karaty\",     \"reasoning\": [       \"From evidence [7]: Dan Karaty is a judge and choreographer on several versions of 'So You Think You Can Dance'\",       \"From evidence [7]: Karat...\n",
      "[Ex 169] \u26a0\ufe0f  JSON parse failed: Extra data: line 17 column 1 (char 858). Trying fallback...\n",
      "[Ex 169] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 169] \u2705 Substring parse OK: answer='Dan Karaty...', citations=[7, 8]\n",
      "[Ex 169] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 169] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 170/200 [1:39:26<25:44, 51.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 170] \ud83d\udd0d Extracting from: {     \"answer\": \"KSCW-DT\",     \"reasoning\": [       \"From evidence [5]: KSCW-DT is a CW-affiliated television station licensed to Wichita, Kansas, and is part of a duopoly with CBS affiliate KWCH-DT\",...\n",
      "[Ex 170] \u2705 JSON parse OK: answer='KSCW-DT...', citations=[3, 5]\n",
      "[Ex 170] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"KWCH-DT\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 170] \u2705 JSON parse OK: answer='KWCH-DT...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 171/200 [1:40:26<26:12, 54.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 171] \ud83d\udd0d Extracting from: . [9] Title: Shea Stadium - Shea Stadium was a multi-purpose stadium in the Flushing Meadows-Corona Park in the New York City borough of Queens, New York.  It was named for Wilbert Shea, a former New ...\n",
      "[Ex 171] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 171] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 171] \ud83d\udcdd Regex result: answer='Shea Stadium...', citations=[]\n",
      "[Ex 171] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Central Park\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 171] \u2705 JSON parse OK: answer='Central Park...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 172/200 [1:41:44<28:37, 61.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 172] \ud83d\udd0d Extracting from: {     \"answer\": \"Eminem and Anita Baker\",     \"reasoning\": [       \"From evidence [4]: Eminem is a rapper and a star of the film 8 Mile\",       \"From evidence [5]: Luis Resto, a musician who has worke...\n",
      "[Ex 172] \u2705 JSON parse OK: answer='Eminem and Anita Baker...', citations=[4, 5]\n",
      "[Ex 172] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"Eminem\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 172] \u2705 JSON parse OK: answer='Eminem...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 173/200 [1:42:18<23:55, 53.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 173] \ud83d\udd0d Extracting from: {   \"answer\": \"Stacy Barthe penned album tracks for Katy Perry\",   \"reasoning\": [     \"From evidence [1]: Stacy Barthe wrote the song 'Hummingbird Heartbeat' for Katy Perry's album 'Teenage Dream'\",  ...\n",
      "[Ex 173] \u2705 JSON parse OK: answer='Stacy Barthe penned album tracks for Katy Perry...', citations=[1, 3]\n",
      "[Ex 173] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Katy Perry\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 173] \u2705 JSON parse OK: answer='Katy Perry...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 174/200 [1:42:52<20:30, 47.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 174] \ud83d\udd0d Extracting from: {   \"answer\": \"Gangsta's Paradise\",   \"reasoning\": [     \"From evidence [3]: Coolio's song 'Gangsta's Paradise' samples Stevie Wonder's 'Pastime Paradise'\",     \"From evidence [5]: Coolio's greatest h...\n",
      "[Ex 174] \u2705 JSON parse OK: answer='Gangsta's Paradise...', citations=[3, 5]\n",
      "[Ex 174] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [5].\",   \"answer\": \"Gangsta's Paradise\",   \"citations\": [     3,     5   ] }...\n",
      "[Ex 174] \u2705 JSON parse OK: answer='Gangsta's Paradise...', citations=[3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 175/200 [1:43:28<18:19, 43.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 175] \ud83d\udd0d Extracting from: {     \"answer\": \"Fiona Bruce is a British Conservative Party politician and the Member of Parliament for Congleton\" }  Reasoning: [7] states that Fiona Bruce is a British Conservative Party politician...\n",
      "[Ex 175] \u26a0\ufe0f  JSON parse failed: Extra data: line 5 column 1 (char 119). Trying fallback...\n",
      "[Ex 175] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 175] \u2705 Substring parse OK: answer='Fiona Bruce is a British Conservative Party politi...', citations=[]\n",
      "[Ex 175] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"British Conservative Party\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 175] \u2705 JSON parse OK: answer='British Conservative Party...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 176/200 [1:43:49<14:51, 37.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 176] \ud83d\udd0d Extracting from: {   \"answer\": \"The biggest store today owned by the State of Qatar is the Al Shaheen Oil Field\",   \"reasoning\": [     \"From evidence [6]: The Al Shaheen Oil Field is owned by the State of Qatar\",     ...\n",
      "[Ex 176] \u26a0\ufe0f  JSON parse failed: Extra data: line 14 column 1 (char 440). Trying fallback...\n",
      "[Ex 176] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 176] \u2705 Substring parse OK: answer='The biggest store today owned by the State of Qata...', citations=[6, 8]\n",
      "[Ex 176] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 176] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 177/200 [1:45:04<18:35, 48.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 177] \ud83d\udd0d Extracting from: {     \"answer\": \"Ryan Babel and Darryl\",     \"reasoning\": [       \"From evidence [5]: 'Wij zijn Ajax' is a song by Ajax and Friends featuring guest vocals by several Ajax players and Dutch vocalists, ...\n",
      "[Ex 177] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 277). Trying fallback...\n",
      "[Ex 177] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 177] \u2705 Substring parse OK: answer='Ryan Babel and Darryl...', citations=[5]\n",
      "[Ex 177] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Ryan Babel\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 177] \u2705 JSON parse OK: answer='Ryan Babel...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 178/200 [1:45:42<16:35, 45.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 178] \ud83d\udd0d Extracting from: {   \"answer\": \"Mike Farrell\",   \"reasoning\": [     \"From evidence [7]: Mike Farrell made a cameo appearance in the film 'Dayton's Devils'\",     \"Therefore, Mike Farrell appeared in a cameo role in 'MA...\n",
      "[Ex 178] \u2705 JSON parse OK: answer='Mike Farrell...', citations=[7]\n",
      "[Ex 178] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [7].\",   \"answer\": \"Captain B.J. Hunnicutt\",   \"citations\": [     6,     7   ] }...\n",
      "[Ex 178] \u2705 JSON parse OK: answer='Captain B.J. Hunnicutt...', citations=[6, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 179/200 [1:46:08<13:47, 39.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 179] \ud83d\udd0d Extracting from: {     \"answer\": \"Cartoon Network Too\",     \"reasoning\": [       \"From evidence [7]: Nick Jr. Too was launched on the same day as a rival channel\",       \"From evidence [1]: Cartoon Network is a childr...\n",
      "[Ex 179] \u2705 JSON parse OK: answer='Cartoon Network Too...', citations=[1, 7]\n",
      "[Ex 179] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 179] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 180/200 [1:46:45<12:53, 38.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 180] \ud83d\udd0d Extracting from: {     \"answer\": \"Haier (GE Appliances)\",     \"reasoning\": [       \"From evidence [6]: GE Appliances is a appliance company headquartered in Louisville, Kentucky and owned by Haier Group\",       \"There...\n",
      "[Ex 180] \u2705 JSON parse OK: answer='Haier (GE Appliances)...', citations=[6]\n",
      "[Ex 180] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 180] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 181/200 [1:47:10<10:58, 34.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 181] \ud83d\udd0d Extracting from: {   \"answer\": \"Insufficient context\",   \"reasoning\": [] }  The provided evidence does not include any information about when the city George Consider Hale was born in was founded....\n",
      "[Ex 181] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 59). Trying fallback...\n",
      "[Ex 181] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 181] \u2705 Substring parse OK: answer='Insufficient context...', citations=[]\n",
      "[Ex 181] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"in the 1830s\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 181] \u2705 JSON parse OK: answer='in the 1830s...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 182/200 [1:47:23<08:28, 28.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 182] \ud83d\udd0d Extracting from: {   \"answer\": \"Jacques Tourneur\",   \"reasoning\": [     \"From evidence [1], [3], [7], and [8]: Jacques Tourneur has directed a total of at least 6 films: They All Come Out, The Comedy of Terrors, Depar...\n",
      "[Ex 182] \u2705 JSON parse OK: answer='Jacques Tourneur...', citations=[1, 3, 7, 8]\n",
      "[Ex 182] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Jacques Tourneur\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 182] \u2705 JSON parse OK: answer='Jacques Tourneur...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 183/200 [1:47:54<08:11, 28.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 183] \ud83d\udd0d Extracting from: {     \"answer\": \"That Darn Cat!\",     \"reasoning\": [       \"From evidence [7]: That Darn Cat! is a Disney film released in 1965 and starred Dean Jones\",       \"From evidence [6]: Dean Jones also starr...\n",
      "[Ex 183] \u2705 JSON parse OK: answer='That Darn Cat!...', citations=[6, 7]\n",
      "[Ex 183] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"That Darn Cat!\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 183] \u2705 JSON parse OK: answer='That Darn Cat!...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 184/200 [1:48:25<07:52, 29.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 184] \ud83d\udd0d Extracting from: {   \"answer\": \"Glacier National Park\",   \"reasoning\": [     \"From evidence [7]: Mount Stimson is the second highest peak in Glacier National Park\",     \"From evidence [4]: Glacier National Park is a n...\n",
      "[Ex 184] \u2705 JSON parse OK: answer='Glacier National Park...', citations=[7]\n",
      "[Ex 184] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"over 1 million\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 184] \u2705 JSON parse OK: answer='over 1 million...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 185/200 [1:48:49<06:59, 27.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 185] \ud83d\udd0d Extracting from: {   \"answer\": \"Brad Elterman is a professional photographer who addressed the rock 'n' roll lifestyle of various bands including Blondie.\",   \"reasoning\": [],   \"evidence\": [6] }  Explanation: The giv...\n",
      "[Ex 185] \u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 180). Trying fallback...\n",
      "[Ex 185] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 185] \u2705 Substring parse OK: answer='Brad Elterman is a professional photographer who a...', citations=[6]\n",
      "[Ex 185] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"its eclectic mix of musical styles\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 185] \u2705 JSON parse OK: answer='its eclectic mix of musical styles...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 186/200 [1:49:25<07:02, 30.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 186] \ud83d\udd0d Extracting from: { \"answer\": \"No\", \"reasoning\": [ \"From evidence [2] and [5], the Portuguese Podengo and Andalusian Hound are distinct breeds from Portugal and Spain respectively\", \"From evidence [3], the West Siberia...\n",
      "[Ex 186] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 380). Trying fallback...\n",
      "[Ex 186] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 186] \u2705 Substring parse OK: answer='No...', citations=[]\n",
      "[Ex 186] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"no\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 186] \u2705 JSON parse OK: answer='no...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 187/200 [1:50:03<07:04, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 187] \ud83d\udd0d Extracting from: {   \"answer\": [\"Manfred von Richthofen\"],   \"reasoning\": [     \"From evidence [1] and [6]: Manfred von Richthofen, also known as the 'Red Baron', was a fighter pilot with the German Air Force during W...\n",
      "[Ex 187] \u2705 JSON parse OK: answer='Manfred von Richthofen...', citations=[1, 6]\n",
      "[Ex 187] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [6].\",   \"answer\": \"books, films and other media\",   \"citations\": [     1,     6   ] }...\n",
      "[Ex 187] \u2705 JSON parse OK: answer='books, films and other media...', citations=[1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 188/200 [1:50:31<06:13, 31.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 188] \ud83d\udd0d Extracting from: Jeremih is an American R&B singer, songwriter, and record producer.  He was born in Chicago, Illinois, and began his career in 2001 as a member of the R&B group \"Destiny's Child\".  He later went solo ...\n",
      "[Ex 188] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 188] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 188] \ud83d\udcdd Regex result: answer='Jeremih is an American R&B singer, songwriter, and...', citations=[]\n",
      "[Ex 188] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 188] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 189/200 [1:51:48<08:16, 45.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 189] \ud83d\udd0d Extracting from: {     \"answer\": \"Johns Hopkins University\",     \"reasoning\": [       \"From evidence [6]: Johns Hopkins University is located in Baltimore, Maryland, United States\"     ],     \"evidence\": [       6    ...\n",
      "[Ex 189] \u2705 JSON parse OK: answer='Johns Hopkins University...', citations=[6]\n",
      "[Ex 189] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 189] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 190/200 [1:52:07<06:11, 37.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 190] \ud83d\udd0d Extracting from: {     \"answer\": \"Caroline Wozniacki was born earlier than Simone Bolelli\",     \"reasoning\": [       \"From evidence [3] and [6]: Caroline Wozniacki's birth year is mentioned as 1990\",       \"From evide...\n",
      "[Ex 190] \u2705 JSON parse OK: answer='Caroline Wozniacki was born earlier than Simone Bo...', citations=[3, 6]\n",
      "[Ex 190] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 190] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 191/200 [1:52:41<05:25, 36.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 191] \ud83d\udd0d Extracting from: {     \"answer\": \"Operation Overlord\",     \"reasoning\": [       \"From evidence [1] and [5]: Operation Overlord is the codename for the Battle of Normandy and the American airborne landings in Normandy ...\n",
      "[Ex 191] \u2705 JSON parse OK: answer='Operation Overlord...', citations=[1, 5]\n",
      "[Ex 191] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"D-Day\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 191] \u2705 JSON parse OK: answer='D-Day...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 192/200 [1:53:09<04:28, 33.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 192] \ud83d\udd0d Extracting from: {     \"answer\": \"General Motors (GM)\",     \"reasoning\": [       \"From evidence [2]: Holden is an Australian automobile manufacturer that was once a subsidiary of General Motors\",       \"From evidence ...\n",
      "[Ex 192] \u26a0\ufe0f  JSON parse failed: Extra data: line 15 column 1 (char 435). Trying fallback...\n",
      "[Ex 192] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 192] \u2705 Substring parse OK: answer='General Motors (GM)...', citations=[1, 2, 8]\n",
      "[Ex 192] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"General Motors\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 192] \u2705 JSON parse OK: answer='General Motors...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 193/200 [1:54:10<04:54, 42.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 193] \ud83d\udd0d Extracting from: {     \"answer\": \"Frankie Valli\",     \"reasoning\": [],     \"evidence\": [        6,        7     ] }  Frankie Valli is the name of the singer who sang \"Can't Take My Eyes Off You\" [6] and \"My Eyes Adore...\n",
      "[Ex 193] \u26a0\ufe0f  JSON parse failed: Extra data: line 10 column 1 (char 100). Trying fallback...\n",
      "[Ex 193] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 193] \u2705 Substring parse OK: answer='Frankie Valli...', citations=[6, 7]\n",
      "[Ex 193] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 193] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 194/200 [1:54:33<03:36, 36.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 194] \ud83d\udd0d Extracting from: {     \"answer\": \"John Cho\",     \"reasoning\": [       \"From evidence [1]: John Cho is best known for his role as John, MILF Guy No. 2, in the 'American Pie' films\",       \"From evidence [5]: Only Eugen...\n",
      "[Ex 194] \u2705 JSON parse OK: answer='John Cho...', citations=[1, 5]\n",
      "[Ex 194] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"John Candy\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 194] \u2705 JSON parse OK: answer='John Candy...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 195/200 [1:55:08<03:00, 36.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 195] \ud83d\udd0d Extracting from: {   \"answer\": \"Luca Parmitano is younger than Ulrich Walter\",   \"reasoning\": [],   \"evidence\": [     [1],     [4]   ] }  Explanation: From evidence [1], we know that Ulrich Walter was born on February...\n",
      "[Ex 195] \u26a0\ufe0f  JSON parse failed: Extra data: line 10 column 1 (char 121). Trying fallback...\n",
      "[Ex 195] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 195] \u2705 Substring parse OK: answer='Luca Parmitano is younger than Ulrich Walter...', citations=[]\n",
      "[Ex 195] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [4].\",   \"answer\": \"Luca Parmitano\",   \"citations\": [     1,     4   ] }...\n",
      "[Ex 195] \u2705 JSON parse OK: answer='Luca Parmitano...', citations=[1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 196/200 [1:56:24<03:11, 47.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 196] \ud83d\udd0d Extracting from: {   \"answer\": \"Muztagh Ata\",   \"reasoning\": [     \"From evidence [1]: Muztagh Ata is the second highest mountain in the northern edge of the Tibetan Plateau, with a height of 7509 meters\"   ],   \"evid...\n",
      "[Ex 196] \u26a0\ufe0f  JSON parse failed: Extra data: line 11 column 1 (char 222). Trying fallback...\n",
      "[Ex 196] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 196] \u2705 Substring parse OK: answer='Muztagh Ata...', citations=[1]\n",
      "[Ex 196] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Manaslu\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 196] \u2705 JSON parse OK: answer='Manaslu...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 197/200 [1:56:59<02:12, 44.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 197] \ud83d\udd0d Extracting from: {     \"answer\": \"Chuck Taylor began his training at 15 years old\",     \"reasoning\": [       \"From evidence [7]: Chuck Taylor began his wrestling training at age 15 under the tutelage of Brandon Walker...\n",
      "[Ex 197] \u2705 JSON parse OK: answer='Chuck Taylor began his training at 15 years old...', citations=[7]\n",
      "[Ex 197] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"15\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 197] \u2705 JSON parse OK: answer='15...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 198/200 [1:57:23<01:15, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 198] \ud83d\udd0d Extracting from: {     \"answer\": \"Brian Billick\",     \"reasoning\": [       \"From evidence [1] and [8]: Brian Billick was the head coach of the Baltimore Ravens from 1999 to 2007 and specifically from the 6th season of...\n",
      "[Ex 198] \u2705 JSON parse OK: answer='Brian Billick...', citations=[1, 8]\n",
      "[Ex 198] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"6th season\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 198] \u2705 JSON parse OK: answer='6th season...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Baseline RAG Prompting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 199/200 [1:57:55<00:36, 36.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 199] \ud83d\udd0d Extracting from: {     \"answer\": \"Vocelli Pizza and Noble Roman's serve pizza\",     \"evidence\": [2, 5]   }  Reasoning: [   \"From evidence [2]: Vocelli Pizza is a pizzeria\",   \"From evidence [5]: Noble Roman's is a piz...\n",
      "[Ex 199] \u26a0\ufe0f  JSON parse failed: Extra data: line 6 column 1 (char 91). Trying fallback...\n",
      "[Ex 199] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 199] \u2705 Substring parse OK: answer='Vocelli Pizza and Noble Roman's serve pizza...', citations=[2, 5]\n",
      "[Ex 199] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"Pizza\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 199] \u2705 JSON parse OK: answer='Pizza...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline RAG Prompting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [1:58:24<00:00, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca BASELINE RAG PROMPTING - EVALUATION RESULTS\n",
      "================================================================================\n",
      "Total Examples: 200\n",
      "Exact Match (EM): 0.175\n",
      "F1 Score: 0.274\n",
      "Citation Precision: 0.458\n",
      "Citation Recall: 0.703\n",
      "Citation F1: 0.402\n",
      "Insufficient Context Detection: 11.7% (9/77)\n",
      "================================================================================\n",
      "\n",
      "\u2705 Baseline evaluation complete!\n",
      "\ud83d\udcca Key Results:\n",
      "   \u2022 Exact Match: 17.5%\n",
      "   \u2022 F1 Score: 0.274\n",
      "   \u2022 Citation F1: 0.402\n",
      "   \u2022 Insufficient Context Detection: 11.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-Training Baseline Evaluation with RAG Prompting\n",
    "# Evaluate base Mistral-7B-Instruct model before fine-tuning\n",
    "\n",
    "print(\"\ud83d\udd0d Starting baseline evaluation with RAG prompting approach...\")\n",
    "print(f\"   Model: Mistral-7B-Instruct (base, no fine-tuning)\")\n",
    "print(f\"   Strategy: RAG with few-shot exemplars\")\n",
    "print(f\"   Dataset: First 100 examples from eval_dataset\\n\")\n",
    "baseline_model = model\n",
    "# Evaluate using unified function\n",
    "baseline_results = evaluate_model_comprehensive(\n",
    "    model=baseline_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    evaluator=evaluator,\n",
    "    model_name=\"Baseline RAG Prompting\",\n",
    "    max_examples=200,  # Evaluate on first 100 examples\n",
    "    use_rag_prompting=True,\n",
    "    verbose_level=\"sample\",  # Print first 5 examples\n",
    "    wandb_prefix=\"baseline_rag\",\n",
    "    building_prompts=building_prompts_rag\n",
    ")\n",
    "\n",
    "# Store for later comparison\n",
    "print(\"\u2705 Baseline evaluation complete!\")\n",
    "print(f\"\ud83d\udcca Key Results:\")\n",
    "print(f\"   \u2022 Exact Match: {baseline_results['em']:.1%}\")\n",
    "print(f\"   \u2022 F1 Score: {baseline_results['f1']:.3f}\")\n",
    "print(f\"   \u2022 Citation F1: {baseline_results['citation_f1']:.3f}\")\n",
    "print(f\"   \u2022 Insufficient Context Detection: {baseline_results['insufficient_context_rate']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khgFmItD0aKa",
   "metadata": {
    "id": "khgFmItD0aKa"
   },
   "source": [
    "# Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2o1qjj92qks",
   "metadata": {
    "id": "2o1qjj92qks"
   },
   "outputs": [],
   "source": [
    "# Training Configuration - Fixed for compatibility and memory optimization\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 2\n",
    "SAVE_STEPS = 200\n",
    "LOGGING_STEPS = 50\n",
    "WARMUP_STEPS = 100\n",
    "OUTPUT_DIR = \"./qlora-checkpoints\"\n",
    "\n",
    "# Calculate realistic training time\n",
    "effective_batch_size = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "steps_per_epoch = TRAIN_SIZE // effective_batch_size\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "estimated_hours = total_steps * 0.1 / 60  # Rough estimate: 0.1 min per step\n",
    "\n",
    "print(f\"\ud83c\udfaf Training Configuration (Memory Optimized):\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE} (effective: {effective_batch_size})\")\n",
    "print(f\"   Max Seq Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   Save Steps: {SAVE_STEPS}\")\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total steps: {total_steps}\")\n",
    "print(f\"   \ud83d\udcb0 Estimated time: ~{estimated_hours:.1f} hours\")\n",
    "print(f\"   \ud83d\udeab Early stopping: DISABLED (fixes memory issues)\")\n",
    "\n",
    "# Training arguments - EVALUATION DISABLED to prevent memory issues\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Logging - EVALUATION DISABLED\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_strategy=\"no\",  # DISABLED: Prevents CUDA OOM during training\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "\n",
    "    # Model selection - DISABLED since no evaluation during training\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    # load_best_model_at_end=False,  # Disabled (no evaluation to determine \"best\")\n",
    "    # metric_for_best_model=None,    # Disabled\n",
    "    # greater_is_better=None,        # Disabled\n",
    "\n",
    "    # Precision - trying fp16 for better compatibility\n",
    "    fp16=True,  # More compatible than bf16\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    # W&B integration\n",
    "    report_to=\"wandb\",\n",
    "    run_name=RUN_NAME,\n",
    "\n",
    "    # Other optimizations\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# Create callback - adjusted for no early stopping\n",
    "wandb_callback = WandBCheckpointCallback(run, OUTPUT_DIR)\n",
    "\n",
    "# Initialize trainer - no compute_metrics needed since eval is disabled\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_curriculum,\n",
    "    eval_dataset=eval_dataset,  # Still needed for post-training evaluation\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[wandb_callback],\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Training arguments configured (evaluation disabled)!\")\n",
    "print(f\"\ud83d\udcca Estimated training time: ~{estimated_hours:.1f} hours\")\n",
    "print(f\"\ud83d\udcb0 Estimated cost: ${estimated_hours * HOURLY_RATE:.2f}\")\n",
    "print(f\"\ud83c\udfaf Fixed schedule: {NUM_EPOCHS} epochs with curriculum learning\")\n",
    "print(f\"\ud83d\udcbe Memory optimized: No evaluation during training\")\n",
    "print(f\"\u2705 Trainer initialized successfully!\")\n",
    "\n",
    "# Memory check before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    cached = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\n\ud83d\udcbe GPU Memory before training:\")\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Cached: {cached:.2f} GB\")\n",
    "    # print(f\"   Available: {vram_gb - cached:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30foopyzruq",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d3ac3626fa30477fb0bb69ad66aa36d3",
      "faf36b712cd04edcb18e1a42dfa2b191",
      "f2bad34d8730443bae07cbea18976b68",
      "39e312a338354fc887b7e9d1342ab58f",
      "91afc639f87340d3b434764f2eedefdd",
      "64841403cf6c48acb9de83bbbfddc134",
      "a069b5a20a064ec0b063f852c3b4d853",
      "fdbbedc7f1f842498dacb53caac8f6bd",
      "549d04ea663344b3b1d98a16716e497d",
      "191cae93f13d4250a4a4c7269ac36c44",
      "8dc94edbbcc54876bb568618f32affd8"
     ]
    },
    "id": "30foopyzruq",
    "outputId": "fd5bf903-93b6-43ee-8068-3b8c0a44b2a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83c\udfcb\ufe0f Starting QLoRA training with curriculum learning...\n",
      "\ud83c\udfaf Target: Improve Answer F1 score on HotpotQA multihop reasoning\n",
      "\u23f1\ufe0f Estimated time: 2.5+ hours\n",
      "\n",
      "============================================================\n",
      "\ud83d\ude80 TRAINING STARTED - Monitor at: https://wandb.ai/jeffgong11235/hotpotqa-qlora/runs/eod1tqyc\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcda PHASE 1: Curriculum Learning (forced gold passages)\n",
      "   Gold context rate: 100.00%\n",
      "\n",
      "--- Example 1 (HotpotQADataCollator) ---\n",
      "--- Example 1 (HotpotQADataCollator) ---\n",
      "\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "  Input Text Length (tokens): 1330  Input Text Length (tokens): 1728\n",
      "\n",
      "  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "  First Target Token Index in Labels: 1330  First Target Token Index in Labels: 1728\n",
      "\n",
      "  Labels around input_length 1330 (indices 1325-1334): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]  Labels around input_length 1728 (indices 1723-1732): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n",
      "\n",
      "\n",
      "--- Example 2 (HotpotQADataCollator) ---\n",
      "--- Example 2 (HotpotQADataCollator) ---\n",
      "\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "  Input Text Length (tokens): 1005  Input Text Length (tokens): 1381\n",
      "\n",
      "  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "  First Target Token Index in Labels: 1005  First Target Token Index in Labels: 1381\n",
      "\n",
      "  Labels around input_length 1005 (indices 1000-1009): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]  Labels around input_length 1381 (indices 1376-1385): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n",
      "\n",
      "\n",
      "--- Example 3 (HotpotQADataCollator) ---\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "--- Example 3 (HotpotQADataCollator) ---\n",
      "\n",
      "  Input Text Length (tokens): 1321  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Input Text Length (tokens): 2349\n",
      "\n",
      "  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  First Target Token Index in Labels: 1321  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "  Labels around input_length 1321 (indices 1316-1325): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]  First Target Token Index in Labels: 2349\n",
      "\n",
      "  Labels around input_length 2349 (indices 2344-2353): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 38:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ac3626fa30477fb0bb69ad66aa36d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udce6 Adapter zip created: ./qlora-checkpoints/checkpoint-125.zip (221.1 MB)\n",
      "\ud83d\udce4 Uploaded artifact with aliases: ['latest']\n",
      "\n",
      "\ud83d\udcbe Saving checkpoint after Phase 1...\n",
      "\ud83d\udce6 Adapter zip created: ./qlora-checkpoints/checkpoint-125.zip (148.1 MB)\n",
      "\ud83d\udce4 Uploaded artifact with aliases: ['latest']\n",
      "\u2705 Checkpoint saved after Phase 1.\n",
      "\n",
      "\ud83c\udfaf PHASE 2: Realistic Training (gold may be missing)\n",
      "   Gold context rate: 100.00%\n",
      "Continuing for 1 more epochs...\n",
      "\n",
      "\ud83c\udfcb\ufe0f Starting Phase 2, Epoch 2/2\n",
      "\n",
      "--- Example 1 (HotpotQADataCollator) ---\n",
      "--- Example 1 (HotpotQADataCollator) ---\n",
      "\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "  Input Text Length (tokens): 1150  Input Text Length (tokens): 1659\n",
      "\n",
      "  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "  First Target Token Index in Labels: 1150  First Target Token Index in Labels: 1659\n",
      "\n",
      "  Labels around input_length 1659 (indices 1654-1663): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]  Labels around input_length 1150 (indices 1145-1154): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n",
      "\n",
      "\n",
      "--- Example 2 (HotpotQADataCollator) ---\n",
      "\n",
      "--- Example 2 (HotpotQADataCollator) ---\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "  Input Text Length (tokens): 991  Input Text Length (tokens): 1357\n",
      "\n",
      "  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "  First Target Token Index in Labels: 1357  First Target Token Index in Labels: 991\n",
      "\n",
      "  Labels around input_length 1357 (indices 1352-1361): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]  Labels around input_length 991 (indices 986-995): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n",
      "\n",
      "\n",
      "--- Example 3 (HotpotQADataCollator) ---\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...\n",
      "\n",
      "--- Example 3 (HotpotQADataCollator) ---  Input Text Length (tokens): 1200\n",
      "\n",
      "  Full Text (first 400 chars): <s>[INST]  \n",
      "\n",
      "Answer concisely by performing reasoning ONLY with selected sources from the evidences provided with you. Its possible that some of the evidences are irrelevant to the question and answer could not find enough sources to support.\n",
      " Respond with the answer directly and cite indices like [1], [3]([1] refers to the first evidence provided to you). If the an answer could not be reasoned th...  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Input Text Length (tokens): 2535  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "\n",
      "  Tokenized Input IDs (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "  Labels Before Masking Input (first 20): [1, 1, 28792, 16289, 28793, 259, 13, 13, 2820, 16981, 3078, 11973, 486, 13801, 24685, 9688, 9880, 395, 5937, 7291]\n",
      "  First Target Token Index in Labels: 1200\n",
      "  Labels After Masking Input (first 20): [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]  Labels around input_length 1200 (indices 1195-1204): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n",
      "\n",
      "  First Target Token Index in Labels: 2535\n",
      "  Labels around input_length 2535 (indices 2530-2539): [-100, -100, -100, -100, -100, 28747, 13, 28751, 13, 28705]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 38:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udce6 Adapter zip created: ./qlora-checkpoints/checkpoint-125.zip (220.9 MB)\n",
      "\ud83d\udce4 Uploaded artifact with aliases: ['latest']\n",
      "\n",
      "============================================================\n",
      "\u2705 TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\u23f1\ufe0f Total training time: 1.30 hours\n",
      "\n",
      "\ud83e\uddf9 Memory cleanup completed\n"
     ]
    }
   ],
   "source": [
    "# Training Loop with Curriculum Learning\n",
    "print(\"\ud83c\udfcb\ufe0f Starting QLoRA training with curriculum learning...\")\n",
    "print(f\"\ud83c\udfaf Target: Improve Answer F1 score on HotpotQA multihop reasoning\")\n",
    "print(f\"\u23f1\ufe0f Estimated time: {len(train_dataset_curriculum) * NUM_EPOCHS / (BATCH_SIZE * GRAD_ACCUM_STEPS) / 100:.1f}+ hours\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\ud83d\ude80 TRAINING STARTED - Monitor at: {run.url}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Phase 1: Curriculum learning with forced gold passages\n",
    "    print(f\"\\n\ud83d\udcda PHASE 1: Curriculum Learning (forced gold passages)\")\n",
    "    print(f\"   Gold context rate: {sum(ex['has_gold_context'] for ex in train_dataset_curriculum) / len(train_dataset_curriculum):.2%}\")\n",
    "\n",
    "    trainer.train_dataset = train_dataset_curriculum\n",
    "\n",
    "    # Start training for 1 epoch\n",
    "    initial_epochs = 1\n",
    "    training_args.num_train_epochs = initial_epochs\n",
    "    trainer.args = training_args\n",
    "    trainer.train()\n",
    "\n",
    "    # --- Manually save checkpoint after Phase 1 ---\n",
    "    print(\"\\n\ud83d\udcbe Saving checkpoint after Phase 1...\")\n",
    "    trainer.save_model(os.path.join(OUTPUT_DIR, f\"checkpoint-phase1-end-step-{trainer.state.global_step}\"))\n",
    "    # Trigger W&B artifact upload for this specific checkpoint\n",
    "    if wandb_callback:\n",
    "        wandb_callback.on_save(trainer.args, trainer.state, trainer.control, model=trainer.model)\n",
    "    print(\"\u2705 Checkpoint saved after Phase 1.\")\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    print(f\"\\n\ud83c\udfaf PHASE 2: Realistic Training (gold may be missing)\")\n",
    "    print(f\"   Gold context rate: {sum(ex['has_gold_context'] for ex in train_dataset_realistic) / len(train_dataset_realistic):.2%}\")\n",
    "\n",
    "    # Switch to realistic dataset for final epoch\n",
    "    trainer.train_dataset = train_dataset_realistic\n",
    "\n",
    "    # Continue training for remaining epochs\n",
    "    # Note: Setting num_train_epochs here means total epochs will be initial_epochs + remaining epochs if starting from scratch\n",
    "    # If resuming, trainer automatically handles epoch counting.\n",
    "    # For manual phase control, let's train for the difference\n",
    "    remaining_epochs = NUM_EPOCHS - initial_epochs\n",
    "    if remaining_epochs > 0:\n",
    "        print(f\"Continuing for {remaining_epochs} more epochs...\")\n",
    "        for epoch in range(initial_epochs, NUM_EPOCHS):\n",
    "            print(f\"\\n\ud83c\udfcb\ufe0f Starting Phase 2, Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "            trainer.train() # This continues training until trainer.state.epoch reaches NUM_EPOCHS\n",
    "\n",
    "\n",
    "    # Training completed successfully\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\u2705 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\u23f1\ufe0f Total training time: {training_time/3600:.2f} hours\")\n",
    "\n",
    "\n",
    "    # Log training completion\n",
    "    wandb.log({\n",
    "        \"training_completed\": True,\n",
    "        \"total_training_time_hours\": training_time / 3600,\n",
    "\n",
    "        \"curriculum_phases\": 2,\n",
    "        \"final_epoch\": NUM_EPOCHS\n",
    "    })\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n\u26a0\ufe0f Training interrupted by user\")\n",
    "    print(f\"\ud83d\udcbe Last checkpoint should be saved in W&B artifacts\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n\u274c Training failed with error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # Log error\n",
    "    wandb.log({\"training_error\": str(e)})\n",
    "\n",
    "finally:\n",
    "    # Final memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n\ud83e\uddf9 Memory cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mH1FEh8tDob2",
   "metadata": {
    "id": "mH1FEh8tDob2"
   },
   "source": [
    "##  Fine-tuned Model Evaluation\n",
    "\n",
    "This section evaluates the QLoRA fine-tuned model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dz74hfqqlsi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638,
     "referenced_widgets": [
      "a5b76b5ac9f3420fbf7c8f782040b496",
      "620dcb369d8e44469f5e6b1d30c9e944",
      "c01e1ac617d340d3bd6c9dbdd7fb8dfa",
      "2e8a0f38378c460b95102fdff1202f2c",
      "4dd6f9fea4ee4324bdf7485ae29cbc9c",
      "5732a89badd84529b9388f5686d25c98",
      "9178784ffa5642e4bd1b916c8a1ea282",
      "403d5350f8fc408aa5f005a22d16c73c",
      "0d4053f76f8743a09bb511320e2b9df2",
      "9691836563a04a8ba549433f9a9c2987",
      "1d328fb9b8b4483db06b0db7598ff15a"
     ]
    },
    "executionInfo": {
     "elapsed": 43402,
     "status": "ok",
     "timestamp": 1759960521404,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "dz74hfqqlsi",
    "outputId": "4bd91d4e-e159-4b17-cb8f-e0f32290e2b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Loading fine-tuned model from W&B artifact for evaluation...\n",
      "\n",
      "\ud83d\udce5 Downloading the 'latest' model artifact from W&B...\n",
      "{'step': 125, 'epoch': 1.0, 'base_model': 'mistralai/Mistral-7B-Instruct-v0.2', 'train_loss': 0, 'learning_rate': 9.9e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'qlora-adapters:latest', 220.95MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:15.4 (14.4MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Artifact downloaded to: /content/artifacts/qlora-adapters:v37\n",
      "Attempting to extract /content/artifacts/qlora-adapters:v37/checkpoint-125.zip to /content/artifacts/qlora-adapters:v37/checkpoint-125_extracted\n",
      "\u2705 Successfully extracted adapter files to /content/artifacts/qlora-adapters:v37/checkpoint-125_extracted.\n",
      "\n",
      "\ud83d\udd0d Contents of extracted adapter directory (/content/artifacts/qlora-adapters:v37/checkpoint-125_extracted):\n",
      "- trainer_state.json (File, 0.00 MB)\n",
      "- tokenizer.json (File, 3.34 MB)\n",
      "- training_args.bin (File, 0.01 MB)\n",
      "- adapter_config.json (File, 0.00 MB)\n",
      "- special_tokens_map.json (File, 0.00 MB)\n",
      "- scheduler.pt (File, 0.00 MB)\n",
      "- scaler.pt (File, 0.00 MB)\n",
      "- tokenizer.model (File, 0.47 MB)\n",
      "- tokenizer_config.json (File, 0.00 MB)\n",
      "- chat_template.jinja (File, 0.00 MB)\n",
      "- optimizer.pt (File, 81.76 MB)\n",
      "- README.md (File, 0.00 MB)\n",
      "- rng_state.pth (File, 0.01 MB)\n",
      "- adapter_model.safetensors (File, 160.06 MB)\n",
      "----------------------------------------\n",
      "\ud83d\udd04 Loading base Mistral model for evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b76b5ac9f3420fbf7c8f782040b496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Base model loaded.\n",
      "\n",
      "\ud83d\udd27 Attempting to load adapter from path: /content/artifacts/qlora-adapters:v37/checkpoint-125_extracted\n",
      "\u2705 Successfully loaded fine-tuned model from W&B artifact.\n",
      "\u2705 Set eval_model to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Fine-tuned Model from W&B Artifact ---\n",
    "print(\"\ud83d\udcca Loading fine-tuned model from W&B artifact for evaluation...\")\n",
    "\n",
    "# --- Define bnb_config here to make the cell more self-contained ---\n",
    "# This is needed to load the base quantized model\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "eval_model = None\n",
    "base_model_for_eval = None\n",
    "adapter_path = None\n",
    "\n",
    "try:\n",
    "    # Ensure W&B run object is available\n",
    "    if 'run' not in globals() or run is None:\n",
    "        print(\"\u274c W&B run object not found. Cannot download artifact.\")\n",
    "        raise RuntimeError(\"W&B run object not initialized.\")\n",
    "\n",
    "    # Download the 'latest' model artifact from W&B\n",
    "    print(\"\\n\ud83d\udce5 Downloading the 'latest' model artifact from W&B...\")\n",
    "    artifact = run.use_artifact(f\"qlora-adapters:latest\")\n",
    "    print(artifact.metadata)\n",
    "    artifact_dir = artifact.download() # Downloads the artifact contents (including the zip)\n",
    "    print(f\"\u2705 Artifact downloaded to: {artifact_dir}\")\n",
    "\n",
    "    # --- Find and extract the zip file ---\n",
    "    import zipfile\n",
    "    import os\n",
    "\n",
    "    zip_files = [f for f in os.listdir(artifact_dir) if f.endswith('.zip')]\n",
    "\n",
    "    if not zip_files:\n",
    "         print(\"\u274c No zip file found in the artifact.\")\n",
    "         raise FileNotFoundError(\"Adapter zip file not found in the downloaded artifact.\")\n",
    "\n",
    "    zip_path = os.path.join(artifact_dir, zip_files[0])\n",
    "    # Extract to a subdirectory within the artifact download directory\n",
    "    # Use a more robust extraction dir name based on zip filename\n",
    "    extract_dir_name = os.path.splitext(zip_files[0])[0] + \"_extracted\"\n",
    "    extract_dir = os.path.join(artifact_dir, extract_dir_name)\n",
    "    os.makedirs(extract_dir, exist_ok=True)\n",
    "    print(f\"Attempting to extract {zip_path} to {extract_dir}\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zipf:\n",
    "        zipf.extractall(extract_dir)\n",
    "\n",
    "    print(f\"\u2705 Successfully extracted adapter files to {extract_dir}.\")\n",
    "    # Now the adapter path is the extracted directory\n",
    "    adapter_path = extract_dir\n",
    "\n",
    "    # --- DEBUG: List contents of extracted directory ---\n",
    "    print(f\"\\n\ud83d\udd0d Contents of extracted adapter directory ({adapter_path}):\")\n",
    "    if os.path.exists(adapter_path):\n",
    "        extracted_contents = os.listdir(adapter_path)\n",
    "        if extracted_contents:\n",
    "            for item in extracted_contents:\n",
    "                item_path = os.path.join(adapter_path, item)\n",
    "                item_type = \"Dir\" if os.path.isdir(item_path) else \"File\"\n",
    "                try:\n",
    "                    item_size = os.path.getsize(item_path) / 1024 / 1024 # Size in MB\n",
    "                    print(f\"- {item} ({item_type}, {item_size:.2f} MB)\")\n",
    "                except Exception as size_e:\n",
    "                    print(f\"- {item} ({item_type}, Error getting size: {size_e})\")\n",
    "        else:\n",
    "            print(\"The extracted directory is empty.\")\n",
    "    else:\n",
    "        print(\"Extracted directory not found.\")\n",
    "    print(\"-\" * 40)\n",
    "    # --- END DEBUG ---\n",
    "\n",
    "\n",
    "    # Load the base model\n",
    "    print(\"\ud83d\udd04 Loading base Mistral model for evaluation...\")\n",
    "    if baseline_model not in globals() or baseline_model is None:\n",
    "      base_model_for_eval = AutoModelForCausalLM.from_pretrained(\n",
    "          MODEL_NAME, # Ensure MODEL_NAME is defined\n",
    "          quantization_config=bnb_config,\n",
    "          device_map=\"auto\",\n",
    "          torch_dtype=torch.bfloat16, # Ensure torch is imported\n",
    "          cache_dir=CACHE_DIR,        # Ensure CACHE_DIR is defined\n",
    "          trust_remote_code=True,\n",
    "          use_cache=False # Important for evaluation\n",
    "      )\n",
    "    else:\n",
    "        base_model_for_eval = baseline_model\n",
    "    print(\"\u2705 Base model loaded.\")\n",
    "\n",
    "    # Load the PEFT adapter onto the base model\n",
    "    print(f\"\\n\ud83d\udd27 Attempting to load adapter from path: {adapter_path}\")\n",
    "    from peft import PeftModel # Ensure PeftModel is imported\n",
    "    eval_model = PeftModel.from_pretrained(base_model_for_eval, adapter_path)\n",
    "    print(\"\u2705 Successfully loaded fine-tuned model from W&B artifact.\")\n",
    "    eval_model.eval() # Set the model to evaluation mode\n",
    "    print(\"\u2705 Set eval_model to evaluation mode.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to load fine-tuned model from W&B artifact: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "#     # Clean up loaded components if any\n",
    "#     if 'eval_model' in locals() and eval_model is not None: del eval_model\n",
    "#     if 'base_model_for_eval' in locals() and base_model_for_eval is not None: del base_model_for_eval\n",
    "#     if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "#     raise RuntimeError(\"Failed to load fine-tuned model for evaluation.\") from e\n",
    "\n",
    "# print(\"\\n\u2705 Fine-tuned model loaded successfully as 'eval_model'!\")\n",
    "# print(\"\ud83d\udcdd Next step: Implement the evaluation loop using 'eval_model'.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The rest of the evaluation logic will go into a subsequent cell based on the plan.\n",
    "# This cell is ONLY for loading the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oscwhyjtii9",
   "metadata": {
    "id": "oscwhyjtii9"
   },
   "source": [
    "### Inference Demo & Sanity Check\n",
    "\n",
    "Quick inference on sample examples to verify model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mYwyA1KR5Xo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12596,
     "status": "ok",
     "timestamp": 1759960534001,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "4mYwyA1KR5Xo",
    "outputId": "b1f61dce-55bc-4b0b-ab97-7d2f09177f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udcac Chatting with the fine-tuned model:\n",
      "Prompt: What is the capital of France?\n",
      "Response: \n",
      "\n",
      "Paris\n",
      "\n",
      "The capital city of France is Paris. Paris is known as the \"City of Light,\" as well as the \"City of Love.\" It's a major European cultural, economic, and political center, and home to\n"
     ]
    }
   ],
   "source": [
    "#sanity check with Example chat with the fine-tuned model\n",
    "print(\"\\n\ud83d\udcac Chatting with the fine-tuned model:\")\n",
    "\n",
    "# Define a simple prompt\n",
    "chat_prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(\n",
    "    chat_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=MAX_SEQ_LENGTH - 100 # Ensure space for generation\n",
    ").to(eval_model.device)\n",
    "\n",
    "# Generate a response\n",
    "with torch.no_grad():\n",
    "    outputs = eval_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50, # Generate up to 50 new tokens\n",
    "        temperature=0.7, # Add some randomness\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode the generated response\n",
    "# Decode only the new tokens generated by the model\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {chat_prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oY8FXUeGFSBm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 366041,
     "status": "ok",
     "timestamp": 1759961183575,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "oY8FXUeGFSBm",
    "outputId": "d973ea8c-288c-4511-b539-f6bf7254879c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\uddea FINE-TUNED MODEL INFERENCE DEMO: Debugging and Visualization\n",
      "================================================================================\n",
      "\u2705 Using loaded fine-tuned model ('eval_model') for demo.\n",
      "\n",
      "\ud83d\udcca Evaluation dataset size: 200\n",
      "\ud83d\udcdd Testing on 20 examples with max_new_tokens=300...\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 1: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Bangalore Naatkal starred which actor and photographer?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [2], [4].\",\n",
      "  \"answer\": \"Ramanaidu Daggubati\",\n",
      "  \"citations\": [\n",
      "    2,\n",
      "    4\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Antha Ezhu Naatkal: Antha 7 Naatkal (read as \"\"Antha Ezhu Naatkal\"\"; English: \"Those Seven Days\" ) is a 1981 Tamil langu...\n",
      "   [2] Bangalore Naatkal: Bangalore Naatkal (English: \"Bangalore Days\" ) is a 2016 Indian Tamil comedy-drama film directed by ...\n",
      "   [3] Saad Khan: Saad Khan, born in Mumbai, India, is an Indian film director, screenwriter, acting teacher, founder ...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 6218\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 468\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [2] shows that It also stars Rana Daggubati, Raai Laxmi, Parvathy and Samantha in other pivotal roles., and evidence [8] indicates that His latest Tamil film Bangalore Naatkal starring Arya, Rana Daggubati, Samantha Ruth Prabhu, Sri Divya, Bobby Simha was released in 2016 and had mixed responses.. Based on [2], [8], the answer is Rana Daggubati.\",\n",
      "  \"answer\": \"Rana Daggubati\",\n",
      "  \"citations\": [\n",
      "    2,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [2] shows that It also stars Rana Daggubati, Raai Laxmi, Parvathy and Samantha in other pivotal roles., and evidence [8] indicates that His latest T...\n",
      "\u2705 JSON parse OK: answer='Rana Daggubati...', citations=[2, 8]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Ramanaidu Daggubati\",   \"citations\": [     2,     4   ] }...\n",
      "\u2705 JSON parse OK: answer='Ramanaidu Daggubati...', citations=[2, 4]\n",
      "\n",
      "   Metrics - F1: 0.500 | EM: 0.000 | Citations: [2, 8] (Gold: [2, 4])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 2: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: What years did Jose Gonzalo Rodriguez Gacha and other leaders of fthe Medallin Cartel operate in Boliva, Colombia, Central America, Peru, the United States, Canada, and Europe?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [3], [6].\",\n",
      "  \"answer\": \"1970s and 1980s\",\n",
      "  \"citations\": [\n",
      "    3,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Juan Carlos Ram\u00edrez Abad\u00eda: Juan Carlos Ram\u00edrez Abad\u00eda (Alias \"Chupeta\") (born February 16, 1963 in Palmira, Colombia) is a drug...\n",
      "   [2] National Liberation Army (Colombia): The National Liberation Army (Spanish: Ej\u00e9rcito de Liberaci\u00f3n Nacional, ELN) is an armed group invol...\n",
      "   [3] Medell\u00edn Cartel: The Medell\u00edn Cartel was a ruthless, highly organized and much-feared Colombian drug cartel originati...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 10655\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 166\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"1970s and 1980s\",   \"citations\": [     3,     6   ] }...\n",
      "\u2705 JSON parse OK: answer='1970s and 1980s...', citations=[3, 6]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [] (Gold: [3, 6])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 3: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Who performed the Simon & Garfunkel song that was a follow-up single to \"The Boxer\" on Australian Idol? \n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Roses in the Snow: Roses in the Snow is the seventh album by country music artist Emmylou Harris, released in 1980.  Wh...\n",
      "   [2] Angels Brought Me Here: \"Angels Brought Me Here\" (aka \"Faith Has Brought Me Here\") is a pop song performed by Australian sin...\n",
      "   [3] Introducing Stan Walker: Introducing... Stan Walker is the debut studio album by season seven \"Australian Idol\" winner, Stan ...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 8362\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 166\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 4: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: The Atik Valide Mosque and Valens Aqueduct are found in what country?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Pertevniyal Valide Sultan Mosque: The Pertevniyal Valide Sultan Mosque, also known as the Aksaray Valide Mosque (Turkish: \"Pertevniyal...\n",
      "   [2] New Mosque (Istanbul): The Yeni Cami (pronounced \"Yeni jami\"), meaning New Mosque; originally named the Valide Sultan Mosqu...\n",
      "   [3] Atik Ali Pasha Mosque: Atik Ali Pasha Mosque may refer to one of two mosques built in the Fatih district of Istanbul, Turke...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 4979\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 553\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [4] shows that The Atik Valide Mosque (Turkish: \\\"Atik Valide Camii, Eski Valide Camii\\\" ) is an Ottoman mosque located on the hill above a large and densely populated district of \\u00dcsk\\u00fcdar, in Istanbul, Turkey., and evidence [6] indicates that The Yeni Valide Mosque (Turkish: \\\"Yeni Valide Camii\\\" ) is an 18th-century Ottoman mosque in the \\u00dcsk\\u00fcdar district of Istanbul, Turkey.. Based on [4], [6], the answer is Turkey.\",\n",
      "  \"answer\": \"Turkey\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [4] shows that The Atik Valide Mosque (Turkish: \\\"Atik Valide Camii, Eski Valide Camii\\\" ) is an Ottoman mosque located on the hill above a large an...\n",
      "\u2705 JSON parse OK: answer='Turkey...', citations=[4, 6]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [4, 6] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 5: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Along side Thomas Alan, which singer's first three albums were recorded at The Boarding House?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] W C A Boarding House: The W C A Boarding House is a historic boarding house at 19 Bliss Street in Springfield, Massachuset...\n",
      "   [2] Brann Boardinghouse: The Brann Boardinghouse is a historic boarding house located on Bryan Street in Tonopah, Nevada.  Th...\n",
      "   [3] The Cure: 'Reflections': The Cure: \"Reflections\" refers to a set of shows in which The Cure played their first three albums \"...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 6349\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 166\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 6: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: The actor that played Gutthi on Comedy nights with Kapil Show also starred in what 2016 Punjabi film directed by Smeep Kang?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Bhaji in Problem: Bhaji in Problem (Punjabi: \u0a2d\u0a3e\u0a1c\u0a40 \u0a07\u0a28 \u0a2a\u0a4d\u0a30\u0a3e\u0a2c\u0a32\u0a2e ) is a 2013 Indian Punjabi-language comedy film directed ...\n",
      "   [2] Sumona Chakravarti: Sumona Chakravarti is an Indian film and Television actress who began her acting career at the age o...\n",
      "   [3] Lucky Di Unlucky Story: Lucky Di Unlucky Story is a 2013 Punjabi comedy film directed by Smeep Kang, and featuring Gippy Gre...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 6267\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\u274c Parsing error: Extra data: line 7 column 1 (char 167)\n",
      "\ud83d\udd04 Falling back to fallback parser...\n",
      "\ud83d\udd04 fallback_parse called...\n",
      "   Input:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"...\n",
      "\u26a0\ufe0f fallback_parse: JSON failed, using regex\n",
      "\ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "\u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "\ud83d\udd0d Found JSON substring, parsing...\n",
      "\u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "\ud83d\udd04 Using regex fallback...\n",
      "\ud83d\udcdd Regex result: answer='insufficient context...', citations=[2, 5]\n",
      "\u2705 QAOutput created from regex\n",
      "Generated answer length: 955\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "To answer this question, evidence [2] shows that From June 2013 to January 2016 she was seen as Manju Sharma in Comedy Nights with Kapil where she played the role of Kapil Sharma's wife., and evidence [5] indicates that Lock is a 2016 Indian Punjabi-language film directed by Smeep Kang, written by Pali Bhupinder Singh and starring Gippy Grewal, Gurpreet Ghuggi, Geeta Basra, Karamjit Anmol & Smeep Kang as the main protagonist of the film and released worldwide on 14 October 2016.. Based on [2], [5], the answer is Lock.\",\n",
      "  \"answer\": \"Lock\",\n",
      "  \"citations\": [\n",
      "    2,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\n",
      "To answer this question, evidence [2] shows that From June 2013 to January 2016 she was seen as Manju Sharma in Comedy Nights with Kapil where she played the role of Kapil Sharma's wife., and evidence [5]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidence...\n",
      "\u26a0\ufe0f  JSON parse failed: Extra data: line 7 column 1 (char 167). Trying fallback...\n",
      "\ud83d\udd0d Found JSON substring, parsing...\n",
      "\u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "\ud83d\udd04 Using regex fallback...\n",
      "\ud83d\udcdd Regex result: answer='insufficient context...', citations=[2, 5]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [2, 5] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 7: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: According to the 2010 census, what was the population of the city after which the vice president, in April 1813, was named?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Elbridge Gerry: Elbridge Gerry ( ; July 17, 1744 (O.S. July 6, 1744) \u2013 November 23, 1814) was an American statesman ...\n",
      "   [2] Zhang Rong (physicist): Zhang Rong (, born February 1964 in Huai'an) is a Chinese physicist who has worked in the area of wi...\n",
      "   [3] Dallas County, Texas: Dallas County is a county in the U.S. state of Texas.  As of the 2010 census, the population was 2,3...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 6003\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 166\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 8: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: What park is just south of the location where the New York Jets played prior to moving to Shea Stadium? \n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [5], [8].\",\n",
      "  \"answer\": \"Central Park\",\n",
      "  \"citations\": [\n",
      "    5,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] 1977 New York Jets season: The 1977 New York Jets season was the 18th season for the team and the 8th in the National Football ...\n",
      "   [2] History of the New York Jets: The history of the New York Jets American football team began in 1959 with the founding of the Titan...\n",
      "   [3] Alex Anthony: Alex Anthony is best known as the Public Address announcer for Major League Baseball's New York Mets...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 8824\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 166\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Central Park\",   \"citations\": [     5,     8   ] }...\n",
      "\u2705 JSON parse OK: answer='Central Park...', citations=[5, 8]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [] (Gold: [5, 8])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 9: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Who wrote Tom Vaughan's popular 2008 film?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [4], [5].\",\n",
      "  \"answer\": \"Dana Fox\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Starter for 10 (film): Starter for 10 is a 2006 British comedy-drama film directed by Tom Vaughan from a screenplay by Davi...\n",
      "   [2] Tom Vaughan (actor): Tom Vaughan (born in Stafford, on 4 August 1985) is an English television actor, best known for play...\n",
      "   [3] Some Kind of Beautiful: Some Kind of Beautiful (Canadian title: How to Make Love Like an Englishman, UK title: Lessons in Lo...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 4563\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 436\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [4] shows that Tom Vaughan (born 5 September 1969) is a Scottish television and film director., and evidence [5] indicates that What Happens in Vegas is a 2008 American comedy film directed by Tom Vaughan, written by Dana Fox and starring Cameron Diaz and Ashton Kutcher.. Based on [4], [5], the answer is Tom Vaughan.\",\n",
      "  \"answer\": \"Tom Vaughan\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [4] shows that Tom Vaughan (born 5 September 1969) is a Scottish television and film director., and evidence [5] indicates that What Happens in Vega...\n",
      "\u2705 JSON parse OK: answer='Tom Vaughan...', citations=[4, 5]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"Dana Fox\",   \"citations\": [     4,     5   ] }...\n",
      "\u2705 JSON parse OK: answer='Dana Fox...', citations=[4, 5]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [4, 5] (Gold: [4, 5])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 10: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Ralph Hefferline was a psychology professor at a university that is located in what city?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [4], [7].\",\n",
      "  \"answer\": \"New York City\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Frank L. Schmidt: Frank L. Schmidt is a retired American psychology professor (University of Iowa) known for his work ...\n",
      "   [2] Princeton Neuroscience Institute: The Princeton Neuroscience Institute (PNI) is a center for neuroscience research at Princeton Univer...\n",
      "   [3] Stanley Coren: Stanley Coren (born 1942) is a psychology professor and neuropsychological researcher who has become...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 6945\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 605\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [4] shows that Ralph Franklin Hefferline (15 February 1910 in Muncie, Indiana \\u2013 16 March 1974) was a psychology professor at Columbia University., and evidence [7] indicates that Columbia University (Columbia; officially Columbia University in the City of New York), established in 1754, is a private Ivy League research university in Upper Manhattan, New York City, often cited as one of the world's most prestigious universities.. Based on [4], [7], the answer is New York City.\",\n",
      "  \"answer\": \"New York City\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [4] shows that Ralph Franklin Hefferline (15 February 1910 in Muncie, Indiana \\u2013 16 March 1974) was a psychology professor at Columbia Universit...\n",
      "\u2705 JSON parse OK: answer='New York City...', citations=[4, 7]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"New York City\",   \"citations\": [     4,     7   ] }...\n",
      "\u2705 JSON parse OK: answer='New York City...', citations=[4, 7]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [4, 7] (Gold: [4, 7])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 11: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Which Dutch media tycoon transferred all of its media activities to Talpa Holding?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [5], [7].\",\n",
      "  \"answer\": \"John de Mol Jr.\",\n",
      "  \"citations\": [\n",
      "    5,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Sky Radio: Sky Radio is a Dutch commercial radio station playing non-stop Adult Contemporary-pop music and is o...\n",
      "   [2] Tien (TV channel): Tien (meaning \"Ten\" in Dutch), previously known as Talpa, was the name of a commercial television ch...\n",
      "   [3] TV 538: TV 538 is a music television channel that airs music videos and live coverage of its radio broadcast...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 4787\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 332\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [5] shows that Talpa Holding is the company in which John de Mol Jr., and evidence [7] indicates that (born 24 April 1955 in The Hague) is a Dutch media tycoon.. Based on [5], [7], the answer is John de Mol Jr.\",\n",
      "  \"answer\": \"John de Mol Jr.\",\n",
      "  \"citations\": [\n",
      "    5,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [5] shows that Talpa Holding is the company in which John de Mol Jr., and evidence [7] indicates that (born 24 April 1955 in The Hague) is a Dutch m...\n",
      "\u2705 JSON parse OK: answer='John de Mol Jr....', citations=[5, 7]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"John de Mol Jr.\",   \"citations\": [     5,     7   ] }...\n",
      "\u2705 JSON parse OK: answer='John de Mol Jr....', citations=[5, 7]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [5, 7] (Gold: [5, 7])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 12: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question:  Dayton's Devils had a cameo from the \"MASH\" star who played what role on the show?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [2], [6].\",\n",
      "  \"answer\": \"Captain B.J. Hunnicutt\",\n",
      "  \"citations\": [\n",
      "    2,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Shamata Anchan: Shamata Anchan is a model and Indian television actress.  She is from Mangalore, Karnataka.  She is ...\n",
      "   [2] Mike Farrell: Michael Joseph Farrell, Jr. (born February 6, 1939) is an American actor, best known for his role as...\n",
      "   [3] Andy Taylor (The Andy Griffith Show): Sheriff Andrew \"Andy\" Jackson Taylor and in earlier episodes as Cousin Andy by Barney Fife is the ma...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 7429\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 332\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [2] shows that Hunnicutt on the television series \\\"M*A*S*H\\\" (1975\\u201383)., and evidence [6] indicates that \\\" M*A*S*H\\\" star Mike Farrell made a cameo appearance.. Based on [2], [6], the answer is Captain B.J.\",\n",
      "  \"answer\": \"Captain B.J.\",\n",
      "  \"citations\": [\n",
      "    2,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [2] shows that Hunnicutt on the television series \\\"M*A*S*H\\\" (1975\\u201383)., and evidence [6] indicates that \\\" M*A*S*H\\\" star Mike Farrell made a...\n",
      "\u2705 JSON parse OK: answer='Captain B.J....', citations=[2, 6]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"Captain B.J. Hunnicutt\",   \"citations\": [     2,     6   ] }...\n",
      "\u2705 JSON parse OK: answer='Captain B.J. Hunnicutt...', citations=[2, 6]\n",
      "\n",
      "   Metrics - F1: 0.800 | EM: 0.000 | Citations: [2, 6] (Gold: [2, 6])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 13: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: What was the father of Kasper Schmeichel voted to be by the IFFHS in 1992?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] IFFHS World's Best Goalkeeper: The IFFHS World's Best Goalkeeper is a football award given annually since 1987 to the most outstand...\n",
      "   [2] Kasper Hvidt: Kasper Hvidt (born 6 February 1976 in Copenhagen) is a Danish retired handball goalkeeper, who lastl...\n",
      "   [3] Isaak Hassler: Isaak Hassler (c. 1530, St. Joachimsthal \u2013 14 July 1591, Nuremberg) was a German Lutheran organist a...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 5190\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 567\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [4] shows that Peter Boles\\u00f3w Schmeichel MBE (] ; born 18 November 1963) is a Danish former professional footballer who played as a goalkeeper, and was voted the IFFHS World's Best Goalkeeper in 1992 and 1993., and evidence [5] indicates that In 1999, he was voted World Player of the Century by the International Federation of Football History & Statistics (IFFHS).. Based on [4], [5], the answer is World Player of the Century.\",\n",
      "  \"answer\": \"World Player of the Century\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [4] shows that Peter Boles\\u00f3w Schmeichel MBE (] ; born 18 November 1963) is a Danish former professional footballer who played as a goalkeeper, ...\n",
      "\u2705 JSON parse OK: answer='World Player of the Century...', citations=[4, 5]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [4, 5] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 14: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: My Neighbor Totoro was produced by a Japanese animation film studio founded in what year?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Makiko Futaki: Makiko Futaki (June 19, 1958 \u2013 May 13, 2016) was a Japanese animator best known for her work at Stud...\n",
      "   [2] Ghibli Museum: The Ghibli Museum (\u4e09\u9df9\u306e\u68ee\u30b8\u30d6\u30ea\u7f8e\u8853\u9928 , Mitaka no Mori Jiburi Bijutsukan , Mitaka Forest Ghibli Museum) is a...\n",
      "   [3] My Neighbor Totoro: My Neighbor Totoro (Japanese: \u3068\u306a\u308a\u306e\u30c8\u30c8\u30ed , Hepburn: Tonari no Totoro ) is a 1988 Japanese animated fant...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 5688\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 461\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [3] shows that My Neighbor Totoro (Japanese: \\u3046\\u305a\\u304d\\u305a , Hepburn: Tonari no Totoro ) is a 1988 Japanese animated fantasy film written and directed by Hayao Miyazaki and produced by Studio Ghibli., and evidence [7] indicates that Studio Ghibli is a Japanese animation film studio founded in 1985.. Based on [3], [7], the answer is 1985.\",\n",
      "  \"answer\": \"1985\",\n",
      "  \"citations\": [\n",
      "    3,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [3] shows that My Neighbor Totoro (Japanese: \\u3046\\u305a\\u304d\\u305a , Hepburn: Tonari no Totoro ) is a 1988 Japanese animated fantasy film written...\n",
      "\u2705 JSON parse OK: answer='1985...', citations=[3, 7]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [3, 7] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 15: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: What country is the theme park served by the Huis Ten Bosch train service themed on?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Oranjezaal: The Oranjezaal refers to a painted ballroom in the Royal palace Huis ten Bosch in the Hague.  It was...\n",
      "   [2] Huis Ten Bosch Station: Huis Ten Bosch Station (\u30cf\u30a6\u30b9\u30c6\u30f3\u30dc\u30b9\u99c5 , Hausutenbosu-eki ) is a railway station on the \u014cmura Line in Haen...\n",
      "   [3] Animal theme park: An animal theme park, also known as a zoological theme park, is a combination of a theme park and a ...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 4846\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 572\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [2] shows that Huis Ten Bosch Station (\\u4e09\\u6c60\\u53f2 , Hausutenbosu-eki ) is a railway station on the \\u00f3mura Line in Haenosaki-ch\\u00f4, Sasebo, Nagasaki Prefecture, Japan, is operated by Kyushu Railway Company (JR Kyushu) and is., and evidence [4] indicates that It runs between Hakata Station in Fukuoka, Fukuoka, and Huis Ten Bosch Station, the station for the Huis Ten Bosch theme park in Sasebo, Nagasaki.. Based on [2], [4], the answer is Japan.\",\n",
      "  \"answer\": \"Japan\",\n",
      "  \"citations\": [\n",
      "    2,\n",
      "    4\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [2] shows that Huis Ten Bosch Station (\\u4e09\\u6c60\\u53f2 , Hausutenbosu-eki ) is a railway station on the \\u00f3mura Line in Haenosaki-ch\\u00f4, Sa...\n",
      "\u2705 JSON parse OK: answer='Japan...', citations=[2, 4]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [2, 4] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 16: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: What age did a wrestler began his training at who began his friendship with Trevor Mann in Brookport, Illinois CPW?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [4], [6].\",\n",
      "  \"answer\": \"15\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Cave-In-Rock Ferry: The Cave-In-Rock Ferry is one of three passenger ferry services that cross the Ohio River into the U...\n",
      "   [2] U.S. Route 45 in Kentucky: U.S. Route 45 (US 45) enters Kentucky at Fulton in Fulton County and travels northeast through Hickm...\n",
      "   [3] Brookport Bridge: The Brookport Bridge (officially the Irvin S. Cobb Bridge) is a ten-span, steel deck (grate), narrow...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 5184\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 457\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [4] shows that The most prominent of these was the then-newly restarted Chaos Pro Wrestling (CPW) in Brookport, Illinois, where he began his friendship with Ricochet., and evidence [6] indicates that Trevor Mann (born October 11, 1988) is an American professional wrestler best known by the ring name Ricochet.. Based on [4], [6], the answer is 15.\",\n",
      "  \"answer\": \"15\",\n",
      "  \"citations\": [\n",
      "    4,\n",
      "    6\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [4] shows that The most prominent of these was the then-newly restarted Chaos Pro Wrestling (CPW) in Brookport, Illinois, where he began his friends...\n",
      "\u2705 JSON parse OK: answer='15...', citations=[4, 6]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"15\",   \"citations\": [     4,     6   ] }...\n",
      "\u2705 JSON parse OK: answer='15...', citations=[4, 6]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [4, 6] (Gold: [4, 6])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 17: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question:  Who was born earlier, Simone Bolelli or Caroline Wozniacki?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [5], [7].\",\n",
      "  \"answer\": \"Simone Bolelli\",\n",
      "  \"citations\": [\n",
      "    5,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] 2012 BMW Open \u2013 Doubles: Simone Bolelli and Horacio Zeballos were the defending champions but decided not to participate.  Bo...\n",
      "   [2] 2012 ATP Vegeta Croatia Open Umag \u2013 Doubles: Simone Bolelli and Fabio Fognini were the defending champions but Bolelli decided not to participate...\n",
      "   [3] 2017 Dubai Tennis Championships \u2013 Men's Doubles: Simone Bolelli and Andreas Seppi were the defending champions, but Bolelli chose to compete in S\u00e3o P...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 4861\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 388\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [5] shows that Simone Bolelli (born 8 October 1985; ] ) is an Italian professional tennis player., and evidence [7] indicates that Caroline Wozniacki (] , ] ; born 11 July 1990) is a Danish professional tennis player.. Based on [5], [7], the answer is Simone Bolelli.\",\n",
      "  \"answer\": \"Simone Bolelli\",\n",
      "  \"citations\": [\n",
      "    5,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [5] shows that Simone Bolelli (born 8 October 1985; ] ) is an Italian professional tennis player., and evidence [7] indicates that Caroline Wozniack...\n",
      "\u2705 JSON parse OK: answer='Simone Bolelli...', citations=[5, 7]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"Simone Bolelli\",   \"citations\": [     5,     7   ] }...\n",
      "\u2705 JSON parse OK: answer='Simone Bolelli...', citations=[5, 7]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [5, 7] (Gold: [5, 7])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 18: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: The NG postcode area is situated approximately how many miles from Gamston, Rushcliffe?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] RM postcode area: The RM postcode area, also known as the Romford postcode area, is a group of 20 postcode districts i...\n",
      "   [2] FY postcode area: The FY postcode area, also known as the Blackpool postcode area, is a group of postcode districts ar...\n",
      "   [3] E postcode area: The E (Eastern) postcode area, also known as the London E postcode area, is the part of the London p...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 5933\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 166\n",
      "   {\n",
      "  \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",\n",
      "  \"answer\": \"insufficient context\",\n",
      "  \"citations\": []\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "\u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [] (Gold: [])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 19: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: What is the legal name of Johnny Campbell's hometown?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [3], [7].\",\n",
      "  \"answer\": \"Londonderry\",\n",
      "  \"citations\": [\n",
      "    3,\n",
      "    7\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Johnny Cooke: Johnny Cooke ( (1934--) 17 1934 (age\u00a082 ) ) born in Bootle is an English amateur lightweight and pro...\n",
      "   [2] J. R. Campbell (communist): John Ross Campbell MM (15 October 1894 \u2013 18 September 1969), best known as J. R. Campbell and also a...\n",
      "   [3] Derry: Derry ( ), officially Londonderry ( ), is the second-largest city in Northern Ireland and the fourth...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 6272\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 526\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [3] shows that While the city is more usually known colloquially as Derry, Londonderry is also commonly used and remains the legal name., and evidence [8] indicates that A person's first legal name generally is the name of the person that was given for the purpose of registration of the birth and which then appears on a birth certificate (see \\\"birth name\\\").. Based on [3], [8], the answer is Londonderry.\",\n",
      "  \"answer\": \"Londonderry\",\n",
      "  \"citations\": [\n",
      "    3,\n",
      "    8\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [3] shows that While the city is more usually known colloquially as Derry, Londonderry is also commonly used and remains the legal name., and eviden...\n",
      "\u2705 JSON parse OK: answer='Londonderry...', citations=[3, 8]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Londonderry\",   \"citations\": [     3,     7   ] }...\n",
      "\u2705 JSON parse OK: answer='Londonderry...', citations=[3, 7]\n",
      "\n",
      "   Metrics - F1: 1.000 | EM: 1.000 | Citations: [3, 8] (Gold: [3, 7])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\ud83d\udcdd EXAMPLE 20: Fine-tuned Model Prediction\n",
      "====================================================================================================\n",
      "\u2753 Question: Which film was Oscar nominated, LaLee's Kin: The Legacy of Cotton or Gimme Shelter, the 1970 Rolling Stones documentary?\n",
      "\u2705 Gold Answer: {\n",
      "  \"reasoning\": \"Relevant evidence found in passages [1], [5].\",\n",
      "  \"answer\": \"LaLee's Kin: The Legacy of Cotton\",\n",
      "  \"citations\": [\n",
      "    1,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\n",
      "\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\n",
      "   [1] Gimme Shelter (1970 film): Gimme Shelter is a 1970 documentary film directed by Albert and David Maysles and Charlotte Zwerin c...\n",
      "   [2] Baird Bryant: Wenzell Baird Bryant (Columbus, Indiana, December 12, 1927 \u2013 Hemet, California, November 13, 2008) w...\n",
      "   [3] Merry Clayton: Merry Clayton (born December 25, 1948) is an American soul and gospel singer and an actress.  She pr...\n",
      "   ...and 5 more passages.\n",
      "\n",
      "\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\n",
      "============================================================\n",
      "Prompt length: 5255\n",
      "the maximum sequence length is:  10000\n",
      "the response type is:  <class 'str'>\n",
      "\ud83d\udc1b DEBUG: Parsed keys: dict_keys(['reasoning', 'answer', 'citations'])\n",
      "Generated answer length: 510\n",
      "   {\n",
      "  \"reasoning\": \"To answer this question, evidence [1] shows that Gimme Shelter is a 1970 documentary film directed by Albert and David Maysles and Charlotte Zwerin chronicling the last weeks of The Rolling Stones' 1969 US tour which culminated in the disastrous Altamont Free Concert., and evidence [5] indicates that It was nominated for Best Documentary Feature at the 74th Academy Awards.. Based on [1], [5], the answer is Gimme Shelter.\",\n",
      "  \"answer\": \"Gimme Shelter\",\n",
      "  \"citations\": [\n",
      "    1,\n",
      "    5\n",
      "  ]\n",
      "}\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"To answer this question, evidence [1] shows that Gimme Shelter is a 1970 documentary film directed by Albert and David Maysles and Charlotte Zwerin chronicling the last weeks of The ...\n",
      "\u2705 JSON parse OK: answer='Gimme Shelter...', citations=[1, 5]\n",
      "\ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"LaLee's Kin: The Legacy of Cotton\",   \"citations\": [     1,     5   ] }...\n",
      "\u2705 JSON parse OK: answer='LaLee's Kin: The Legacy of Cotton...', citations=[1, 5]\n",
      "\n",
      "   Metrics - F1: 0.000 | EM: 0.000 | Citations: [1, 5] (Gold: [1, 5])\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\ud83e\uddf9 CUDA memory cleared after demo.\n",
      "\n",
      "================================================================================\n",
      "\u2705 FINE-TUNED MODEL INFERENCE DEMO COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned Model Inference Demo for Debugging and Visualization\n",
    "print(\"\ud83e\uddea FINE-TUNED MODEL INFERENCE DEMO: Debugging and Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Ensure necessary variables and functions are defined\n",
    "if 'eval_dataset' not in globals() or eval_dataset is None:\n",
    "    raise RuntimeError(\"eval_dataset is not loaded. Please run the data loading cells first.\")\n",
    "if 'eval_model' not in globals() or eval_model is None:\n",
    "    raise RuntimeError(\"eval_model is not loaded. Please run the cell to load the fine-tuned model first.\")\n",
    "else:\n",
    "    eval_model.eval() # Ensure fine-tuned model is in eval mode\n",
    "    print(\"\u2705 Using loaded fine-tuned model ('eval_model') for demo.\")\n",
    "\n",
    "\n",
    "if 'evaluator' not in globals() or evaluator is None:\n",
    "    raise RuntimeError(\"HotpotQAEvaluator is not initialized. Please run the evaluation setup cell.\")\n",
    "if 'generate_answer' not in globals():\n",
    "    raise RuntimeError(\"generate_answer function is not defined. Please run the evaluation setup cell.\")\n",
    "if 'extract_answer_and_citations' not in globals():\n",
    "    raise RuntimeError(\"extract_answer_and_citations function is not defined. Please run the evaluation setup cell.\")\n",
    "if 'building_prompts_rag' not in globals():\n",
    "     print(\"\u26a0\ufe0f building_prompts_rag not found. Using default RAG instruction (might affect quality).\")\n",
    "     building_prompts_rag = {'instruction': \"Answer the question using the provided evidence.\", 'cot_exemplar': \"\"}\n",
    "# MODEL_NAME, bnb_config, CACHE_DIR are not needed in this cell anymore as we are not loading the base model here.\n",
    "\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Evaluation dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# --- Reduce number of examples and max_new_tokens for faster debugging ---\n",
    "num_examples = min(20, len(eval_dataset)) # Reduced to 2 examples\n",
    "temp_max_new_tokens = 300 # Reduced max new tokens\n",
    "print(f\"\ud83d\udcdd Testing on {num_examples} examples with max_new_tokens={temp_max_new_tokens}...\")\n",
    "# --- End Reduction ---\n",
    "\n",
    "\n",
    "# Select a few examples from the evaluation dataset for the demo\n",
    "demo_examples = eval_dataset.shuffle(seed=42).select(range(num_examples))\n",
    "\n",
    "\n",
    "for i, example in enumerate(demo_examples):\n",
    "    print(f\"\\n\" + \"=\"*100)\n",
    "    print(f\"\ud83d\udcdd EXAMPLE {i+1}: Fine-tuned Model Prediction\")\n",
    "    print(f\"=\"*100)\n",
    "    question = example['question']\n",
    "    gold_answer_text = example['answer']\n",
    "    passages = example['passages']\n",
    "\n",
    "    print(f\"\u2753 Question: {question}\")\n",
    "    print(f\"\u2705 Gold Answer: {gold_answer_text}\")\n",
    "\n",
    "    print(f\"\\n\ud83d\udcda Available Evidence Passages (first 3 titles & snippets):\")\n",
    "    for j, passage in enumerate(passages[:3], 1):\n",
    "        print(f\"   [{j}] {passage.get('title', 'N/A')}: {passage.get('text', '')[:100]}...\")\n",
    "    if len(passages) > 3:\n",
    "         print(f\"   ...and {len(passages)-3} more passages.\")\n",
    "\n",
    "\n",
    "    print(f\"\\n\ud83e\udd16 FINE-TUNED MODEL PREDICTION:\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        # Generate prediction using the fine-tuned eval_model\n",
    "        # Ensure generate_answer uses building_prompts_rag for the fine-tuned model\n",
    "        # Use the reduced max_new_tokens for this demo\n",
    "        finetuned_prediction = generate_answer(question, passages, building_prompts_rag, eval_model, max_new_tokens=temp_max_new_tokens)\n",
    "        print(f\"   {finetuned_prediction}\")\n",
    "\n",
    "        # Extract answers and citations\n",
    "        finetuned_answer, finetuned_citations = extract_answer_and_citations(finetuned_prediction)\n",
    "        gold_answer, gold_citations = extract_answer_and_citations(gold_answer_text)\n",
    "\n",
    "        # Calculate metrics\n",
    "        finetuned_f1 = evaluator.answer_f1_score(finetuned_answer, gold_answer)\n",
    "        finetuned_em = evaluator.answer_exact_match(finetuned_answer, gold_answer)\n",
    "        finetuned_citation_acc = evaluator.answer_f1_score(str(finetuned_citations), str(gold_citations)) # Simple citation F1 on string repr\n",
    "\n",
    "        print(f\"\\n   Metrics - F1: {finetuned_f1:.3f} | EM: {finetuned_em:.3f} | Citations: {finetuned_citations} (Gold: {gold_citations})\")\n",
    "    except Exception as e:\n",
    "         print(f\"   \u274c Error generating fine-tuned prediction: {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "         finetuned_prediction = \"Error generating prediction.\"\n",
    "         finetuned_f1, finetuned_em, finetuned_citation_acc = 0.0, 0.0, 0.0\n",
    "         print(f\"\\n   Metrics - F1: {finetuned_f1:.3f} | EM: {finetuned_em:.3f} | Citations: N/A (Gold: {gold_citations if 'gold_citations' in locals() else 'N/A'})\")\n",
    "\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "\n",
    "# No cleanup needed for eval_model here, as it's loaded in a separate cell.\n",
    "# Cleanup CUDA memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n\ud83e\uddf9 CUDA memory cleared after demo.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"\u2705 FINE-TUNED MODEL INFERENCE DEMO COMPLETE!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f-46UNCZu1Lj",
   "metadata": {
    "id": "f-46UNCZu1Lj"
   },
   "source": [
    "As we could see that the finetuning result of LLM make it stupid and replies mechanically. The debugging implies problem is with the Lora Adapter. The issue usualy has to do with data& format, such as validation data preparation is problematic or loss computation is undesired from ground-truth or prompt template formatting has issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bzikliunr9u",
   "metadata": {
    "id": "bzikliunr9u"
   },
   "source": [
    "### Fine-tuned Model Full Evaluation\n",
    "\n",
    "Comprehensive evaluation of fine-tuned model on full evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf8ded",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7628337,
     "status": "ok",
     "timestamp": 1759968811954,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "e8cf8ded",
    "outputId": "2fed7462-05cb-4f45-8822-7e9057e7a1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting fine-tuned model evaluation...\n",
      "   Model: Mistral-7B-Instruct (QLoRA fine-tuned)\n",
      "   Strategy: Direct JSON output (instruction-tuned)\n",
      "   Dataset: Full eval_dataset (200 examples)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d Evaluating Fine-tuned QLoRA on 200 examples...\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 1 ---\n",
      "Question: What nationality was Oliver Reed's character in the film Royal Flash?...\n",
      "[Ex 0] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 0] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 0] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 0] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "Predicted: insufficient context\n",
      "Gold: insufficient context\n",
      "Pred Citations: []\n",
      "Gold Citations: []\n",
      "F1: 1.000, EM: 1.000, Citation F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   0%|          | 1/200 [00:11<38:31, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 2 ---\n",
      "Question: Pacific Mozart Ensemble performed which German composer's Der Lindberghflug in 2002?...\n",
      "[Ex 1] \ud83d\udd0d Extracting from: in 1874 in Munich.  The opera is based on the eponymous play by Shakespeare, which itself is based on a much older English folk ballad.  It is a well-known fact that the work is based on the English p...\n",
      "[Ex 1] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 1] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 1] \ud83d\udcdd Regex result: answer='in 1874 in Munich.  The opera is based on the epon...', citations=[]\n",
      "[Ex 1] \ud83d\udd0d Extracting from: {   \"reasoning\": \"From evidence [7]: Peter Wallace Hobbs formed the electrical appliance company Russell Hobbs with Bill Russell From evidence [8]: Russell Hobbs is a manufacturer of household applian...\n",
      "[Ex 1] \u2705 JSON parse OK: answer='Kurt Julian Weill...', citations=[5, 8]\n",
      "Predicted: in 1874 in Munich.  The opera is based on the eponymous play by Shakespeare, which itself is based o...\n",
      "Gold: Kurt Julian Weill\n",
      "Pred Citations: []\n",
      "Gold Citations: [5, 8]\n",
      "F1: 0.000, EM: 0.000, Citation F1: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   1%|          | 2/200 [01:26<2:42:03, 49.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 3 ---\n",
      "Question: Who released the song \"With or Without You\" first, Jai McDowall or U2?...\n",
      "[Ex 2] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Believe is the debut studio album by Scottish singer and \\\"Britain's Got Talent\\\" winner Jai McDowall., and evidence [6] indicates t...\n",
      "[Ex 2] \u2705 JSON parse OK: answer='U2...', citations=[5, 6]\n",
      "[Ex 2] \ud83d\udd0d Extracting from: {   \"reasoning\": \"From evidence [22]: Austrolebias bellottii is a species of fish that lives in the basins of the Paran\\u00e1 River and Uruguay River From evidence [24]: The Uruguay River flows from n...\n",
      "[Ex 2] \u2705 JSON parse OK: answer='U2...', citations=[5, 6]\n",
      "Predicted: U2\n",
      "Gold: U2\n",
      "Pred Citations: [5, 6]\n",
      "Gold Citations: [5, 6]\n",
      "F1: 1.000, EM: 1.000, Citation F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   2%|\u258f         | 3/200 [01:56<2:11:35, 40.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 4 ---\n",
      "Question: What Kentucky county has a population of 60,316 and features the Lake Louisvilla neighborhood?...\n",
      "[Ex 3] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that As of the 2010 census, the population was 60,316., and evidence [5] indicates that It is located between Westport Road in Louisville...\n",
      "[Ex 3] \u2705 JSON parse OK: answer='Oldham County...', citations=[1, 5]\n",
      "[Ex 3] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Oldham County\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 3] \u2705 JSON parse OK: answer='Oldham County...', citations=[1, 5]\n",
      "Predicted: Oldham County\n",
      "Gold: Oldham County\n",
      "Pred Citations: [1, 5]\n",
      "Gold Citations: [1, 5]\n",
      "F1: 1.000, EM: 1.000, Citation F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   2%|\u258f         | 4/200 [02:25<1:56:51, 35.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- Example 5 ---\n",
      "Question: Para Hills West, South Australia lies within a city with what estimated population?...\n",
      "[Ex 4] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Para Hills West is a suburb of Adelaide, South Australia, and is within the City of Salisbury., and evidence [2] indicates that It h...\n",
      "[Ex 4] \u2705 JSON parse OK: answer='138,535...', citations=[1, 2]\n",
      "[Ex 4] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"138,535\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 4] \u2705 JSON parse OK: answer='138,535...', citations=[1, 2]\n",
      "Predicted: 138,535\n",
      "Gold: 138,535\n",
      "Pred Citations: [1, 2]\n",
      "Gold Citations: [1, 2]\n",
      "F1: 1.000, EM: 1.000, Citation F1: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   2%|\u258e         | 5/200 [03:01<1:56:22, 35.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 5] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that A Christmas Carol in Prose, Being a Ghost-Story of Christmas, commonly known as A Christmas Carol, is a novella by Charles Dickens, ...\n",
      "[Ex 5] \u2705 JSON parse OK: answer='1863...', citations=[5, 8]\n",
      "[Ex 5] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 5] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   3%|\u258e         | 6/200 [03:46<2:05:54, 38.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 6] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 6] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 6] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 6] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   4%|\u258e         | 7/200 [03:58<1:36:54, 30.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 7] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that From there on began her professional partnership with Kapil Sharma that is still going on., and evidence [8] indicates that Kapil Sh...\n",
      "[Ex 7] \u2705 JSON parse OK: answer='Lucky Di Unlucky Story...', citations=[2, 8]\n",
      "[Ex 7] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 7] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   4%|\u258d         | 8/200 [04:34<1:42:27, 32.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 8] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 8] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 8] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 8] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   4%|\u258d         | 9/200 [04:46<1:21:55, 25.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 9] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 9] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 9] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Dudman\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 9] \u2705 JSON parse OK: answer='Dudman...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   5%|\u258c         | 10/200 [04:58<1:08:05, 21.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 10] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 10] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 10] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 10] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   6%|\u258c         | 11/200 [05:10<58:25, 18.55s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 11] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [6] shows that In the European Theater of Operations the 10th Armored Division was part of both the Twelfth United States Army Group and Sixth Unit...\n",
      "[Ex 11] \u2705 JSON parse OK: answer='Joint Chiefs of Staff...', citations=[6, 8]\n",
      "[Ex 11] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 11] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   6%|\u258c         | 12/200 [05:44<1:12:47, 23.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 12] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 12] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 12] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 12] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 12] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 12] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[5, 6]\n",
      "[Ex 12] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 12] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   6%|\u258b         | 13/200 [07:00<2:02:17, 39.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 13] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Since I cannot determine a defin...\n",
      "[Ex 13] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 13] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 13] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 13] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 13] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   7%|\u258b         | 14/200 [07:18<1:42:16, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 14] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation:  To answer this que...\n",
      "[Ex 14] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 14] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 14] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 14] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 14] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[5, 8]\n",
      "[Ex 14] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 14] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   8%|\u258a         | 15/200 [08:23<2:11:13, 42.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 15] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Tenerife South Airport (Spanish: \\\"Aeropuerto de Tenerife Sur\\\" ) (IATA: TFS, ICAO: GCTS) , previously known as Tenerife South\\u2013...\n",
      "[Ex 15] \u2705 JSON parse OK: answer='Tenerife South Airport...', citations=[5, 8]\n",
      "[Ex 15] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 15] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   8%|\u258a         | 16/200 [09:19<2:22:35, 46.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 16] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 16] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 16] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"no\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 16] \u2705 JSON parse OK: answer='no...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   8%|\u258a         | 17/200 [09:31<1:50:10, 36.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 17] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that Freeport (officially The Incorporated Village of Freeport) is a village in the town of Hempstead, Nassau County, New York, USA, on t...\n",
      "[Ex 17] \u2705 JSON parse OK: answer='Nassau County...', citations=[3, 4]\n",
      "[Ex 17] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"Nassau County\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 17] \u2705 JSON parse OK: answer='Nassau County...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:   9%|\u2589         | 18/200 [10:10<1:52:33, 37.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 18] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that Defending titlist is Ole Einar Bj\\u00f8rndalen of Norway., and evidence [6] indicates that Ole Einar Bj\\u00f8rndalen (born 27 Januar...\n",
      "[Ex 18] \u2705 JSON parse OK: answer='27 January 1974...', citations=[3, 6]\n",
      "[Ex 18] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"27 January 1974\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 18] \u2705 JSON parse OK: answer='27 January 1974...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  10%|\u2589         | 19/200 [10:51<1:55:05, 38.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 19] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Gasherbrum II (Urdu: \\u200e ); surveyed as K4, is the 13th highest mountain in the world at 8035 m above sea level., and evidence [4...\n",
      "[Ex 19] \u2705 JSON parse OK: answer='Gasherbrum II...', citations=[2, 4]\n",
      "[Ex 19] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Gasherbrum II\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 19] \u2705 JSON parse OK: answer='Gasherbrum II...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  10%|\u2588         | 20/200 [11:28<1:54:06, 38.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 20] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  In order to determine a definiti...\n",
      "[Ex 20] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 20] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 20] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 20] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 20] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[5, 7]\n",
      "[Ex 20] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"John de Mol Jr.\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 20] \u2705 JSON parse OK: answer='John de Mol Jr....', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  10%|\u2588         | 21/200 [12:26<2:11:18, 44.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 21] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that The car depicted on the cover is a \\\"Sassy Grass Green\\\" Plymouth Barracuda with the car's iconic hockey-stick decal saying \\\"Earth....\n",
      "[Ex 21] \u2705 JSON parse OK: answer='July 2014...', citations=[3, 6]\n",
      "[Ex 21] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 21] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  11%|\u2588         | 22/200 [13:22<2:21:00, 47.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 22] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  This is because the evidence doe...\n",
      "[Ex 22] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 22] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 22] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 22] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Rachel Anne Maddow\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 22] \u2705 JSON parse OK: answer='Rachel Anne Maddow...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  12%|\u2588\u258f        | 23/200 [14:27<2:35:44, 52.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 23] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 23] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 23] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 23] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  12%|\u2588\u258f        | 24/200 [14:39<1:58:42, 40.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 24] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that My Neighbor Totoro (Japanese: \\u3046\\u305a\\u304d\\u305a\\u304d , Hepburn: Tonari no Totoro ) is a 1988 Japanese animated fantasy film ...\n",
      "[Ex 24] \u2705 JSON parse OK: answer='1985...', citations=[3, 7]\n",
      "[Ex 24] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 24] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  12%|\u2588\u258e        | 25/200 [15:23<2:01:20, 41.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 25] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that The Reading Post (until 2009, the Reading Evening Post), was an English local newspaper covering Reading, Berkshire and surrounding ...\n",
      "[Ex 25] \u2705 JSON parse OK: answer='until 2009...', citations=[2, 3]\n",
      "[Ex 25] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [3].\",   \"answer\": \"2009\",   \"citations\": [     2,     3   ] }...\n",
      "[Ex 25] \u2705 JSON parse OK: answer='2009...', citations=[2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  13%|\u2588\u258e        | 26/200 [16:00<1:56:53, 40.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 26] \ud83d\udd0d Extracting from: continued their relationship with Jack Lenor Larsen.  In 1970, Olga de Amaral started a textile studio in Bogot\u00e1.  Olga de Amaral is a Colombian textile artist who has become an icon of the Colombian ...\n",
      "[Ex 26] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 26] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 26] \ud83d\udcdd Regex result: answer='continued their relationship with Jack Lenor Larse...', citations=[]\n",
      "[Ex 26] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"1970s and 1980s\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 26] \u2705 JSON parse OK: answer='1970s and 1980s...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  14%|\u2588\u258e        | 27/200 [17:17<2:27:24, 51.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 27] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Clement Greenberg ( ), occasionally writing under the pseudonym K., and evidence [7] indicates that Dame Agatha Mary Clarissa Christ...\n",
      "[Ex 27] \u2705 JSON parse OK: answer='Dame Agatha Mary Clarissa Christie...', citations=[5, 7]\n",
      "[Ex 27] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"Clement Greenberg\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 27] \u2705 JSON parse OK: answer='Clement Greenberg...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  14%|\u2588\u258d        | 28/200 [18:00<2:19:33, 48.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 28] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that He was in operational command during two of the most significant air battles in the European theatre in the Second World War, helpin...\n",
      "[Ex 28] \u2705 JSON parse OK: answer='Battle of Britain and Battle of Malta...', citations=[3, 7]\n",
      "[Ex 28] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Battle of Britain and the Battle of Malta\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 28] \u2705 JSON parse OK: answer='Battle of Britain and the Battle of Malta...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  14%|\u2588\u258d        | 29/200 [18:48<2:18:27, 48.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 29] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 29] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 29] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 29] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  15%|\u2588\u258c        | 30/200 [19:00<1:46:25, 37.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 30] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that Moller \\u2013 Maersk Group, which was founded by his father., and evidence [6] indicates that Arnold Peter M\\u00f8ller, commonly kno...\n",
      "[Ex 30] \u2705 JSON parse OK: answer='Arnold Peter M\u00f8ller...', citations=[3, 6]\n",
      "[Ex 30] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [3].\",   \"answer\": \"A.P. M\\u00f8ller\",   \"citations\": [     2,     3   ] }...\n",
      "[Ex 30] \u2705 JSON parse OK: answer='A.P. M\u00f8ller...', citations=[2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  16%|\u2588\u258c        | 31/200 [19:32<1:41:30, 36.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 31] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that In 1985, he became the second French citizen in space, after Jean-Loup Chr\\u00e9tien, when he flew aboard NASA's Space Shuttle missi...\n",
      "[Ex 31] \u2705 JSON parse OK: answer='Samantha Cristoforetti...', citations=[5, 6]\n",
      "[Ex 31] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 31] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  16%|\u2588\u258c        | 32/200 [20:14<1:45:37, 37.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 32] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that Kim Ki-bum (born August 21, 1987) is a South Korean actor and singer., and evidence [6] indicates that It starred Super Junior's Kim...\n",
      "[Ex 32] \u2705 JSON parse OK: answer='Park Ye-jin...', citations=[3, 6]\n",
      "[Ex 32] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"Park Ye-jin\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 32] \u2705 JSON parse OK: answer='Park Ye-jin...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  16%|\u2588\u258b        | 33/200 [20:45<1:39:05, 35.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 33] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that The Lingnan Fine Arts Museum () of the Academia Sinica is a museum in Nangang District, Taipei, Taiwan., and evidence [8] indicates ...\n",
      "[Ex 33] \u2705 JSON parse OK: answer='Keelung...', citations=[4, 8]\n",
      "[Ex 33] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"Keelung\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 33] \u2705 JSON parse OK: answer='Keelung...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  17%|\u2588\u258b        | 34/200 [21:15<1:34:06, 34.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 34] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Olathe East High School is a public high school located in Olathe, Kansas, United States, serving students in grades 9-12., and evid...\n",
      "[Ex 34] \u2705 JSON parse OK: answer='Olathe, Kansas...', citations=[2, 4]\n",
      "[Ex 34] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"located in Olathe, Kansas\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 34] \u2705 JSON parse OK: answer='located in Olathe, Kansas...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  18%|\u2588\u258a        | 35/200 [21:53<1:36:53, 35.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 35] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  If you have more information, pl...\n",
      "[Ex 35] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 35] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 35] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 35] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 35] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  18%|\u2588\u258a        | 36/200 [22:32<1:39:16, 36.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 36] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that The composer of \\\"Aloha \\u0244Oe\\\" and numerous other works, she authored her biography during her imprisonment following the overth...\n",
      "[Ex 36] \u2705 JSON parse OK: answer='Aloha \u0244Oe...', citations=[1, 5]\n",
      "[Ex 36] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"Aloha \\u02bbOe\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 36] \u2705 JSON parse OK: answer='Aloha \u02bbOe...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  18%|\u2588\u258a        | 37/200 [23:10<1:40:26, 36.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 37] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 37] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 37] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 37] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  19%|\u2588\u2589        | 38/200 [23:22<1:19:30, 29.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 38] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 38] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 38] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 38] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  20%|\u2588\u2589        | 39/200 [23:34<1:04:58, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 39] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that At the Dissolution of the Netherlands Antilles on 10 October 2010 the prison changed its name once more and became the Sentro di Det...\n",
      "[Ex 39] \u2705 JSON parse OK: answer='10 October 2010...', citations=[1, 2]\n",
      "[Ex 39] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"10 October 2010\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 39] \u2705 JSON parse OK: answer='10 October 2010...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  20%|\u2588\u2588        | 40/200 [24:13<1:16:00, 28.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 40] \ud83d\udd0d Extracting from: played by the Orlando Magic in the National Basketball Association (NBA).  The franchise has played in the NBA playoffs for exactly half of its existence (14 playoff appearances in 28 years), and twic...\n",
      "[Ex 40] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 40] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 40] \ud83d\udcdd Regex result: answer='played by the Orlando Magic in the National Basket...', citations=[]\n",
      "[Ex 40] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"Eastern Conference champion Orlando Magic against the Western Conference champion Houston Rockets.\",   \"citations\": [    ...\n",
      "[Ex 40] \u2705 JSON parse OK: answer='Eastern Conference champion Orlando Magic against ...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  20%|\u2588\u2588        | 41/200 [25:29<1:53:11, 42.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 41] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 41] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 41] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 41] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 41] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 41] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[2, 8]\n",
      "[Ex 41] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 41] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  21%|\u2588\u2588        | 42/200 [26:45<2:18:52, 52.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 42] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [7] shows that Chester Charles Bennington (March 20, 1976 \\u2013 July 20, 2017) was an American singer and songwriter., and evidence [8] indicates ...\n",
      "[Ex 42] \u2705 JSON parse OK: answer='Mikael Stanne...', citations=[7, 8]\n",
      "[Ex 42] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 42] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  22%|\u2588\u2588\u258f       | 43/200 [27:19<2:03:19, 47.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 43] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 43] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 43] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 43] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  22%|\u2588\u2588\u258f       | 44/200 [27:31<1:35:02, 36.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 44] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [6] shows that Urbain's Horseman is a Canadian television drama miniseries, broadcast on CBC Television in the 2007\\u20132008 television season., a...\n",
      "[Ex 44] \u2705 JSON parse OK: answer='Evey...', citations=[6, 8]\n",
      "[Ex 44] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 44] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  22%|\u2588\u2588\u258e       | 45/200 [28:06<1:33:11, 36.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 45] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that \\\"Frontline\\\" gives a prominent place to various issues of development and hindrances in the Indian states., and evidence [2] indica...\n",
      "[Ex 45] \u2705 JSON parse OK: answer='The Hindu...', citations=[1, 2]\n",
      "[Ex 45] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"The Hindu Group\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 45] \u2705 JSON parse OK: answer='The Hindu Group...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  23%|\u2588\u2588\u258e       | 46/200 [28:32<1:25:07, 33.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 46] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 46] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 46] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 46] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 46] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 46] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[4, 5]\n",
      "[Ex 46] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 46] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  24%|\u2588\u2588\u258e       | 47/200 [29:48<1:57:31, 46.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 47] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Helen Dunmore FRSL (12 December 1952 \\u2013 5 June 2017) was a British poet, novelist and children's writer., and evidence [4] indic...\n",
      "[Ex 47] \u2705 JSON parse OK: answer='no...', citations=[1, 4]\n",
      "[Ex 47] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [4].\",   \"answer\": \"no\",   \"citations\": [     1,     4   ] }...\n",
      "[Ex 47] \u2705 JSON parse OK: answer='no...', citations=[1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  24%|\u2588\u2588\u258d       | 48/200 [30:21<1:46:22, 41.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 48] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that In 1998, selected members of Fleetwood Mac were inducted into the Rock and Roll Hall of Fame, and received the Brit Award for Outsta...\n",
      "[Ex 48] \u2705 JSON parse OK: answer='Fleetwood Mac...', citations=[1, 2]\n",
      "[Ex 48] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"Fleetwood Mac\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 48] \u2705 JSON parse OK: answer='Fleetwood Mac...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  24%|\u2588\u2588\u258d       | 49/200 [30:59<1:42:38, 40.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 49] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 49] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 49] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 49] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  25%|\u2588\u2588\u258c       | 50/200 [31:10<1:20:09, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 50] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Gimme Shelter is a 1970 documentary film directed by Albert and David Maysles and Charlotte Zwerin chronicling the last weeks of The...\n",
      "[Ex 50] \u2705 JSON parse OK: answer='Gimme Shelter...', citations=[1, 5]\n",
      "[Ex 50] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"LaLee's Kin: The Legacy of Cotton\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 50] \u2705 JSON parse OK: answer='LaLee's Kin: The Legacy of Cotton...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  26%|\u2588\u2588\u258c       | 51/200 [31:49<1:24:30, 34.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 51] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that The islands concerned are sometimes referred to as the Kingdom of Mann and the Isles, although only some of the later rulers claimed...\n",
      "[Ex 51] \u2705 JSON parse OK: answer='Kingdom of Mann and the Isles...', citations=[1, 7]\n",
      "[Ex 51] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 51] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  26%|\u2588\u2588\u258c       | 52/200 [32:27<1:26:54, 35.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 52] \ud83d\udd0d Extracting from: events and one by calculating the age of Jesus at his death.  The two accounts of the nativity agree that Jesus was born in Bethlehem in the time of Herod the Great, but they differ in many details. [...\n",
      "[Ex 52] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 52] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 52] \ud83d\udcdd Regex result: answer='events and one by calculating the age of Jesus at ...', citations=[]\n",
      "[Ex 52] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Part I\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 52] \u2705 JSON parse OK: answer='Part I...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  26%|\u2588\u2588\u258b       | 53/200 [33:41<1:54:59, 46.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 53] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  From evidence [1]: Sisse Graum J...\n",
      "[Ex 53] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 53] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 53] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 53] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 53] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 8]\n",
      "[Ex 53] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 53] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  27%|\u2588\u2588\u258b       | 54/200 [34:56<2:14:04, 55.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 54] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Katie Sagona (born November 26, 1989) is an American child actress., and evidence [5] indicates that The term child actor or child a...\n",
      "[Ex 54] \u2705 JSON parse OK: answer='child actress...', citations=[2, 5]\n",
      "[Ex 54] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"child actor\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 54] \u2705 JSON parse OK: answer='child actor...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  28%|\u2588\u2588\u258a       | 55/200 [35:33<2:00:04, 49.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 55] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  The question asks for the name o...\n",
      "[Ex 55] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 55] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 55] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 55] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 55] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  28%|\u2588\u2588\u258a       | 56/200 [36:47<2:16:58, 57.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 56] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that Ralph Franklin Hefferline (15 February 1910 in Muncie, Indiana \\u2013 16 March 1974) was a psychology professor at Columbia Universi...\n",
      "[Ex 56] \u2705 JSON parse OK: answer='New York City...', citations=[4, 7]\n",
      "[Ex 56] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"New York City\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 56] \u2705 JSON parse OK: answer='New York City...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  28%|\u2588\u2588\u258a       | 57/200 [37:29<2:05:30, 52.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 57] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that \\\"Bo Diddley\\\" is a rhythm and blues and rock and roll song first recorded and sung by Bo Diddley at the Universal Recording Studio ...\n",
      "[Ex 57] \u2705 JSON parse OK: answer='rock and roll...', citations=[2, 6]\n",
      "[Ex 57] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 57] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  29%|\u2588\u2588\u2589       | 58/200 [38:34<2:13:21, 56.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 58] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 58] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 58] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 58] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 58] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 58] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[4, 6]\n",
      "[Ex 58] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"China\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 58] \u2705 JSON parse OK: answer='China...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  30%|\u2588\u2588\u2589       | 59/200 [39:49<2:25:11, 61.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 59] \ud83d\udd0d Extracting from: . [8] Title: George Balanchine - George Balanchine (1904\u20131983) was a Russian-born American choreographer and ballet dancer.  Balanchine is known for his collaborations with Igor Stravinsky, Aaron Copl...\n",
      "[Ex 59] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 59] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 59] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 59] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"The Los Angeles Dance Theater\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 59] \u2705 JSON parse OK: answer='The Los Angeles Dance Theater...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  30%|\u2588\u2588\u2588       | 60/200 [41:03<2:33:14, 65.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 60] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that Josey Scott (born Joseph Scott Sappington; May 3, 1972) is the former lead vocalist of the rock band Saliva., and evidence [5] indic...\n",
      "[Ex 60] \u2705 JSON parse OK: answer='Ian David Karslake Watkins...', citations=[3, 5]\n",
      "[Ex 60] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [5].\",   \"answer\": \"Lostprophets disbanded in 2013 after Watkins was charged with sexual offences in late 2012.\",   \"citations\": [     3,    ...\n",
      "[Ex 60] \u2705 JSON parse OK: answer='Lostprophets disbanded in 2013 after Watkins was c...', citations=[3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  30%|\u2588\u2588\u2588       | 61/200 [41:39<2:11:32, 56.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 61] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that The plaster casts Freeman subsequently made were convincing enough to be considered critical pieces of evidence by anthropologists J...\n",
      "[Ex 61] \u2705 JSON parse OK: answer='Cynthia Plaster Caster...', citations=[1, 6]\n",
      "[Ex 61] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 61] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  31%|\u2588\u2588\u2588       | 62/200 [42:29<2:05:21, 54.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 62] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 62] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 62] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 62] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  32%|\u2588\u2588\u2588\u258f      | 63/200 [42:40<1:35:02, 41.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 63] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Note: The reasoning here is that...\n",
      "[Ex 63] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 63] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 63] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 63] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"Wheeling, West Virginia\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 63] \u2705 JSON parse OK: answer='Wheeling, West Virginia...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  32%|\u2588\u2588\u2588\u258f      | 64/200 [42:58<1:18:20, 34.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 64] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 64] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 64] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Godiva\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 64] \u2705 JSON parse OK: answer='Godiva...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  32%|\u2588\u2588\u2588\u258e      | 65/200 [43:10<1:02:17, 27.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 65] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Special Correspondents is a 2016 British-Canadian-American satirical comedy film written, directed by and starring Ricky Gervais., a...\n",
      "[Ex 65] \u2705 JSON parse OK: answer='Eric Banadinovi\u00e7...', citations=[5, 8]\n",
      "[Ex 65] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Eric Banadinovi\\u0107\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 65] \u2705 JSON parse OK: answer='Eric Banadinovi\u0107...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  33%|\u2588\u2588\u2588\u258e      | 66/200 [43:50<1:09:50, 31.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 66] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Although [1] shows that After B\u00fc...\n",
      "[Ex 66] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 66] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 66] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 66] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 66] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 2]\n",
      "[Ex 66] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"Germany\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 66] \u2705 JSON parse OK: answer='Germany...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  34%|\u2588\u2588\u2588\u258e      | 67/200 [44:33<1:17:09, 34.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 67] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 67] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 67] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"1998\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 67] \u2705 JSON parse OK: answer='1998...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  34%|\u2588\u2588\u2588\u258d      | 68/200 [44:44<1:01:12, 27.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 68] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  The evidence does not provide en...\n",
      "[Ex 68] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 68] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 68] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 68] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 68] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[]\n",
      "[Ex 68] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 68] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  34%|\u2588\u2588\u2588\u258d      | 69/200 [45:58<1:30:51, 41.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 69] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 69] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 69] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 69] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  35%|\u2588\u2588\u2588\u258c      | 70/200 [46:10<1:10:42, 32.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 70] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that The Lincoln Memorial Railsplitters are the athletic teams that represent the Lincoln Memorial University, located in Harrogate, Tenn...\n",
      "[Ex 70] \u2705 JSON parse OK: answer='Lincoln Memorial University...', citations=[5, 8]\n",
      "[Ex 70] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Lincoln Memorial University\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 70] \u2705 JSON parse OK: answer='Lincoln Memorial University...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  36%|\u2588\u2588\u2588\u258c      | 71/200 [46:42<1:10:14, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 71] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 71] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 71] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 71] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  36%|\u2588\u2588\u2588\u258c      | 72/200 [46:54<56:08, 26.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 72] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 72] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 72] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 72] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  36%|\u2588\u2588\u2588\u258b      | 73/200 [47:05<46:15, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 73] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 73] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 73] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 73] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  37%|\u2588\u2588\u2588\u258b      | 74/200 [47:17<39:29, 18.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 74] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Franklin's experiments with the machine eventually led to new theories about electricity and inventing the lightning rod., and evide...\n",
      "[Ex 74] \u2705 JSON parse OK: answer='Benjamin Franklin...', citations=[1, 2]\n",
      "[Ex 74] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 74] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  38%|\u2588\u2588\u2588\u258a      | 75/200 [47:44<44:31, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 75] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 75] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 75] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 75] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  38%|\u2588\u2588\u2588\u258a      | 76/200 [47:56<38:08, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 76] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [6] shows that Fort Worth is the 16th-largest city in the United States and the fifth-largest city in the state of Texas., and evidence [8] indicat...\n",
      "[Ex 76] \u2705 JSON parse OK: answer='Fort Worth...', citations=[6, 8]\n",
      "[Ex 76] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"Fort Worth\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 76] \u2705 JSON parse OK: answer='Fort Worth...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  38%|\u2588\u2588\u2588\u258a      | 77/200 [48:27<45:28, 22.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 77] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  I cannot determine a definitive ...\n",
      "[Ex 77] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 77] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 77] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 77] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 77] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  39%|\u2588\u2588\u2588\u2589      | 78/200 [48:43<41:22, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 78] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that The 2016 Oklahoma Sooners football team represented the University of Oklahoma in the 2016 NCAA Division I FBS football season, the ...\n",
      "[Ex 78] \u2705 JSON parse OK: answer='2016 Oklahoma Sooners...', citations=[1, 4]\n",
      "[Ex 78] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [4].\",   \"answer\": \"2016 Oklahoma Sooners football team\",   \"citations\": [     1,     4   ] }...\n",
      "[Ex 78] \u2705 JSON parse OK: answer='2016 Oklahoma Sooners football team...', citations=[1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  40%|\u2588\u2588\u2588\u2589      | 79/200 [49:18<49:52, 24.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 79] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  This is because the evidence doe...\n",
      "[Ex 79] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 79] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 79] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 79] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 79] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  40%|\u2588\u2588\u2588\u2588      | 80/200 [49:34<44:08, 22.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 80] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that Take It Easy is an abstract strategy board game created by Peter Burley., and evidence [7] indicates that \\\"Kae-in's Taste\\\" or \\\"Ka...\n",
      "[Ex 80] \u2705 JSON parse OK: answer='Take It Easy...', citations=[4, 7]\n",
      "[Ex 80] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 80] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  40%|\u2588\u2588\u2588\u2588      | 81/200 [50:09<51:29, 25.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 81] \ud83d\udd0d Extracting from: sey - The Town of Ramsey is the northernmost town of the Falkland Islands, located in the Atlantic Ocean, approximately 320 miles east of the South American continent, and is one of the oldest towns i...\n",
      "[Ex 81] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 81] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 81] \ud83d\udcdd Regex result: answer='sey - The Town of Ramsey is the northernmost town ...', citations=[]\n",
      "[Ex 81] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 81] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  41%|\u2588\u2588\u2588\u2588      | 82/200 [51:23<1:19:26, 40.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 82] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 82] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 82] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 82] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  42%|\u2588\u2588\u2588\u2588\u258f     | 83/200 [51:35<1:01:55, 31.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 83] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  I cannot determine a definitive ...\n",
      "[Ex 83] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 83] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 83] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 83] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 83] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  42%|\u2588\u2588\u2588\u2588\u258f     | 84/200 [51:51<52:16, 27.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 84] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that It was awarded two Grammys: Best Country Instrumental Performance for O'Connor, and Best Country Collaboration with Vocals for Vince...\n",
      "[Ex 84] \u2705 JSON parse OK: answer='Carl Perkins...', citations=[2, 8]\n",
      "[Ex 84] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"Mark O'Connor\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 84] \u2705 JSON parse OK: answer='Mark O'Connor...', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  42%|\u2588\u2588\u2588\u2588\u258e     | 85/200 [52:31<59:15, 30.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 85] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  In order to answer this question...\n",
      "[Ex 85] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 85] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 85] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 85] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Philip Jos\\u00e9 Farmer\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 85] \u2705 JSON parse OK: answer='Philip Jos\u00e9 Farmer...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  43%|\u2588\u2588\u2588\u2588\u258e     | 86/200 [53:45<1:23:28, 43.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 86] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Red Dead Redemption II is an upcoming western action-adventure video game developed and published by Rockstar Games for release on P...\n",
      "[Ex 86] \u2705 JSON parse OK: answer='Rockstar Games...', citations=[1, 6]\n",
      "[Ex 86] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 86] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  44%|\u2588\u2588\u2588\u2588\u258e     | 87/200 [54:24<1:20:03, 42.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 87] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 87] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 87] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 87] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  44%|\u2588\u2588\u2588\u2588\u258d     | 88/200 [54:36<1:02:01, 33.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 88] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 88] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 88] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 88] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  44%|\u2588\u2588\u2588\u2588\u258d     | 89/200 [54:47<49:26, 26.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 89] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation:  To answer this que...\n",
      "[Ex 89] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 89] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 89] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 89] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 89] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 5]\n",
      "[Ex 89] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Bruce Conner\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 89] \u2705 JSON parse OK: answer='Bruce Conner...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  45%|\u2588\u2588\u2588\u2588\u258c     | 90/200 [56:02<1:15:13, 41.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 90] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Output with evidence: {   \"reaso...\n",
      "[Ex 90] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 90] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 90] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 90] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 90] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[4, 7]\n",
      "[Ex 90] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 90] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  46%|\u2588\u2588\u2588\u2588\u258c     | 91/200 [57:16<1:32:43, 51.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 91] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [7] shows that Jeffrey Adam \\\"Duff\\\" Goldman (born December 17, 1974) is a pastry chef and television personality., and evidence [8] indicates that...\n",
      "[Ex 91] \u2705 JSON parse OK: answer='Duff Goldman...', citations=[7, 8]\n",
      "[Ex 91] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"Jeffrey Adam \\\"Duff\\\" Goldman\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 91] \u2705 JSON parse OK: answer='Jeffrey Adam \"Duff\" Goldman...', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  46%|\u2588\u2588\u2588\u2588\u258c     | 92/200 [57:51<1:23:00, 46.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 92] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 92] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 92] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 92] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  46%|\u2588\u2588\u2588\u2588\u258b     | 93/200 [58:02<1:03:49, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 93] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Crosby's trademark warm bass-baritone voice made him the best-selling recording artist of the 20th century, having sold over one bil...\n",
      "[Ex 93] \u2705 JSON parse OK: answer='Harry Lillis \"Bing\" Crosby Jr....', citations=[1, 3]\n",
      "[Ex 93] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Harry Lillis \\\"Bing\\\" Crosby Jr.\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 93] \u2705 JSON parse OK: answer='Harry Lillis \"Bing\" Crosby Jr....', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  47%|\u2588\u2588\u2588\u2588\u258b     | 94/200 [58:50<1:09:33, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 94] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation: To answer this ques...\n",
      "[Ex 94] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 94] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 94] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 94] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [5].\",   \"answer\": \"Greyia\",   \"citations\": [     3,     5   ] }...\n",
      "[Ex 94] \u2705 JSON parse OK: answer='Greyia...', citations=[3, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  48%|\u2588\u2588\u2588\u2588\u258a     | 95/200 [1:00:04<1:27:12, 49.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 95] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Baker Hughes, a GE Company has its headquarters split between the legacy BHI headquarters in Houston, Texas and the legacy GE Oil & ...\n",
      "[Ex 95] \u2705 JSON parse OK: answer='Valley Gardens Middle School...', citations=[1, 4]\n",
      "[Ex 95] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [7].\",   \"answer\": \"Jaguar Land Rover\",   \"citations\": [     6,     7   ] }...\n",
      "[Ex 95] \u2705 JSON parse OK: answer='Jaguar Land Rover...', citations=[6, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  48%|\u2588\u2588\u2588\u2588\u258a     | 96/200 [1:00:39<1:18:34, 45.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 96] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 96] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 96] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Ken Howard\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 96] \u2705 JSON parse OK: answer='Ken Howard...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  48%|\u2588\u2588\u2588\u2588\u258a     | 97/200 [1:00:51<1:00:27, 35.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 97] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 97] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 97] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 97] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  49%|\u2588\u2588\u2588\u2588\u2589     | 98/200 [1:01:02<47:50, 28.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 98] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that A veteran World War I fighter pilot ace, he was a recipient of the \\\"Pour le M\\u00e9rite\\\"., and evidence [6] indicates that World W...\n",
      "[Ex 98] \u2705 JSON parse OK: answer='1918...', citations=[3, 6]\n",
      "[Ex 98] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"1918\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 98] \u2705 JSON parse OK: answer='1918...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  50%|\u2588\u2588\u2588\u2588\u2589     | 99/200 [1:01:44<54:05, 32.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 99] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 99] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 99] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 99] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  50%|\u2588\u2588\u2588\u2588\u2588     | 100/200 [1:01:55<43:18, 25.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 100] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  However, if you meant to ask abo...\n",
      "[Ex 100] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 100] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 100] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 100] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 100] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[7, 8]\n",
      "[Ex 100] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"City and County of Honolulu\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 100] \u2705 JSON parse OK: answer='City and County of Honolulu...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  50%|\u2588\u2588\u2588\u2588\u2588     | 101/200 [1:03:10<1:06:43, 40.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 101] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [7] shows that She has co-written twenty of Swift's officially-released songs and singles, including \\\"White Horse,\\\" \\\"Teardrops on My Guitar,\\\" a...\n",
      "[Ex 101] \u2705 JSON parse OK: answer='\"White Horse\"...', citations=[7, 8]\n",
      "[Ex 101] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"White Horse\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 101] \u2705 JSON parse OK: answer='White Horse...', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  51%|\u2588\u2588\u2588\u2588\u2588     | 102/200 [1:03:54<1:07:59, 41.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 102] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 102] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 102] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"Alistair Grant\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 102] \u2705 JSON parse OK: answer='Alistair Grant...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 103/200 [1:04:06<52:41, 32.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 103] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 103] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 103] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 103] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 103] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 103] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 6]\n",
      "[Ex 103] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [6].\",   \"answer\": \"United Arab Emirates\",   \"citations\": [     1,     6   ] }...\n",
      "[Ex 103] \u2705 JSON parse OK: answer='United Arab Emirates...', citations=[1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 104/200 [1:05:07<1:05:52, 41.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 104] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Mustafa Kemal Atat\\u0131rk (] ; 19 May 1881 \\u2013 10 November 1938) was a Turkish army officer, revolutionary, and founder of the R...\n",
      "[Ex 104] \u26a0\ufe0f  JSON parse failed: Invalid \\uXXXX escape: line 3 column 371 (char 373. Trying fallback...\n",
      "[Ex 104] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 104] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 104] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 104] \ud83d\udcdd Regex result: answer='historic house museum...', citations=[1, 8]\n",
      "[Ex 104] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"museum\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 104] \u2705 JSON parse OK: answer='museum...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  52%|\u2588\u2588\u2588\u2588\u2588\u258e    | 105/200 [1:06:01<1:11:14, 44.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 105] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that \\\"The Captain of Her Heart\\\" is a single by the Swiss duo Double in 1985., and evidence [6] indicates that Double (pronounced \\\"doo-...\n",
      "[Ex 105] \u2705 JSON parse OK: answer='1985...', citations=[2, 6]\n",
      "[Ex 105] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 105] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 106/200 [1:06:33<1:04:35, 41.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 106] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that He is also notable as the father of the musicians Jakob Hassler, Hans Leo Hassler and Kasper Hassler., and evidence [4] indicates th...\n",
      "[Ex 106] \u2705 JSON parse OK: answer='Boles\u00f8w...', citations=[3, 4]\n",
      "[Ex 106] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 106] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 107/200 [1:07:18<1:05:33, 42.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 107] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that The Man in the High Castle (1962) is an alternative history novel by American writer Philip K., and evidence [7] indicates that The ...\n",
      "[Ex 107] \u2705 JSON parse OK: answer='Philip K. Dick....', citations=[5, 7]\n",
      "[Ex 107] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"Philip K. Dick\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 107] \u2705 JSON parse OK: answer='Philip K. Dick...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 108/200 [1:07:48<59:14, 38.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 108] \ud83d\udd0d Extracting from: grand duchies have survived to this day as independent countries, such as Luxembourg and Liechtenstein. [7] Title: Peter Wallace Hobbs - Peter Wallace Hobbs formed the electrical appliance company Rus...\n",
      "[Ex 108] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 108] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 108] \ud83d\udcdd Regex result: answer='grand duchies have survived to this day as indepen...', citations=[]\n",
      "[Ex 108] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"Army of the Holy Roman\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 108] \u2705 JSON parse OK: answer='Army of the Holy Roman...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  55%|\u2588\u2588\u2588\u2588\u2588\u258d    | 109/200 [1:09:02<1:14:52, 49.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 109] \ud83d\udd0d Extracting from: grasses and other herbaceous vegetation.  National Grasslands are typically administered by the United States Forest Service, part of the United States Department of Agriculture.   Based on the availa...\n",
      "[Ex 109] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 109] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 109] \ud83d\udcdd Regex result: answer='grasses and other herbaceous vegetation.  National...', citations=[]\n",
      "[Ex 109] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"Croatan, Nantahala, and Uwharrie\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 109] \u2705 JSON parse OK: answer='Croatan, Nantahala, and Uwharrie...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 110/200 [1:09:17<58:18, 38.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 110] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 110] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 110] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 110] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 110] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 110] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 5]\n",
      "[Ex 110] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Washington Street\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 110] \u2705 JSON parse OK: answer='Washington Street...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 111/200 [1:10:31<1:13:25, 49.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 111] \ud83d\udd0d Extracting from: .C.. [9] Title: Bill Austin - Bill Austin (born August 24, 1932) is a former American football player.  He played college football at Alabama A&M and was a third round draft pick by the Detroit Lions ...\n",
      "[Ex 111] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 111] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 111] \ud83d\udcdd Regex result: answer='.C.....', citations=[]\n",
      "[Ex 111] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"Chuck Noll\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 111] \u2705 JSON parse OK: answer='Chuck Noll...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 112/200 [1:10:58<1:02:38, 42.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 112] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that He played in the National Basketball League for several teams, including the Whiting/Hammond Ciesar All-Americans, Chicago Bruins, a...\n",
      "[Ex 112] \u2705 JSON parse OK: answer='Vincent J. McGowan...', citations=[5, 6]\n",
      "[Ex 112] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 112] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  56%|\u2588\u2588\u2588\u2588\u2588\u258b    | 113/200 [1:11:37<1:00:13, 41.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 113] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 113] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 113] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 113] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 114/200 [1:11:48<46:40, 32.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 114] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation: To answer this ques...\n",
      "[Ex 114] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 114] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 114] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 114] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 114] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[6, 8]\n",
      "[Ex 114] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"Riviera\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 114] \u2705 JSON parse OK: answer='Riviera...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  57%|\u2588\u2588\u2588\u2588\u2588\u258a    | 115/200 [1:13:02<1:03:45, 45.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 115] \ud83d\udd0d Extracting from: . [8] Title: The album - The album \"Roses in the Snow\" was released in 1980 and was produced by Peter Hobbs.  The second single, a remake of a Simon & Garfunkel song, \"The Boxer\" reached #13.  Peter W...\n",
      "[Ex 115] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 115] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 115] \ud83d\udcdd Regex result: answer='....', citations=[]\n",
      "[Ex 115] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 115] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 116/200 [1:14:16<1:15:14, 53.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 116] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Alba Longa (occasionally written Albalonga in Italian sources) was an ancient city of Latium in central Italy, 12 mi southeast of Ro...\n",
      "[Ex 116] \u2705 JSON parse OK: answer='Alba Longa...', citations=[5, 8]\n",
      "[Ex 116] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"12 mi southeast of Rome\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 116] \u2705 JSON parse OK: answer='12 mi southeast of Rome...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 117/200 [1:14:53<1:07:16, 48.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 117] \ud83d\udd0d Extracting from: 1966). [8] Title: Milt Campbell - Milt Campbell (June 18, 1918 \u2013 December 14, 1984) was an American football player, coach, and college athletic administrator.  He was the head basketball coach at the...\n",
      "[Ex 117] \u26a0\ufe0f  JSON parse failed: Extra data: line 1 column 5 (char 4). Trying fallback...\n",
      "[Ex 117] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 117] \ud83d\udcdd Regex result: answer='1966)....', citations=[]\n",
      "[Ex 117] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 117] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  59%|\u2588\u2588\u2588\u2588\u2588\u2589    | 118/200 [1:16:07<1:16:59, 56.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 118] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that Chapter II is the second studio album by American R&B singer Ashanti, released by Murder Inc., and evidence [7] indicates that Antho...\n",
      "[Ex 118] \u2705 JSON parse OK: answer='January 28, 1971...', citations=[4, 7]\n",
      "[Ex 118] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 118] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 119/200 [1:16:56<1:13:03, 54.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 119] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Over the course of their nearly twenty-year career, the group toured heavily and released six studio albums, the majority on indepen...\n",
      "[Ex 119] \u2705 JSON parse OK: answer='Motion City Soundtrack...', citations=[2, 8]\n",
      "[Ex 119] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [8].\",   \"answer\": \"Motion City Soundtrack\",   \"citations\": [     2,     8   ] }...\n",
      "[Ex 119] \u2705 JSON parse OK: answer='Motion City Soundtrack...', citations=[2, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 120/200 [1:17:26<1:02:17, 46.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 120] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 120] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 120] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"He is from Pago Pago, American Samoa and played college football at Oregon.\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 120] \u2705 JSON parse OK: answer='He is from Pago Pago, American Samoa and played co...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 121/200 [1:17:37<47:35, 36.15s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 121] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that Peter Atencio (born March 15, 1983) is an American television and film director best known for directing the sketch comedy series \\\"...\n",
      "[Ex 121] \u2705 JSON parse OK: answer='Jordan Peele...', citations=[4, 5]\n",
      "[Ex 121] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 121] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 122/200 [1:18:14<47:11, 36.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 122] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 122] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 122] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 122] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 123/200 [1:18:26<37:04, 28.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 123] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  I cannot determine a definitive ...\n",
      "[Ex 123] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 123] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 123] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 123] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 123] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 124/200 [1:18:42<31:41, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 124] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that Them were a Northern Irish band formed in Belfast in April 1964, most prominently known for the garage rock standard \\\"Gloria\\\" and ...\n",
      "[Ex 124] \u2705 JSON parse OK: answer='Them...', citations=[4, 5]\n",
      "[Ex 124] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 124] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 125/200 [1:19:17<35:00, 28.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 125] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Since the evidence [1] shows tha...\n",
      "[Ex 125] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 125] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 125] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 125] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 125] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 2]\n",
      "[Ex 125] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"pornstar\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 125] \u2705 JSON parse OK: answer='pornstar...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 126/200 [1:20:01<40:38, 32.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 126] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that He played the role of James \\\"Sonny\\\" Crockett in the 1980s television series \\\"Miami Vice\\\" and had the eponymous lead role in the ...\n",
      "[Ex 126] \u2705 JSON parse OK: answer='Don Johnson...', citations=[1, 8]\n",
      "[Ex 126] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"Donald Wayne Johnson\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 126] \u2705 JSON parse OK: answer='Donald Wayne Johnson...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 127/200 [1:20:38<41:36, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 127] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that The population was 1,650 at the 2010 census., and evidence [6] indicates that A favorite with artists, writers and naturalists, St.....\n",
      "[Ex 127] \u2705 JSON parse OK: answer='St....', citations=[5, 6]\n",
      "[Ex 127] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 127] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 128/200 [1:21:02<37:28, 31.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 128] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  To answer this question, evidenc...\n",
      "[Ex 128] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 128] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 128] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 128] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 128] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[5, 7]\n",
      "[Ex 128] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 128] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 129/200 [1:22:16<52:07, 44.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 129] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that It is the first instalment in the \\\"Fifty Shades\\\" trilogy that traces the deepening relationship between a college graduate, Anasta...\n",
      "[Ex 129] \u2705 JSON parse OK: answer='2011...', citations=[1, 5]\n",
      "[Ex 129] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [8].\",   \"answer\": \"2011\",   \"citations\": [     4,     8   ] }...\n",
      "[Ex 129] \u2705 JSON parse OK: answer='2011...', citations=[4, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 130/200 [1:22:52<48:18, 41.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 130] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 130] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 130] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 130] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 131/200 [1:23:03<37:21, 32.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 131] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that He reached the quarterfinals of the 1995 Paris Masters, the 1998 Canada Masters and the 1998 Cincinnati Masters, and achieved a care...\n",
      "[Ex 131] \u2705 JSON parse OK: answer='Fabio Fognini...', citations=[2, 4]\n",
      "[Ex 131] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Fabio Fognini\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 131] \u2705 JSON parse OK: answer='Fabio Fognini...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 132/200 [1:23:39<37:50, 33.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 132] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 132] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 132] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"Dryopteris\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 132] \u2705 JSON parse OK: answer='Dryopteris...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 133/200 [1:23:51<30:00, 26.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 133] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [6] shows that Astrid Kirchherr (born 20 May 1938) is a German photographer and artist and is well known for her association with the Beatles (alon...\n",
      "[Ex 133] \u26a0\ufe0f  JSON parse failed: Invalid \\uXXXX escape: line 3 column 238 (char 240. Trying fallback...\n",
      "[Ex 133] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 133] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 133] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 133] \ud83d\udcdd Regex result: answer='German...', citations=[6, 8]\n",
      "[Ex 133] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"German\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 133] \u2705 JSON parse OK: answer='German...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 134/200 [1:24:41<37:13, 33.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 134] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Since I cannot determine a defin...\n",
      "[Ex 134] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 134] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 134] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 134] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"Fade Out: The Calamitous Final Days of MGM\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 134] \u2705 JSON parse OK: answer='Fade Out: The Calamitous Final Days of MGM...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 135/200 [1:24:58<31:26, 29.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 135] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that The women's teams are sometimes called the Lady Longhorns, but generally both the men's and women's teams are referred to as the Lon...\n",
      "[Ex 135] \u2705 JSON parse OK: answer='University of Texas...', citations=[4, 7]\n",
      "[Ex 135] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 135] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 136/200 [1:25:34<32:55, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 136] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 136] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 136] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"poet\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 136] \u2705 JSON parse OK: answer='poet...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 137/200 [1:25:45<26:17, 25.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 137] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  The evidence does not provide en...\n",
      "[Ex 137] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 137] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 137] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 137] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 137] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 138/200 [1:26:01<23:09, 22.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 138] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 138] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 138] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Londonderry\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 138] \u2705 JSON parse OK: answer='Londonderry...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 139/200 [1:26:13<19:29, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 139] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  The question asks which is farth...\n",
      "[Ex 139] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 139] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 139] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 139] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 139] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 5]\n",
      "[Ex 139] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [5].\",   \"answer\": \"Sheridan County\",   \"citations\": [     1,     5   ] }...\n",
      "[Ex 139] \u2705 JSON parse OK: answer='Sheridan County...', citations=[1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 140/200 [1:27:07<29:33, 29.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 140] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 140] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 140] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Championship\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 140] \u2705 JSON parse OK: answer='Championship...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 141/200 [1:27:18<23:45, 24.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 141] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 141] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 141] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 141] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 142/200 [1:27:30<19:41, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 142] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }                                  ...\n",
      "[Ex 142] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 142] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 142] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 143/200 [1:28:44<34:38, 36.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 143] \ud83d\udd0d Extracting from: 5 with a new foreword by Peter Bergman and a new afterword by Peter J.  Nietzsche: Philosopher, Psychologist, Antichrist is a book about the German philosopher Friedrich Nietzsche by the philosopher W...\n",
      "[Ex 143] \u26a0\ufe0f  JSON parse failed: Extra data: line 1 column 3 (char 2). Trying fallback...\n",
      "[Ex 143] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 143] \ud83d\udcdd Regex result: answer='5 with a new foreword by Peter Bergman and a new a...', citations=[]\n",
      "[Ex 143] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [7].\",   \"answer\": \"Theodor W. Adorno\",   \"citations\": [     6,     7   ] }...\n",
      "[Ex 143] \u2705 JSON parse OK: answer='Theodor W. Adorno...', citations=[6, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 144/200 [1:29:54<43:21, 46.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 144] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Huis Ten Bosch Station (\u30cf\u30a6\u30b9\u30c6\u30f3\u30dc\u30b9\u99c5 , Hausutenbosu-eki ) is a railway station on the \\u0053\\u004f\\u0055\\u004d Line in Haenosaki-ch\\u00f...\n",
      "[Ex 144] \u2705 JSON parse OK: answer='Japan...', citations=[2, 4]\n",
      "[Ex 144] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 144] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 145/200 [1:30:47<44:25, 48.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 145] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  From evidence [2]: Courage the C...\n",
      "[Ex 145] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 145] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 145] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 145] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 145] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[2, 5]\n",
      "[Ex 145] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [5].\",   \"answer\": \"Cartoon Network\",   \"citations\": [     2,     5   ] }...\n",
      "[Ex 145] \u2705 JSON parse OK: answer='Cartoon Network...', citations=[2, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 146/200 [1:32:01<50:31, 56.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 146] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 146] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 146] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 146] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 147/200 [1:32:12<37:48, 42.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 147] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 147] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 147] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"21\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 147] \u2705 JSON parse OK: answer='21...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 148/200 [1:32:24<28:58, 33.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 148] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [4] shows that Tom Vaughan (born 5 September 1969) is a Scottish television and film director., and evidence [5] indicates that What Happens in Veg...\n",
      "[Ex 148] \u2705 JSON parse OK: answer='Tom Vaughan...', citations=[4, 5]\n",
      "[Ex 148] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [5].\",   \"answer\": \"Dana Fox\",   \"citations\": [     4,     5   ] }...\n",
      "[Ex 148] \u2705 JSON parse OK: answer='Dana Fox...', citations=[4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 149/200 [1:32:59<28:45, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 149] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation: To answer this ques...\n",
      "[Ex 149] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 149] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 149] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 149] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [7].\",   \"answer\": \"International Ultraviolet Explorer\",   \"citations\": [     4,     7   ] }...\n",
      "[Ex 149] \u2705 JSON parse OK: answer='International Ultraviolet Explorer...', citations=[4, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 150/200 [1:34:05<36:21, 43.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 150] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Li Yitong (; born December 23, 1995 in Xi'an, Shaanxi, China) is a Chinese idol singer., and evidence [8] indicates that It started ...\n",
      "[Ex 150] \u2705 JSON parse OK: answer='Dragon TV...', citations=[1, 8]\n",
      "[Ex 150] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [8].\",   \"answer\": \"Dragon TV\",   \"citations\": [     2,     8   ] }...\n",
      "[Ex 150] \u2705 JSON parse OK: answer='Dragon TV...', citations=[2, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 151/200 [1:34:42<33:53, 41.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 151] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }    However, if you are asking abo...\n",
      "[Ex 151] \u26a0\ufe0f  JSON parse failed: Extra data: line 10 column 1 (char 170). Trying fallback...\n",
      "[Ex 151] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 151] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 151] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 151] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 8]\n",
      "[Ex 151] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [8].\",   \"answer\": \"W. R. Grace Building\",   \"citations\": [     1,     8   ] }...\n",
      "[Ex 151] \u2705 JSON parse OK: answer='W. R. Grace Building...', citations=[1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 152/200 [1:35:55<40:55, 51.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 152] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Bangalore Naatkal (English: \\\"Bangalore Days\\\" ) is a 2016 Indian Tamil comedy-drama film directed by Bommarillu Bhaskar, which is a...\n",
      "[Ex 152] \u2705 JSON parse OK: answer='Rana Daggubati...', citations=[2, 8]\n",
      "[Ex 152] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"Ramanaidu Daggubati\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 152] \u2705 JSON parse OK: answer='Ramanaidu Daggubati...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 153/200 [1:36:47<40:09, 51.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 153] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 153] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 153] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 153] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 154/200 [1:36:59<30:09, 39.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 154] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 154] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 154] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 154] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 155/200 [1:37:10<23:15, 31.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 155] \ud83d\udd0d Extracting from: ) are an Iranian ethnic group from Azerbaijan.  They speak the Azerbaijani language, which is a Turkic language. [9] Title: Yoruba people - Yoruba people (Yoruba: \"\u1eb9gb\u00e1 \u1eb9gb\u00e0\" ) are an African ethnic g...\n",
      "[Ex 155] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 155] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 155] \ud83d\udcdd Regex result: answer=') are an Iranian ethnic group from Azerbaijan.  Th...', citations=[]\n",
      "[Ex 155] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"Norwegian language\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 155] \u2705 JSON parse OK: answer='Norwegian language...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 156/200 [1:38:18<30:55, 42.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 156] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 156] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 156] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [7], [8].\",   \"answer\": \"yes\",   \"citations\": [     7,     8   ] }...\n",
      "[Ex 156] \u2705 JSON parse OK: answer='yes...', citations=[7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 157/200 [1:38:30<23:38, 33.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 157] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 157] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 157] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Esp\\u00edrito Santo Financial Group\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 157] \u2705 JSON parse OK: answer='Esp\u00edrito Santo Financial Group...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 158/200 [1:38:41<18:36, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 158] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  In order to answer this question...\n",
      "[Ex 158] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 158] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 158] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 158] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 158] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[1, 2]\n",
      "[Ex 158] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [2].\",   \"answer\": \"Canada\",   \"citations\": [     1,     2   ] }...\n",
      "[Ex 158] \u2705 JSON parse OK: answer='Canada...', citations=[1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 159/200 [1:39:56<27:54, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 159] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 159] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 159] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [4].\",   \"answer\": \"Battle of H\\u00fcrtgen Forest\",   \"citations\": [     1,     4   ] }...\n",
      "[Ex 159] \u2705 JSON parse OK: answer='Battle of H\u00fcrtgen Forest...', citations=[1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 160/200 [1:40:07<21:22, 32.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 160] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 160] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 160] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [8].\",   \"answer\": \"the United States Army\",   \"citations\": [     2,     8   ] }...\n",
      "[Ex 160] \u2705 JSON parse OK: answer='the United States Army...', citations=[2, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 161/200 [1:40:19<16:52, 25.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 161] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that The Battle of Tannenberg was fought between Russia and Germany from 26\\u201330 August 1914, during the first month of World War I., ...\n",
      "[Ex 161] \u2705 JSON parse OK: answer='August 1914...', citations=[2, 3]\n",
      "[Ex 161] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [3].\",   \"answer\": \"26\\u201330 August 1914\",   \"citations\": [     2,     3   ] }...\n",
      "[Ex 161] \u2705 JSON parse OK: answer='26\u201330 August 1914...', citations=[2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 162/200 [1:40:57<18:50, 29.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 162] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that The Irrational Atheist: Dissecting the Unholy Trinity of Dawkins, Harris, and Hitchens is a 2008 non-fiction book by Vox Day., and e...\n",
      "[Ex 162] \u2705 JSON parse OK: answer='alt-right activist...', citations=[2, 8]\n",
      "[Ex 162] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [8].\",   \"answer\": \"alt-right\",   \"citations\": [     2,     8   ] }...\n",
      "[Ex 162] \u2705 JSON parse OK: answer='alt-right...', citations=[2, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 163/200 [1:41:38<20:25, 33.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 163] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Daewoong Pharmaceutical Co., Ltd (Korean: \\uac70\\uac15\\uac0c) is a Seoul, South Korea-based bioengineering company operating as a su...\n",
      "[Ex 163] \u2705 JSON parse OK: answer='16...', citations=[2, 7]\n",
      "[Ex 163] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"world's 16th largest city\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 163] \u2705 JSON parse OK: answer='world's 16th largest city...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 164/200 [1:42:23<21:52, 36.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 164] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that The 1970 release of the single \\\"Ride a White Swan\\\" marked the culmination of this development, and the group soon became a commerc...\n",
      "[Ex 164] \u2705 JSON parse OK: answer='1967...', citations=[2, 4]\n",
      "[Ex 164] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"1967\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 164] \u2705 JSON parse OK: answer='1967...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 165/200 [1:42:55<20:32, 35.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 165] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  This question asks for the birth...\n",
      "[Ex 165] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 165] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 165] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 165] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"7 November 192610\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 165] \u2705 JSON parse OK: answer='7 November 192610...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 166/200 [1:44:09<26:35, 46.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 166] \ud83d\udd0d Extracting from: 0) in the BBC adaptation of \"The House of Eliot\".  She appeared in the 2007 West End revival of \"Equus\", for which she received a nomination for the Laurence Olivier Award for Best Actress. [9] Title:...\n",
      "[Ex 166] \u26a0\ufe0f  JSON parse failed: Extra data: line 1 column 2 (char 1). Trying fallback...\n",
      "[Ex 166] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 166] \ud83d\udcdd Regex result: answer='0) in the BBC adaptation of \"The House of Eliot\". ...', citations=[]\n",
      "[Ex 166] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 166] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 167/200 [1:45:23<30:15, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 167] \ud83d\udd0d Extracting from: June 6, 2005, by Capitol Records. [9] Title: The Blueprint - The Blueprint is the fourth studio album by American rapper Jay-Z, released on March 30, 2001 by Roc-A-Fella Records.  The album was record...\n",
      "[Ex 167] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 167] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 167] \ud83d\udcdd Regex result: answer='June 6, 2005, by Capitol Records....', citations=[]\n",
      "[Ex 167] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Baudot code\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 167] \u2705 JSON parse OK: answer='Baudot code...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 168/200 [1:46:37<32:21, 60.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 168] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 168] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 168] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 168] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 169/200 [1:46:49<23:44, 45.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 169] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that He is best known as the lead guitarist in the band Skyhooks, as the snide judge of \\\"Red Faces\\\", a segment of the long-running vari...\n",
      "[Ex 169] \u2705 JSON parse OK: answer='Redmond \"Red\" Symons...', citations=[2, 7]\n",
      "[Ex 169] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [7].\",   \"answer\": \"Sophie Charlene Akland Monk\",   \"citations\": [     3,     7   ] }...\n",
      "[Ex 169] \u2705 JSON parse OK: answer='Sophie Charlene Akland Monk...', citations=[3, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 170/200 [1:47:31<22:29, 44.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 170] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that The station is owned by Gray Television, as part of a duopoly with CW affiliate KSCW-DT (channel 33); Gray also operates Univision a...\n",
      "[Ex 170] \u2705 JSON parse OK: answer='WTLV...', citations=[2, 8]\n",
      "[Ex 170] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"KWCH-DT\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 170] \u2705 JSON parse OK: answer='KWCH-DT...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 171/200 [1:48:14<21:25, 44.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 171] \ud83d\udd0d Extracting from: 7.  The Jets lost to the Pittsburgh Steelers in the AFC Championship game, and the season ended with a 24-19 loss against the Pittsburgh Steelers. [7] Title: Peter Wallace Hobbs - Peter Wallace Hobbs ...\n",
      "[Ex 171] \u26a0\ufe0f  JSON parse failed: Extra data: line 1 column 2 (char 1). Trying fallback...\n",
      "[Ex 171] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 171] \ud83d\udcdd Regex result: answer='7.  The Jets lost to the Pittsburgh Steelers in th...', citations=[]\n",
      "[Ex 171] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Central Park\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 171] \u2705 JSON parse OK: answer='Central Park...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 172/200 [1:49:28<24:51, 53.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 172] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 172] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 172] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Eminem\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 172] \u2705 JSON parse OK: answer='Eminem...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 173/200 [1:49:40<18:20, 40.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 173] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Penning album tracks for Katy Perry (\\\"Hummingbird Heartbeat\\\"), Kelly Rowland (\\\"Everywhere You Go\\\") and Rihanna (\\\"Cheers (Drink ...\n",
      "[Ex 173] \u2705 JSON parse OK: answer='Katy Perry...', citations=[1, 8]\n",
      "[Ex 173] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 173] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 174/200 [1:50:23<17:55, 41.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 174] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that It samples the chorus and instrumentation of Stevie Wonder's 1976 song \\\"Pastime Paradise\\\"., and evidence [8] indicates that Fantas...\n",
      "[Ex 174] \u2705 JSON parse OK: answer='Gangsta's Paradise...', citations=[5, 8]\n",
      "[Ex 174] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [8].\",   \"answer\": \"Gangsta's Paradise\",   \"citations\": [     5,     8   ] }...\n",
      "[Ex 174] \u2705 JSON parse OK: answer='Gangsta's Paradise...', citations=[5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 175/200 [1:50:57<16:19, 39.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 175] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation: To answer this ques...\n",
      "[Ex 175] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 175] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 175] \u26a0\ufe0f  Substring parse failed. Using regex...\n",
      "[Ex 175] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 175] \ud83d\udcdd Regex result: answer='insufficient context...', citations=[3, 5]\n",
      "[Ex 175] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 175] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 176/200 [1:52:09<19:37, 49.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 176] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Selfridges, also known as Selfridges & Co., is a chain of high end department stores in the United Kingdom, operated by Selfridges R...\n",
      "[Ex 176] \u2705 JSON parse OK: answer='Harrods...', citations=[5, 6]\n",
      "[Ex 176] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [6].\",   \"answer\": \"Harrods\",   \"citations\": [     5,     6   ] }...\n",
      "[Ex 176] \u2705 JSON parse OK: answer='Harrods...', citations=[5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 177/200 [1:52:38<16:30, 43.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 177] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that The song also features rap parts from Darryl, RB Djan and Ryan Babel., and evidence [7] indicates that He can play as a striker or l...\n",
      "[Ex 177] \u2705 JSON parse OK: answer='Ryan Babel...', citations=[1, 7]\n",
      "[Ex 177] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [7].\",   \"answer\": \"Ryan Babel\",   \"citations\": [     1,     7   ] }...\n",
      "[Ex 177] \u2705 JSON parse OK: answer='Ryan Babel...', citations=[1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 178/200 [1:53:04<13:54, 37.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 178] \ud83d\udd0d Extracting from: o appearance in \"The Last Stand\" as Mystique.  He also appeared in the \"X-Men Origins: Wolverine\" film as Wolverine, but only in a cameo appearance. [9] Title: The Andy Griffith Show - Sheriff Andrew ...\n",
      "[Ex 178] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 178] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 178] \ud83d\udcdd Regex result: answer='o appearance in \"The Last Stand\" as Mystique.  He ...', citations=[]\n",
      "[Ex 178] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [6].\",   \"answer\": \"Captain B.J. Hunnicutt\",   \"citations\": [     2,     6   ] }...\n",
      "[Ex 178] \u2705 JSON parse OK: answer='Captain B.J. Hunnicutt...', citations=[2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 179/200 [1:54:19<17:08, 48.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 179] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [6] shows that It launched 24 April 2006, the same day as rival channel Cartoon Network Too., and evidence [8] indicates that Cartoon Network Too w...\n",
      "[Ex 179] \u2705 JSON parse OK: answer='Cartoon Network Too...', citations=[6, 8]\n",
      "[Ex 179] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [6], [8].\",   \"answer\": \"Cartoon Network Too\",   \"citations\": [     6,     8   ] }...\n",
      "[Ex 179] \u2705 JSON parse OK: answer='Cartoon Network Too...', citations=[6, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 180/200 [1:54:47<14:14, 42.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 180] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 180] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 180] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"GE Appliances\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 180] \u2705 JSON parse OK: answer='GE Appliances...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 181/200 [1:54:58<10:34, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 181] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Bonnie Hale Leman was the founder of Quilter's Newsletter Magazine, and one of the nation's first female magazine publishers., and e...\n",
      "[Ex 181] \u2705 JSON parse OK: answer='1850...', citations=[5, 6]\n",
      "[Ex 181] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 181] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 182/200 [1:55:35<10:16, 34.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 182] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 182] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 182] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"Jacques Tourneur\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 182] \u2705 JSON parse OK: answer='Jacques Tourneur...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 183/200 [1:55:46<07:47, 27.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 183] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 183] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 183] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [4].\",   \"answer\": \"That Darn Cat!\",   \"citations\": [     2,     4   ] }...\n",
      "[Ex 183] \u2705 JSON parse OK: answer='That Darn Cat!...', citations=[2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 184/200 [1:55:58<06:03, 22.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 184] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 184] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 184] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 184] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 185/200 [1:56:09<04:50, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 185] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  Explanation:  To answer this que...\n",
      "[Ex 185] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 185] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 185] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 185] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"its eclectic mix of musical styles\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 185] \u2705 JSON parse OK: answer='its eclectic mix of musical styles...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 186/200 [1:57:24<08:21, 35.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 186] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 186] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 186] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 186] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 187/200 [1:57:35<06:10, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 187] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Manfred von Richthofen, also known as the \\\"Red Baron\\\", was a fighter pilot with the German Air Force during World War I and one of...\n",
      "[Ex 187] \u2705 JSON parse OK: answer='books, films...', citations=[1, 3]\n",
      "[Ex 187] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"books, films and other media\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 187] \u2705 JSON parse OK: answer='books, films and other media...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 188/200 [1:58:13<06:16, 31.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 188] \ud83d\udd0d Extracting from: , and record producer.  He is known for his collaborations with 50 Cent, Chris Brown, Ne-Yo, and Big Sean, among others.  The album sold 170,000 copies in the US and produced four singles.  Jeremih's ...\n",
      "[Ex 188] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 188] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 188] \ud83d\udcdd Regex result: answer=', and record producer.  He is known for his collab...', citations=[]\n",
      "[Ex 188] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 188] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 189/200 [1:59:27<08:06, 44.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 189] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that The Johns Hopkins University (commonly referred to as Johns Hopkins, JHU, or simply Hopkins) is an American private research univers...\n",
      "[Ex 189] \u2705 JSON parse OK: answer='Johns Hopkins University...', citations=[5, 8]\n",
      "[Ex 189] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 189] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 190/200 [2:00:05<07:02, 42.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 190] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [5] shows that Simone Bolelli (born 8 October 1985; ] ) is an Italian professional tennis player., and evidence [7] indicates that Caroline Wozniac...\n",
      "[Ex 190] \u2705 JSON parse OK: answer='Simone Bolelli...', citations=[5, 7]\n",
      "[Ex 190] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [5], [7].\",   \"answer\": \"Simone Bolelli\",   \"citations\": [     5,     7   ] }...\n",
      "[Ex 190] \u2705 JSON parse OK: answer='Simone Bolelli...', citations=[5, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 191/200 [2:00:37<05:53, 39.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 191] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that The American airborne landings in Normandy were the first American combat operations during Operation Overlord, the invasion of Norm...\n",
      "[Ex 191] \u2705 JSON parse OK: answer='American airborne landings...', citations=[2, 3]\n",
      "[Ex 191] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [6].\",   \"answer\": \"D-Day\",   \"citations\": [     3,     6   ] }...\n",
      "[Ex 191] \u2705 JSON parse OK: answer='D-Day...', citations=[3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 192/200 [2:01:16<05:13, 39.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 192] \ud83d\udd0d Extracting from: ANZ Bank New Zealand is a major banking group in New Zealand with its headquarters in Wellington.  ANZ Bank New Zealand is a subsidiary of ANZ, Australia and New Zealand Banking Group Limited, and ope...\n",
      "[Ex 192] \u26a0\ufe0f  JSON parse failed: Expecting value: line 1 column 1 (char 0). Trying fallback...\n",
      "[Ex 192] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 192] \ud83d\udcdd Regex result: answer='ANZ Bank New Zealand is a major banking group in N...', citations=[]\n",
      "[Ex 192] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 192] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 193/200 [2:02:31<05:48, 49.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 193] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  I cannot determine a definitive ...\n",
      "[Ex 193] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 193] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 193] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 193] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 193] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 194/200 [2:02:47<03:58, 39.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 194] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [1] shows that Armed and Dangerous is a 1986 American action-crime comedy film starring John Candy, Eugene Levy, Robert Loggia and Meg Ryan., and e...\n",
      "[Ex 194] \u2705 JSON parse OK: answer='John Candy...', citations=[1, 6]\n",
      "[Ex 194] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [6].\",   \"answer\": \"John Candy\",   \"citations\": [     1,     6   ] }...\n",
      "[Ex 194] \u2705 JSON parse OK: answer='John Candy...', citations=[1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 195/200 [2:03:22<03:11, 38.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 195] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [2] shows that Luca Parmitano (born 27 September 1976 in Patern\\u00f4, Sicily) is an Italian engineer and astronaut in the European Astronaut Corps...\n",
      "[Ex 195] \u2705 JSON parse OK: answer='Luca Parmitano...', citations=[2, 7]\n",
      "[Ex 195] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [2], [7].\",   \"answer\": \"Luca Parmitano\",   \"citations\": [     2,     7   ] }...\n",
      "[Ex 195] \u2705 JSON parse OK: answer='Luca Parmitano...', citations=[2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 196/200 [2:03:55<02:27, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 196] \ud83d\udd0d Extracting from: 00 m higher. [9] Title: Muztagh Ata - Muztagh Ata, or Muztagata (Uyghur: \u0645\u06c7\u0632 \u062a\u0627\u063a \u0626\u0627\u062a\u0627, \u041c\u0443\u0437\u0442\u0430\u04a3 \u0410\u0442\u0430, literally \"ice-mountain-father\"; ), is the second highest (7509 metres) of the mountains which form t...\n",
      "[Ex 196] \u26a0\ufe0f  JSON parse failed: Extra data: line 1 column 2 (char 1). Trying fallback...\n",
      "[Ex 196] \ud83d\udd04 Using regex fallback...\n",
      "[Ex 196] \ud83d\udcdd Regex result: answer='00 m higher....', citations=[]\n",
      "[Ex 196] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 196] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 197/200 [2:05:09<02:23, 47.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 197] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }...\n",
      "[Ex 197] \u2705 JSON parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 197] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [4], [6].\",   \"answer\": \"15\",   \"citations\": [     4,     6   ] }...\n",
      "[Ex 197] \u2705 JSON parse OK: answer='15...', citations=[4, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 198/200 [2:05:21<01:14, 37.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 198] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"To answer this question, evidence [3] shows that Brian Harold Billick (born February 28, 1954) is a former National Football League coach and commentator., and evidence [4] indicate...\n",
      "[Ex 198] \u2705 JSON parse OK: answer='the third season...', citations=[3, 4]\n",
      "[Ex 198] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [3], [4].\",   \"answer\": \"6th season\",   \"citations\": [     3,     4   ] }...\n",
      "[Ex 198] \u2705 JSON parse OK: answer='6th season...', citations=[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEvaluating Fine-tuned QLoRA: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 199/200 [2:05:54<00:35, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ex 199] \ud83d\udd0d Extracting from:  {   \"reasoning\": \"Based on the available evidence, I cannot determine a definitive answer to this question.\",   \"answer\": \"insufficient context\",   \"citations\": [] }  This is because Vocelli Pizza an...\n",
      "[Ex 199] \u26a0\ufe0f  JSON parse failed: Extra data: line 8 column 1 (char 168). Trying fallback...\n",
      "[Ex 199] \ud83d\udd0d Found JSON substring, parsing...\n",
      "[Ex 199] \u2705 Substring parse OK: answer='insufficient context...', citations=[]\n",
      "[Ex 199] \ud83d\udd0d Extracting from: {   \"reasoning\": \"Relevant evidence found in passages [1], [3].\",   \"answer\": \"Pizza\",   \"citations\": [     1,     3   ] }...\n",
      "[Ex 199] \u2705 JSON parse OK: answer='Pizza...', citations=[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned QLoRA: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [2:07:08<00:00, 38.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca FINE-TUNED QLORA - EVALUATION RESULTS\n",
      "================================================================================\n",
      "Total Examples: 200\n",
      "Exact Match (EM): 0.415\n",
      "F1 Score: 0.464\n",
      "Citation Precision: 0.575\n",
      "Citation Recall: 0.750\n",
      "Citation F1: 0.575\n",
      "Insufficient Context Detection: 60.0% (51/85)\n",
      "================================================================================\n",
      "\n",
      "\u2705 Fine-tuned evaluation complete!\n",
      "\ud83d\udcca Key Results:\n",
      "   \u2022 Exact Match: 41.5%\n",
      "   \u2022 F1 Score: 0.464\n",
      "   \u2022 Citation F1: 0.575\n",
      "   \u2022 Insufficient Context Detection: 60.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned Model Full Evaluation\n",
    "# Evaluate QLoRA fine-tuned model on full evaluation dataset\n",
    "\n",
    "print(\"\ud83d\ude80 Starting fine-tuned model evaluation...\")\n",
    "print(f\"   Model: Mistral-7B-Instruct (QLoRA fine-tuned)\")\n",
    "print(f\"   Strategy: Direct JSON output (instruction-tuned)\")\n",
    "print(f\"   Dataset: Full eval_dataset ({len(eval_dataset)} examples)\\n\")\n",
    "\n",
    "# Evaluate using unified function\n",
    "finetuned_results = evaluate_model_comprehensive(\n",
    "    model=eval_model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=eval_dataset,\n",
    "    evaluator=evaluator,\n",
    "    model_name=\"Fine-tuned QLoRA\",\n",
    "    max_examples=None,  # Evaluate on full dataset\n",
    "    use_rag_prompting=False,  # Use direct input_text from dataset\n",
    "    verbose_level=\"sample\",  # Print first 5 examples\n",
    "    wandb_prefix=\"final_eval\",\n",
    "    building_prompts=None\n",
    ")\n",
    "\n",
    "# Store for later comparison\n",
    "print(\"\u2705 Fine-tuned evaluation complete!\")\n",
    "print(f\"\ud83d\udcca Key Results:\")\n",
    "print(f\"   \u2022 Exact Match: {finetuned_results['em']:.1%}\")\n",
    "print(f\"   \u2022 F1 Score: {finetuned_results['f1']:.3f}\")\n",
    "print(f\"   \u2022 Citation F1: {finetuned_results['citation_f1']:.3f}\")\n",
    "print(f\"   \u2022 Insufficient Context Detection: {finetuned_results['insufficient_context_rate']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0h5a0b1clbig",
   "metadata": {
    "id": "0h5a0b1clbig"
   },
   "source": [
    "##  Baseline vs Fine-tuned Comparison\n",
    "\n",
    "Side-by-side comparison of baseline RAG prompting approach vs QLoRA fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "jb8260gfnfm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1760067607731,
     "user": {
      "displayName": "jeff gong",
      "userId": "14678463099730251443"
     },
     "user_tz": 240
    },
    "id": "jb8260gfnfm",
    "outputId": "cbebf2cb-5e73-4301-8000-6ced0ea85b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\ud83d\udcca BASELINE vs FINE-TUNED MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "                        Metric Baseline (RAG) Fine-tuned (QLoRA) \u0394 (Absolute)   \u0394 (%)\n",
      "              Exact Match (EM)          0.175              0.415       +0.240 +137.1%\n",
      "                      F1 Score          0.274              0.464       +0.190  +69.3%\n",
      "            Citation Precision          0.458              0.575       +0.117  +25.5%\n",
      "               Citation Recall          0.703              0.750       +0.047   +6.7%\n",
      "                   Citation F1          0.402              0.575       +0.173  +43.0%\n",
      "Insufficient Context Detection          0.117              0.600       +0.483 +412.8%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\ud83d\udcc8 SUMMARY:\n",
      "   \u2022 Dataset sizes: Baseline (100 examples) vs Fine-tuned (200 examples)\n",
      "   \u2022 Average improvement: 0.208 (+115.8%)\n",
      "   \u2022 Best improvement: Insufficient Context Detection (+0.483, +412.8%)\n",
      "\n",
      "================================================================================\n",
      "\ud83c\udf89 Evaluation complete! Fine-tuned model shows improvement across all metrics.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comprehensive Side-by-Side Comparison\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca BASELINE vs FINE-TUNED MODEL COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "baseline_results={'em':0.175, 'f1':0.274, 'citation_f1':0.402, 'citation_precision':0.458,'citation_recall':0.703 ,'insufficient_context_detection_rate':0.117, 'total_examples':200}\n",
    "\n",
    "\n",
    "finetuned_results = {'em':0.415,'f1':0.464,'citation_f1':0.575, 'citation_precision':0.575, 'citation_recall':0.750, 'insufficient_context_detection_rate':0.6, 'total_examples':200}\n",
    "\n",
    "\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Exact Match (EM)',\n",
    "        'F1 Score',\n",
    "        'Citation Precision',\n",
    "        'Citation Recall',\n",
    "        'Citation F1',\n",
    "        'Insufficient Context Detection'\n",
    "    ],\n",
    "    'Baseline (RAG)': [\n",
    "        baseline_results['em'],\n",
    "        baseline_results['f1'],\n",
    "        baseline_results['citation_precision'],\n",
    "        baseline_results['citation_recall'],\n",
    "        baseline_results['citation_f1'],\n",
    "        baseline_results['insufficient_context_detection_rate']\n",
    "    ],\n",
    "    'Fine-tuned (QLoRA)': [\n",
    "        finetuned_results['em'],\n",
    "        finetuned_results['f1'],\n",
    "        finetuned_results['citation_precision'],\n",
    "        finetuned_results['citation_recall'],\n",
    "        finetuned_results['citation_f1'],\n",
    "        finetuned_results['insufficient_context_detection_rate']\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate improvements\n",
    "comparison_df['\u0394 (Absolute)'] = comparison_df['Fine-tuned (QLoRA)'] - comparison_df['Baseline (RAG)']\n",
    "comparison_df['\u0394 (%)'] = (comparison_df['\u0394 (Absolute)'] / comparison_df['Baseline (RAG)']) * 100\n",
    "\n",
    "# Format for display\n",
    "comparison_df_display = comparison_df.copy()\n",
    "comparison_df_display['Baseline (RAG)'] = comparison_df_display['Baseline (RAG)'].apply(lambda x: f\"{x:.3f}\")\n",
    "comparison_df_display['Fine-tuned (QLoRA)'] = comparison_df_display['Fine-tuned (QLoRA)'].apply(lambda x: f\"{x:.3f}\")\n",
    "comparison_df_display['\u0394 (Absolute)'] = comparison_df_display['\u0394 (Absolute)'].apply(lambda x: f\"{x:+.3f}\")\n",
    "comparison_df_display['\u0394 (%)'] = comparison_df_display['\u0394 (%)'].apply(lambda x: f\"{x:+.1f}%\")\n",
    "\n",
    "print(comparison_df_display.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\ud83d\udcc8 SUMMARY:\")\n",
    "print(f\"   \u2022 Dataset sizes: Baseline (100 examples) vs Fine-tuned ({finetuned_results['total_examples']} examples)\")\n",
    "print(f\"   \u2022 Average improvement: {comparison_df['\u0394 (Absolute)'].mean():.3f} ({comparison_df['\u0394 (%)'].mean():+.1f}%)\")\n",
    "\n",
    "# Identify best improvements\n",
    "best_metric = comparison_df.loc[comparison_df['\u0394 (Absolute)'].idxmax(), 'Metric']\n",
    "best_improvement = comparison_df.loc[comparison_df['\u0394 (Absolute)'].idxmax(), '\u0394 (Absolute)']\n",
    "best_improvement_pct = comparison_df.loc[comparison_df['\u0394 (Absolute)'].idxmax(), '\u0394 (%)']\n",
    "print(f\"   \u2022 Best improvement: {best_metric} (+{best_improvement:.3f}, +{best_improvement_pct:.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "# # Log comparison to W&B\n",
    "# if wandb.run:\n",
    "#     # Create W&B table\n",
    "#     comparison_table = wandb.Table(dataframe=comparison_df)\n",
    "#     wandb.log({\n",
    "#         \"model_comparison\": comparison_table,\n",
    "#         \"avg_improvement_absolute\": comparison_df['\u0394 (Absolute)'].mean(),\n",
    "#         \"avg_improvement_percent\": comparison_df['\u0394 (%)'].mean()\n",
    "#     })\n",
    "\n",
    "#     # Log bar chart comparison\n",
    "#     metrics_for_chart = ['Exact Match (EM)', 'F1 Score', 'Citation F1']\n",
    "#     chart_data = []\n",
    "#     for metric in metrics_for_chart:\n",
    "#         row = comparison_df[comparison_df['Metric'] == metric].iloc[0]\n",
    "#         chart_data.append([metric, \"Baseline\", row['Baseline (RAG)']])\n",
    "#         chart_data.append([metric, \"Fine-tuned\", row['Fine-tuned (QLoRA)']])\n",
    "\n",
    "#     chart_table = wandb.Table(data=chart_data, columns=[\"Metric\", \"Model\", \"Score\"])\n",
    "#     wandb.log({\n",
    "#         \"comparison_bar_chart\": wandb.plot.bar(\n",
    "#             chart_table,\n",
    "#             \"Metric\",\n",
    "#             \"Score\",\n",
    "#             title=\"Baseline vs Fine-tuned Performance\"\n",
    "#         )\n",
    "#     })\n",
    "\n",
    "    # print(\"\\n\u2705 Comparison logged to W&B!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83c\udf89 Evaluation complete! Fine-tuned model shows improvement across all metrics.\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hPzUf_IKNFur",
   "metadata": {
    "id": "hPzUf_IKNFur"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01bf9cefce43421d800b14b223487194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "044ece2fc5894057921c5c3d931d31f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27e593352e8d4cf8979ddaa25566fb03",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_ec976b5c1f5448e882998360593be166",
      "value": "\u2007166M/166M\u2007[00:01&lt;00:00,\u2007168MB/s]"
     }
    },
    "04e93e20c6f649f78a7dbcf66a2fb00b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06feb49afef6433498442b813539716e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0910ae59a4074b55af9f7413642000dd",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_f995fecfbb6042698d8d9155895a8c5f",
      "value": "\u20074.94G/4.94G\u2007[01:26&lt;00:00,\u200791.1MB/s]"
     }
    },
    "07b9e0c1fd5b4169b342e6a05766caf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40bea4c4d16f486ebc02e1f2fe08648f",
       "IPY_MODEL_81c2a555d761422bbcd5ec905cd4a955",
       "IPY_MODEL_c5af8b913c1d4f20835bb6b247d95374"
      ],
      "layout": "IPY_MODEL_8fa7ee8a8f744804bdda4711d866c998"
     }
    },
    "083383565c284f7bb71f045efa337cd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "085305ec56e346ebbdf98a7f4110c941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0910ae59a4074b55af9f7413642000dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d4053f76f8743a09bb511320e2b9df2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0d8b59acef194d93b73eb1ba54d42676": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e3c62526f7740b1af7e45bc8c0c72fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ff0bfa4510b45458f48dc8f185283d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6f5bed45ea4420a863e61242518e0eb",
       "IPY_MODEL_41ce69f8fc6a4112a4edf7072de860b5",
       "IPY_MODEL_06feb49afef6433498442b813539716e"
      ],
      "layout": "IPY_MODEL_cc128d431dd346aaa8cde7999808bc75"
     }
    },
    "14eef61174bf46d5a68f0d661b2c823b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1612a18bc3e94b68865ddecc13bf857b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "164af62aa685453bbb95511f65b6842b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17c033f53f2b482094180210cc621303": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "187a6a6288ca4bdba264a1d2ed89874c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "191cae93f13d4250a4a4c7269ac36c44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19d9aaa0d4f74837994d4ad10935b74d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1cf16a2faed1473ba9ae4f4645e1c06b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d328fb9b8b4483db06b0db7598ff15a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e7d3b05ea3245da9c5bd72092096471": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ec2b9929b4548d99aa981ac5ca67a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a9a6ee0c1b443d79c8bc2734b7b731e",
       "IPY_MODEL_7ac61abe432f4bea90d106a6e4624873",
       "IPY_MODEL_b84fee492fe041549927c1c9ceda470c"
      ],
      "layout": "IPY_MODEL_fdf6db8980a94224a87f646f1222ff86"
     }
    },
    "1fa47185cda4487bbcbd1bd0ade7aa08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f444056f2d34e2e83c87aaa8bd08c74",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_97c351c9a07a415db893fc5e4f37b353",
      "value": "model.safetensors.index.json:\u2007"
     }
    },
    "2215bbc48a8845878e336f0e93a7c18b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22f01198bd9a4a78b56e8c48b978ba37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2332f6013fa54ef0957511b0b8d011aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2431ccdffec249b5b6b5a7c361b0e270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24c29762c9ef4e7e80d9c9294e116af3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e7d3b05ea3245da9c5bd72092096471",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_383facbcd33641088d8ce1c0a9679fff",
      "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
     }
    },
    "26dd87a3420640ec8b0ea4340949b36f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47548c8559464ae1aeec4836f9a84def",
      "max": 493443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdf35d781fd446058421b2c19000df03",
      "value": 493443
     }
    },
    "27e593352e8d4cf8979ddaa25566fb03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "281ddd4688ad454e8d2cf7fa118406d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083383565c284f7bb71f045efa337cd4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79dbe78107474dc28bc60a3fa5fea04e",
      "value": 1
     }
    },
    "2828185fc1fe41418db2b23adfc46cb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2909f357e4e0498b96e735071a2b72fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4171e5f724f8465a838211dc999bc5cf",
       "IPY_MODEL_ec3daf2847104603b36d677f5e791e1f",
       "IPY_MODEL_044ece2fc5894057921c5c3d931d31f3"
      ],
      "layout": "IPY_MODEL_5aac047c359641ab92baaf95a99ceb4d"
     }
    },
    "2d86d10d6c8b438a95a3215fc9089bdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e110498596345228afc933e0692b9c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e8a0f38378c460b95102fdff1202f2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9691836563a04a8ba549433f9a9c2987",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_1d328fb9b8b4483db06b0db7598ff15a",
      "value": "\u20073/3\u2007[00:20&lt;00:00,\u2007\u20076.89s/it]"
     }
    },
    "2ec4e474c8114be7978b0b2867567604": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3119963fe9d14d249896d0f705b91f01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "32eaf86422d842068b067a47e8172e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "335079439dfa48898565592d96b5e3a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34748267fb164a64a74cdaf23510fd2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35ccbbe7d2ca4289a33055e128f2e0d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24c29762c9ef4e7e80d9c9294e116af3",
       "IPY_MODEL_bb5eccb2a97242e7810e7cf13ce713fb",
       "IPY_MODEL_4a7197cfd8544606a0f6bb35c2afd956"
      ],
      "layout": "IPY_MODEL_b65e89a8d36c4701a8ea30070b2cc6d7"
     }
    },
    "363ab3dc989b465da01f6dad251a9e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36eb02379ec9430d83eb522601d99a7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "383facbcd33641088d8ce1c0a9679fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "38408e50ddea46dca419ff77cee9dc32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52d7d4ec01664f4b9e2f10ada5265376",
       "IPY_MODEL_a37129fe3edd4682ba7baaf60d28d26a",
       "IPY_MODEL_eb0f5bcd20df4e158c66c63de50a96e5"
      ],
      "layout": "IPY_MODEL_2332f6013fa54ef0957511b0b8d011aa"
     }
    },
    "393e1c4fc4444138baf45aa868857376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d2810c47bc547268c7ef0e9275cceeb",
       "IPY_MODEL_bd4a2b63edeb4d938a86a18c08bfb319",
       "IPY_MODEL_ca2cf828cf5b47779ddb270af704b9e5"
      ],
      "layout": "IPY_MODEL_4cb63cdd54184cd5bfdf5019770bada4"
     }
    },
    "39c2b0015fdd434687d68e211419ab8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39e312a338354fc887b7e9d1342ab58f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_191cae93f13d4250a4a4c7269ac36c44",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_8dc94edbbcc54876bb568618f32affd8",
      "value": "\u2007596/596\u2007[00:00&lt;00:00,\u200769.4kB/s]"
     }
    },
    "3b48c0cf2c2f4a5b81af800b8d20232b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d27c4fe526644f6bbb710a6e1d04c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4fff7bee8584db98b91a19708319453",
       "IPY_MODEL_86bfc101972d4e108269f33340d1e7e3",
       "IPY_MODEL_978554eb4607431b91cdaf138ebb84b2"
      ],
      "layout": "IPY_MODEL_4aba96986c95416782a068d1022877ab"
     }
    },
    "3d2810c47bc547268c7ef0e9275cceeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_335079439dfa48898565592d96b5e3a0",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_085305ec56e346ebbdf98a7f4110c941",
      "value": "config.json:\u2007100%"
     }
    },
    "3e84f05ae0274c69ab6f322e6ba722d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0a127e339cb4174bf00654cf3f32f78",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_d2c8af1cb1df4c52af892a5225dcc1c6",
      "value": "model-00002-of-00003.safetensors:\u2007100%"
     }
    },
    "3fba0d9d196c45e297027f6e6085b763": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4018b3b5fc444356a4954b1e4a4f5b08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85ac65e52717443ba04947a77a61f131",
       "IPY_MODEL_c037a04f65c9439d80f8b7d8654ac95c",
       "IPY_MODEL_f1c4402da7994ca1a04f2783b7240890"
      ],
      "layout": "IPY_MODEL_7c287fef7c73452abacae570c16684c6"
     }
    },
    "403d5350f8fc408aa5f005a22d16c73c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40bea4c4d16f486ebc02e1f2fe08648f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_164af62aa685453bbb95511f65b6842b",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_4cd721c36321453da4ddcf8c688720e1",
      "value": "tokenizer.json:\u2007"
     }
    },
    "413160e06dfc47598c79d7871bf552ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbd58975e5774330802e46950eb59a9e",
       "IPY_MODEL_26dd87a3420640ec8b0ea4340949b36f",
       "IPY_MODEL_b9b03cd8015d472f92928b29714581a7"
      ],
      "layout": "IPY_MODEL_505b22c0710845e4a5627454cdd2c0a0"
     }
    },
    "4171e5f724f8465a838211dc999bc5cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4596ce9ec834648896d56e39e60821a",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_04e93e20c6f649f78a7dbcf66a2fb00b",
      "value": "distractor/train-00000-of-00002.parquet:\u2007100%"
     }
    },
    "41ce69f8fc6a4112a4edf7072de860b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f4288faa20d45b68b8aaa1434d196c8",
      "max": 4943162336,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_829ced0967144b4b8d759d36191c8a3c",
      "value": 4943162336
     }
    },
    "458e9b1854684513b7ae8d66c0eab48c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47548c8559464ae1aeec4836f9a84def": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4764aef8050f4ff4872d7289e525097c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "485a6625f59149909c1288f50a6a796a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a7197cfd8544606a0f6bb35c2afd956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_633708e71daf4140a8161ec3405cda70",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_804c36176e5c4d1ab7964e24584614a6",
      "value": "\u20073/3\u2007[00:29&lt;00:00,\u2007\u20079.50s/it]"
     }
    },
    "4aba96986c95416782a068d1022877ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cb63cdd54184cd5bfdf5019770bada4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cd721c36321453da4ddcf8c688720e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4dd6f9fea4ee4324bdf7485ae29cbc9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ea46134a60d4522b8076987fb045033": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fa21d794f734d078c62f1c96ca342ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fe26d02caad41528d5e64a45004a070": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "505b22c0710845e4a5627454cdd2c0a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "509cd0b9862b4c318dd9682615bc945d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52d7d4ec01664f4b9e2f10ada5265376": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d8b59acef194d93b73eb1ba54d42676",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_f8b6638f7efe4ba6a8b399f989a79685",
      "value": "Generating\u2007validation\u2007split:\u2007100%"
     }
    },
    "549d04ea663344b3b1d98a16716e497d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55db34967bd841ef93d14add79018799": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "560bb55736aa424c936bd8c68605bc22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56ac002fe0c24f8b81960a2a03bbb794": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5732a89badd84529b9388f5686d25c98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59b21156ec7846d089f84b03d5740b4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5aac047c359641ab92baaf95a99ceb4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f444056f2d34e2e83c87aaa8bd08c74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "620dcb369d8e44469f5e6b1d30c9e944": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5732a89badd84529b9388f5686d25c98",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_9178784ffa5642e4bd1b916c8a1ea282",
      "value": "Loading\u2007checkpoint\u2007shards:\u2007100%"
     }
    },
    "633708e71daf4140a8161ec3405cda70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "634eda616a654f489fd58a6a3296aa70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64841403cf6c48acb9de83bbbfddc134": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65c424f15a0c41d6b12dcbc4760b0a0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_707f577cb8ab4bfa91a1fbf7136b62b7",
       "IPY_MODEL_d58ee84b514848838fed2e73df07485e",
       "IPY_MODEL_a2e4ef4eb36d4d63b5b94c5b342af164"
      ],
      "layout": "IPY_MODEL_dd93f787b0e747e8a8ef199d26a07213"
     }
    },
    "67773175857d4d3abe5e5f6ee172fe2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d544667e1a4349a3b5d101c91546b8dc",
      "max": 166162479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8fa6b91dc7dd44cd81a7108634853fb7",
      "value": 166162479
     }
    },
    "698ff698b96f485fa51f305ca9d45218": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_187a6a6288ca4bdba264a1d2ed89874c",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_3fba0d9d196c45e297027f6e6085b763",
      "value": "\u20079.52k/?\u2007[00:00&lt;00:00,\u2007854kB/s]"
     }
    },
    "6b32657d3d3b4f589b7cbcbc22e3e905": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b4da0ecb6eb45d693aa4236cde922cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee6c1750eb744d5899aef6b15386bb36",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_deee54bc243a407b8f508000c9086b98",
      "value": 1
     }
    },
    "6ccb751588da46569f9322368b5eb3c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71f19f72f2f549e8b61d80a3935fffe5",
       "IPY_MODEL_7e25296404824c128702f2d24100b242",
       "IPY_MODEL_f9dd8777920b477282429d6ce9bd4429"
      ],
      "layout": "IPY_MODEL_3b48c0cf2c2f4a5b81af800b8d20232b"
     }
    },
    "6f36eeeb291846a7b6479382f11aa0fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "702b5111780f4d7cbc0721f0377f868a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "707f577cb8ab4bfa91a1fbf7136b62b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fe26d02caad41528d5e64a45004a070",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2431ccdffec249b5b6b5a7c361b0e270",
      "value": "Fetching\u20073\u2007files:\u2007100%"
     }
    },
    "71f19f72f2f549e8b61d80a3935fffe5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_560bb55736aa424c936bd8c68605bc22",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_702b5111780f4d7cbc0721f0377f868a",
      "value": "special_tokens_map.json:\u2007100%"
     }
    },
    "72b9562cb00341c08d71a4f9b813e0d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cd5077a206574c8a8fcbfbe88588358c",
       "IPY_MODEL_67773175857d4d3abe5e5f6ee172fe2c",
       "IPY_MODEL_962d2223b9f54554bf97f424156515c0"
      ],
      "layout": "IPY_MODEL_96cf6240ea83430cb142b4cb20aee8a0"
     }
    },
    "76261fcd41784089b133b81910fc29c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7720c5396332436f85f593241e1955a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7783ec693b064f1daac952ad899df9de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79dbe78107474dc28bc60a3fa5fea04e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a682bcdcf6f4fe09d64bc54a972f7df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e84f05ae0274c69ab6f322e6ba722d3",
       "IPY_MODEL_ed17f280177e46abb93219ed4792f05c",
       "IPY_MODEL_cfbb8c4c46d842c7905d41b1cf5e9cf5"
      ],
      "layout": "IPY_MODEL_c8b46470b21b45958ad4ecb2c5e4e419"
     }
    },
    "7a846fc69808402499ae77acec99ceff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ac61abe432f4bea90d106a6e4624873": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b32657d3d3b4f589b7cbcbc22e3e905",
      "max": 90447,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76261fcd41784089b133b81910fc29c2",
      "value": 90447
     }
    },
    "7c287fef7c73452abacae570c16684c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d209fda5a6446499539d1d7203f5faf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7e25296404824c128702f2d24100b242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55db34967bd841ef93d14add79018799",
      "max": 414,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2a5165e0f2e4a238d051a3a40bd99b2",
      "value": 414
     }
    },
    "7e46a4c0ee0b4367a188e4036772595c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "804c36176e5c4d1ab7964e24584614a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "814299fbdb7f4192b17b5700bce29d76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "815af36056bf412cb4ef1d64e0553a2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81c2a555d761422bbcd5ec905cd4a955": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d209fda5a6446499539d1d7203f5faf",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3119963fe9d14d249896d0f705b91f01",
      "value": 1
     }
    },
    "829ced0967144b4b8d759d36191c8a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "82fb7e5fd4d14b5a86e361ad1c04d2f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "842982ed82d24b54b66030814605b85b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccd3a3229be94a75950748c57a40431d",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_c76d3754ceee483da3b4ad1f8118f3e9",
      "value": "\u200725.1k/?\u2007[00:00&lt;00:00,\u20073.08MB/s]"
     }
    },
    "85421197a0f7408a899cfb296a39ed8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85ac65e52717443ba04947a77a61f131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cee2a3249cc6401ebab75a8a2f191578",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_a66ed9b6136f4ccc92605415103fa9c0",
      "value": "distractor/validation-00000-of-00001.par(\u2026):\u2007100%"
     }
    },
    "86bfc101972d4e108269f33340d1e7e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2278ecf3d03484aa04367bffbb8ed89",
      "max": 4540516344,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a2d1fbb3adf94ecda033e3ccc4a6d186",
      "value": 4540516344
     }
    },
    "87cd0c83a63f4a8982b9afbb77da4131": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "8a9a6ee0c1b443d79c8bc2734b7b731e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56ac002fe0c24f8b81960a2a03bbb794",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2828185fc1fe41418db2b23adfc46cb4",
      "value": "Generating\u2007train\u2007split:\u2007100%"
     }
    },
    "8b2d0a2daf394531b8506f7080bf719e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b3e7f8e1e4341c9b2b0f52f31181089": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9065c28a88b5488cb5413415bfeae51a",
       "IPY_MODEL_be101df85dfd4e3d8396e0d815da49d8",
       "IPY_MODEL_bf69f8e28e64435192aaf73e65ea2854"
      ],
      "layout": "IPY_MODEL_90133904a807401c83391a6eb9acda8d"
     }
    },
    "8cbc4fa9adcb438696b0035c476136d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc94edbbcc54876bb568618f32affd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e046df5dcaa443c83b7b0842b4b4538": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fa6b91dc7dd44cd81a7108634853fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fa7ee8a8f744804bdda4711d866c998": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fc4f404697743d6bd9bb17a2d59417c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90133904a807401c83391a6eb9acda8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9065c28a88b5488cb5413415bfeae51a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f06d62d0716456887fceda88251e689",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_fd3a4eef23c64d4e87c365d9b9e2b95f",
      "value": "generation_config.json:\u2007100%"
     }
    },
    "9178784ffa5642e4bd1b916c8a1ea282": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91afc639f87340d3b434764f2eedefdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95af57ee1e804f85bfde6c350a193750": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "962d2223b9f54554bf97f424156515c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17c033f53f2b482094180210cc621303",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_b89ceba61b1d4f1e9202f93c88d91ca7",
      "value": "\u2007166M/166M\u2007[00:01&lt;00:00,\u2007120MB/s]"
     }
    },
    "9691836563a04a8ba549433f9a9c2987": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96cf6240ea83430cb142b4cb20aee8a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "978554eb4607431b91cdaf138ebb84b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d890aec42924427f8a5d3cf7448a2a15",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_32eaf86422d842068b067a47e8172e84",
      "value": "\u20074.54G/4.54G\u2007[01:36&lt;00:00,\u2007201MB/s]"
     }
    },
    "97c351c9a07a415db893fc5e4f37b353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f06d62d0716456887fceda88251e689": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f4288faa20d45b68b8aaa1434d196c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a03f4788e6af4e0dab5b02361ee00655": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a069b5a20a064ec0b063f852c3b4d853": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2d1fbb3adf94ecda033e3ccc4a6d186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2e4ef4eb36d4d63b5b94c5b342af164": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cf16a2faed1473ba9ae4f4645e1c06b",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_509cd0b9862b4c318dd9682615bc945d",
      "value": "\u20073/3\u2007[01:37&lt;00:00,\u200722.72s/it]"
     }
    },
    "a37129fe3edd4682ba7baaf60d28d26a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_815af36056bf412cb4ef1d64e0553a2f",
      "max": 7405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_efa8482fb9f44bac804feda7b9b8dcb6",
      "value": 7405
     }
    },
    "a4e6e88de34749c7a10da51e7ac7057e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b14d4c10af1f436da42fedd91676b943",
       "IPY_MODEL_281ddd4688ad454e8d2cf7fa118406d0",
       "IPY_MODEL_cb32a67aea674945b107ec8c114bbe76"
      ],
      "layout": "IPY_MODEL_a03f4788e6af4e0dab5b02361ee00655"
     }
    },
    "a5b76b5ac9f3420fbf7c8f782040b496": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_620dcb369d8e44469f5e6b1d30c9e944",
       "IPY_MODEL_c01e1ac617d340d3bd6c9dbdd7fb8dfa",
       "IPY_MODEL_2e8a0f38378c460b95102fdff1202f2c"
      ],
      "layout": "IPY_MODEL_4dd6f9fea4ee4324bdf7485ae29cbc9c"
     }
    },
    "a66ed9b6136f4ccc92605415103fa9c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aa4c0504edf44137ad2c93949741a03d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae2330341f024f25893268efb1351bc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87cd0c83a63f4a8982b9afbb77da4131",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01bf9cefce43421d800b14b223487194",
      "value": 1
     }
    },
    "b14d4c10af1f436da42fedd91676b943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85421197a0f7408a899cfb296a39ed8b",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_7720c5396332436f85f593241e1955a2",
      "value": "tokenizer_config.json:\u2007"
     }
    },
    "b2a5165e0f2e4a238d051a3a40bd99b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b2e6911217ef4bbb8e164a1519f69d91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f841fc538e1e477989c48eabc9f30246",
       "IPY_MODEL_6b4da0ecb6eb45d693aa4236cde922cc",
       "IPY_MODEL_698ff698b96f485fa51f305ca9d45218"
      ],
      "layout": "IPY_MODEL_4764aef8050f4ff4872d7289e525097c"
     }
    },
    "b4596ce9ec834648896d56e39e60821a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4fff7bee8584db98b91a19708319453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1fe713e86044d6b8ba3203474d48187",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_458e9b1854684513b7ae8d66c0eab48c",
      "value": "model-00003-of-00003.safetensors:\u2007100%"
     }
    },
    "b65e89a8d36c4701a8ea30070b2cc6d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b75d37e839ad46c1b028daeaf16ce0dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b84fee492fe041549927c1c9ceda470c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34748267fb164a64a74cdaf23510fd2b",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_19d9aaa0d4f74837994d4ad10935b74d",
      "value": "\u200790447/90447\u2007[00:02&lt;00:00,\u200736360.97\u2007examples/s]"
     }
    },
    "b89ceba61b1d4f1e9202f93c88d91ca7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8b76a3d07004320b81747587e2a4f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b94b35bfeab348449a9c5de97195849a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9b03cd8015d472f92928b29714581a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e3c62526f7740b1af7e45bc8c0c72fc",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2e110498596345228afc933e0692b9c3",
      "value": "\u2007493k/493k\u2007[00:00&lt;00:00,\u20072.61MB/s]"
     }
    },
    "ba851777dfe845a9ab888330073d0e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bb5eccb2a97242e7810e7cf13ce713fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fa21d794f734d078c62f1c96ca342ca",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_daaad329400341d7a6f82259f2fccb3d",
      "value": 3
     }
    },
    "bbd58975e5774330802e46950eb59a9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa4c0504edf44137ad2c93949741a03d",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_22f01198bd9a4a78b56e8c48b978ba37",
      "value": "tokenizer.model:\u2007100%"
     }
    },
    "bd4a2b63edeb4d938a86a18c08bfb319": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce4d3ca7c29f4945b80b4777562e9342",
      "max": 596,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1612a18bc3e94b68865ddecc13bf857b",
      "value": 596
     }
    },
    "be101df85dfd4e3d8396e0d815da49d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ea46134a60d4522b8076987fb045033",
      "max": 111,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc55ce38fd9448a0b6855a819a815f9f",
      "value": 111
     }
    },
    "bf69f8e28e64435192aaf73e65ea2854": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7783ec693b064f1daac952ad899df9de",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_8e046df5dcaa443c83b7b0842b4b4538",
      "value": "\u2007111/111\u2007[00:00&lt;00:00,\u200715.6kB/s]"
     }
    },
    "c01e1ac617d340d3bd6c9dbdd7fb8dfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_403d5350f8fc408aa5f005a22d16c73c",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0d4053f76f8743a09bb511320e2b9df2",
      "value": 3
     }
    },
    "c037a04f65c9439d80f8b7d8654ac95c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d672863a0d494cbd92ad3ddcb161eb61",
      "max": 27452575,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8fc4f404697743d6bd9bb17a2d59417c",
      "value": 27452575
     }
    },
    "c1fe713e86044d6b8ba3203474d48187": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c31306979cf5433f9969b5f7a700924c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fa47185cda4487bbcbd1bd0ade7aa08",
       "IPY_MODEL_ae2330341f024f25893268efb1351bc7",
       "IPY_MODEL_842982ed82d24b54b66030814605b85b"
      ],
      "layout": "IPY_MODEL_7a846fc69808402499ae77acec99ceff"
     }
    },
    "c5af8b913c1d4f20835bb6b247d95374": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36eb02379ec9430d83eb522601d99a7f",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_b8b76a3d07004320b81747587e2a4f85",
      "value": "\u20071.80M/?\u2007[00:00&lt;00:00,\u20072.22MB/s]"
     }
    },
    "c76d3754ceee483da3b4ad1f8118f3e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8b46470b21b45958ad4ecb2c5e4e419": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca2cf828cf5b47779ddb270af704b9e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b75d37e839ad46c1b028daeaf16ce0dc",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_363ab3dc989b465da01f6dad251a9e35",
      "value": "\u2007596/596\u2007[00:00&lt;00:00,\u200754.7kB/s]"
     }
    },
    "cb32a67aea674945b107ec8c114bbe76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b2d0a2daf394531b8506f7080bf719e",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_6f36eeeb291846a7b6479382f11aa0fa",
      "value": "\u20072.10k/?\u2007[00:00&lt;00:00,\u2007253kB/s]"
     }
    },
    "cbcafa96eb9c4d119f3d71b01538ca00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc128d431dd346aaa8cde7999808bc75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccd3a3229be94a75950748c57a40431d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd5077a206574c8a8fcbfbe88588358c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39c2b0015fdd434687d68e211419ab8b",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_59b21156ec7846d089f84b03d5740b4d",
      "value": "distractor/train-00001-of-00002.parquet:\u2007100%"
     }
    },
    "cdf35d781fd446058421b2c19000df03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ce4d3ca7c29f4945b80b4777562e9342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cee2a3249cc6401ebab75a8a2f191578": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfbb8c4c46d842c7905d41b1cf5e9cf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d86d10d6c8b438a95a3215fc9089bdd",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_e435f20e3fb94e1cbce2c1f14fa27d5f",
      "value": "\u20075.00G/5.00G\u2007[01:36&lt;00:00,\u2007129MB/s]"
     }
    },
    "d02038fea5cc41fbb3ed8ffef21b48fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d2278ecf3d03484aa04367bffbb8ed89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2c8af1cb1df4c52af892a5225dcc1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3ac3626fa30477fb0bb69ad66aa36d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_faf36b712cd04edcb18e1a42dfa2b191",
       "IPY_MODEL_f2bad34d8730443bae07cbea18976b68",
       "IPY_MODEL_39e312a338354fc887b7e9d1342ab58f"
      ],
      "layout": "IPY_MODEL_91afc639f87340d3b434764f2eedefdd"
     }
    },
    "d544667e1a4349a3b5d101c91546b8dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d58ee84b514848838fed2e73df07485e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cbc4fa9adcb438696b0035c476136d2",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d02038fea5cc41fbb3ed8ffef21b48fb",
      "value": 3
     }
    },
    "d672863a0d494cbd92ad3ddcb161eb61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d890aec42924427f8a5d3cf7448a2a15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "daaad329400341d7a6f82259f2fccb3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dd93f787b0e747e8a8ef199d26a07213": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deee54bc243a407b8f508000c9086b98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e0a127e339cb4174bf00654cf3f32f78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e435f20e3fb94e1cbce2c1f14fa27d5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6f5bed45ea4420a863e61242518e0eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_814299fbdb7f4192b17b5700bce29d76",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_b94b35bfeab348449a9c5de97195849a",
      "value": "model-00001-of-00003.safetensors:\u2007100%"
     }
    },
    "eb0f5bcd20df4e158c66c63de50a96e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa63898fabf64f6884e53bf11fd55bfa",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_14eef61174bf46d5a68f0d661b2c823b",
      "value": "\u20077405/7405\u2007[00:00&lt;00:00,\u200729147.55\u2007examples/s]"
     }
    },
    "ec3daf2847104603b36d677f5e791e1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95af57ee1e804f85bfde6c350a193750",
      "max": 165624177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cbcafa96eb9c4d119f3d71b01538ca00",
      "value": 165624177
     }
    },
    "ec976b5c1f5448e882998360593be166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed17f280177e46abb93219ed4792f05c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ec4e474c8114be7978b0b2867567604",
      "max": 4999819336,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba851777dfe845a9ab888330073d0e8e",
      "value": 4999819336
     }
    },
    "ee6c1750eb744d5899aef6b15386bb36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "efa8482fb9f44bac804feda7b9b8dcb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f0444a5dc306443393ba2d383e02eb86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1c4402da7994ca1a04f2783b7240890": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_634eda616a654f489fd58a6a3296aa70",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_2215bbc48a8845878e336f0e93a7c18b",
      "value": "\u200727.5M/27.5M\u2007[00:00&lt;00:00,\u200743.2MB/s]"
     }
    },
    "f2bad34d8730443bae07cbea18976b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdbbedc7f1f842498dacb53caac8f6bd",
      "max": 596,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_549d04ea663344b3b1d98a16716e497d",
      "value": 596
     }
    },
    "f841fc538e1e477989c48eabc9f30246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0444a5dc306443393ba2d383e02eb86",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_485a6625f59149909c1288f50a6a796a",
      "value": "README.md:\u2007"
     }
    },
    "f8b6638f7efe4ba6a8b399f989a79685": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f995fecfbb6042698d8d9155895a8c5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9dd8777920b477282429d6ce9bd4429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82fb7e5fd4d14b5a86e361ad1c04d2f7",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_7e46a4c0ee0b4367a188e4036772595c",
      "value": "\u2007414/414\u2007[00:00&lt;00:00,\u200750.9kB/s]"
     }
    },
    "fa63898fabf64f6884e53bf11fd55bfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "faf36b712cd04edcb18e1a42dfa2b191": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64841403cf6c48acb9de83bbbfddc134",
      "placeholder": "\u200b",
      "style": "IPY_MODEL_a069b5a20a064ec0b063f852c3b4d853",
      "value": "config.json:\u2007100%"
     }
    },
    "fc55ce38fd9448a0b6855a819a815f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd3a4eef23c64d4e87c365d9b9e2b95f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdbbedc7f1f842498dacb53caac8f6bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdf6db8980a94224a87f646f1222ff86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}